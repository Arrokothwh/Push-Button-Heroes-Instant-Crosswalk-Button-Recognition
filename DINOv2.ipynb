{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5396dace",
   "metadata": {},
   "source": [
    "Download torch and torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29a9face",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch\n",
    "# pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65cb3bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674a9168",
   "metadata": {},
   "source": [
    "Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04e97117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA GeForce RTX 4080 is available.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.datasets import ImageFolder\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from copy import deepcopy\n",
    "\n",
    "USE_GPU = True\n",
    "dtype = torch.float32 \n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)} is available.\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"No GPU available. Training will run on CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc9ed10",
   "metadata": {},
   "source": [
    "If the output is \"using device: cpu\": download CUDA toolkit (https://developer.nvidia.com/cuda-downloads)\n",
    "https://pytorch.org/get-started/locally/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bba1aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data \n",
    "random_trans = [v2.ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2), saturation=(0.8, 1.2), hue=(-0.1, 0.1)), \n",
    "                v2.RandomRotation(degrees=10)]\n",
    "\n",
    "transform = v2.Compose([\n",
    "                v2.ToImage(), \n",
    "                v2.ToDtype(torch.float32, scale=True),\n",
    "                v2.RandomApply(random_trans)\n",
    "                # v2.ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2), saturation=(0.8, 1.2), hue=(-0.1, 0.1)),\n",
    "                # v2.RandomChannelPermutation(),\n",
    "                # v2.Normalize((0.2382, 0.2426, 0.2483), (0.3126, 0.3170, 0.3241))\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80516c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CropButtonDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform = None):\n",
    "        self.data = ImageFolder(data_dir, transform=transform)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "    \n",
    "    # return classes\n",
    "    def classes(self):\n",
    "        return self.data.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eaae6175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creat a dictionary that maps index to label\n",
    "data_dir='ClassificationDataset_label/train'\n",
    "target_to_class = {v: k for k, v in ImageFolder(data_dir).class_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ae25686",
   "metadata": {},
   "outputs": [],
   "source": [
    "button_train = CropButtonDataset('ClassificationDataset_label/train', transform=transform)\n",
    "loader_train = DataLoader(button_train, batch_size=64, shuffle=True)\n",
    "\n",
    "button_val = CropButtonDataset('ClassificationDataset_label/val', transform=transform)\n",
    "loader_val = DataLoader(button_val, batch_size=14, shuffle=True)\n",
    "\n",
    "button_test = CropButtonDataset('ClassificationDataset_label/test', transform=transform)\n",
    "loader_test = DataLoader(button_test, batch_size=14, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ffa704",
   "metadata": {},
   "source": [
    "Check If load correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5f9b601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1d77d460f50>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1155e0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loaded!\n",
      "Images shape: torch.Size([14, 3, 320, 320])\n",
      "Labels: tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "for images, labels in loader_val:\n",
    "    print(\"Batch loaded!\")\n",
    "    print(\"Images shape:\", images.shape)\n",
    "    print(\"Labels:\", labels)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed97514",
   "metadata": {},
   "source": [
    "Load pre-trained model and check output shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3917ebfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "image = Image.open('ClassificationDataset_label/train/left/4ebee0cf-69805b37-20250502_173224_0_class0.jpg')\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained('facebook/dinov2-base', use_fast=True)\n",
    "dino = AutoModel.from_pretrained('facebook/dinov2-base')\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "outputs = dino(**inputs)\n",
    "last_hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0462b155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 257, 768])\n"
     ]
    }
   ],
   "source": [
    "print(last_hidden_states.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e136a99",
   "metadata": {},
   "source": [
    "Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb7a2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DinoClassifier(nn.Module):\n",
    "    def __init__(self, backbone_name='facebook/dinov2-base'):\n",
    "        super().__init__()\n",
    "        self.processor = AutoImageProcessor.from_pretrained(backbone_name, use_fast=True)\n",
    "        self.backbone = AutoModel.from_pretrained(backbone_name)\n",
    "\n",
    "        # freeze backbone\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        hidden_dim = self.backbone.config.hidden_size\n",
    "\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.norm = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(hidden_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(512, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    # CLS + Mean pooling\n",
    "    def forward(self, x_tensor):  # x_tensor: Tensor, shape [B, 3, H, W]\n",
    "        # inputs = self.processor(images=x_tensor, return_tensors=\"pt\") \n",
    "        # inputs = {k: v.to(x_tensor.device) for k, v in inputs.items()}\n",
    "\n",
    "        # outputs = self.backbone(**inputs)\n",
    "        # last_hidden = outputs.last_hidden_state  \n",
    "\n",
    "        # cls_token = last_hidden[:, 0, :]             \n",
    "        # mean_pool = last_hidden[:, 1:, :].mean(dim=1) \n",
    "\n",
    "        # fusion = torch.cat([cls_token, mean_pool], dim=-1) \n",
    "\n",
    "        # 1. Preprocess image\n",
    "        inputs = self.processor(images=x_tensor, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(x_tensor.device) for k, v in inputs.items()}\n",
    "\n",
    "        # 2. Backbone forward\n",
    "        with torch.no_grad():\n",
    "            outputs = self.backbone(**inputs)\n",
    "        last_hidden = outputs.last_hidden_state  # (B, N+1, D)\n",
    "\n",
    "        # 3. Multi-head attention on patch + CLS tokens\n",
    "        attn_out, _ = self.attn(last_hidden, last_hidden, last_hidden)  # shape: (B, N+1, D)\n",
    "        attn_out = self.norm(attn_out)\n",
    "\n",
    "        # 4. Use [CLS] token only\n",
    "        cls_token = attn_out[:, 0, :]  # (B, D)\n",
    "\n",
    "        # 5. Classifier\n",
    "        logits = self.classifier(cls_token)\n",
    "        return logits\n",
    "        # return self.classifier(fusion)  \n",
    "\n",
    "    \n",
    "model = DinoClassifier()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a128e4ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier.0.weight\n",
      "classifier.0.bias\n",
      "classifier.1.weight\n",
      "classifier.1.bias\n",
      "classifier.2.weight\n",
      "classifier.2.bias\n",
      "classifier.5.weight\n",
      "classifier.5.bias\n",
      "classifier.6.weight\n",
      "classifier.6.bias\n",
      "classifier.9.weight\n",
      "classifier.9.bias\n"
     ]
    }
   ],
   "source": [
    "# check freeze\n",
    "for name, param in model.named_parameters():\n",
    "    # if not freeze\n",
    "    if (param.requires_grad):\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d8d1c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy_final(loader, model, out=False):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=dtype)\n",
    "            y = y.to(device=device, dtype=dtype).unsqueeze(1)\n",
    "            scores = model(x)\n",
    "            preds = (torch.sigmoid(scores) > 0.5).long()\n",
    "\n",
    "            if out:\n",
    "                print(\"Sample preds:\", torch.sigmoid(scores).flatten().tolist())\n",
    "                print(\"Sample labels:\", y.flatten().tolist())\n",
    "            \n",
    "            num_correct += (preds == y.long()).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8433fcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, epochs=10, opt=False):\n",
    "    x1 = list(range(len(loader_train) * epochs))\n",
    "    y1 = []\n",
    "    y2 = []\n",
    "    y3 = []\n",
    "    model = model.to(device=device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    cnt = 1\n",
    "    total_cnt = epochs * len(loader_train)\n",
    "    for e in range(epochs):\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            model.train()\n",
    "            x = x.to(device=device, dtype=dtype)\n",
    "            y = y.to(device=device, dtype=dtype).unsqueeze(1)\n",
    "\n",
    "            scores = model(x)\n",
    "            loss = criterion(scores, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            y1.append(loss.item())\n",
    "            acc_val = check_accuracy_final(loader_val, model, out=opt)\n",
    "            acc_train = check_accuracy_final(loader_train, model, out=opt)\n",
    "            y2.append(acc_val)\n",
    "            y3.append(check_accuracy_final(loader_train, model))\n",
    "            print(\"iteration: \" +  str(cnt) + \"/\" + str(total_cnt) + \"   loss = \" + str(loss.item()) + \n",
    "                  \"   train_acc = \" + str(acc_train) + \"   val_acc = \" + str(acc_val) )\n",
    "\n",
    "            cnt += 1\n",
    "    return (x1, y1, y2, y3)\n",
    "\n",
    "def plotpic(data):\n",
    "    x, y1, y2, y3 = data\n",
    "    plt.figure()\n",
    "    plt.plot(x, y1, marker='.')\n",
    "    plt.xlabel('Iteratoin')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss')\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(x, y2, marker='o', label='Validation Accuracy')\n",
    "    plt.plot(x, y3, marker='o', label='Training Accuracy')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training vs Validation Accuracy')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58d67999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "# sgdMNb = optim.SGD(model.parameters(), lr=1e-2, momentum=0.8, nesterov=True)\n",
    "# adamw = optim.AdamW(model.parameters(), lr=5e-5, weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2436af1",
   "metadata": {},
   "source": [
    "Find best hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "95be1894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_search(DinoClassifier, loader_train, loader_val, device, epochs=3, opt=False):\n",
    "    lr_list = 10 ** np.random.uniform(-2, -6, (10,))\n",
    "    wd_list = 10 ** np.random.uniform(-2, -4, (5,))\n",
    "    beta_list = [(0.85, 0.99), (0.85, 0.995), (0.9, 0.98), (0.9, 0.99), (0.9, 0.995), (0.9, 0.999),\n",
    "    (0.95, 0.98), (0.95, 0.99), (0.95, 0.995), (0.95, 0.999)]\n",
    "\n",
    "    best_acc = 0.0\n",
    "    best_model = None\n",
    "    best_params = {}\n",
    "    \n",
    "    for lr, wd, betas in itertools.product(lr_list, wd_list, beta_list):\n",
    "        print(f\"\\nðŸ”§ Testing AdamW: lr={lr}, wd={wd}, betas={betas}\")\n",
    "        \n",
    "        model = DinoClassifier().to(device)\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            filter(lambda p: p.requires_grad, model.parameters()),\n",
    "            lr=lr,\n",
    "            weight_decay=wd,\n",
    "            betas=betas\n",
    "        )\n",
    "\n",
    "        data = train(model, optimizer, epochs=epochs, opt=opt)\n",
    "        _, _, val_accs, _ = data\n",
    "\n",
    "        final_val_acc = val_accs[-1]\n",
    "        print(f\"Final val acc for lr={lr}, wd={wd}: {final_val_acc:.4f}\")\n",
    "\n",
    "        if final_val_acc > best_acc:\n",
    "            best_acc = final_val_acc\n",
    "            best_params = {\"lr\": lr, \"weight_decay\": wd}\n",
    "            best_model = deepcopy(model)\n",
    "            best_training_data = data\n",
    "\n",
    "    print(\"\\n Best Hyperparameters:\")\n",
    "    print(best_params)\n",
    "    print(f\"Best val acc: {best_acc:.4f}\")\n",
    "\n",
    "    # save model\n",
    "    torch.save(best_model.state_dict(), \"best_dino_model.pt\")\n",
    "    print(\" Saved best model to best_dino_model.pt\")\n",
    "\n",
    "    return best_model, best_params, best_training_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ab6a273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.515914167041141e-06, wd=0.0004609389889268166, betas=(0.85, 0.99)\n",
      "iteration: 1/9   loss = 0.7170377969741821   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.6934084892272949   train_acc = 0.4701492537313433   val_acc = 0.5714285714285714\n",
      "iteration: 3/9   loss = 0.7131143808364868   train_acc = 0.5   val_acc = 0.6428571428571429\n",
      "iteration: 4/9   loss = 0.78199303150177   train_acc = 0.4925373134328358   val_acc = 0.6428571428571429\n",
      "iteration: 5/9   loss = 0.6965426206588745   train_acc = 0.47761194029850745   val_acc = 0.5714285714285714\n",
      "iteration: 6/9   loss = 0.8168330192565918   train_acc = 0.48507462686567165   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.7336984276771545   train_acc = 0.4925373134328358   val_acc = 0.6428571428571429\n",
      "iteration: 8/9   loss = 0.7498762011528015   train_acc = 0.4925373134328358   val_acc = 0.6428571428571429\n",
      "iteration: 9/9   loss = 0.810998797416687   train_acc = 0.4701492537313433   val_acc = 0.5\n",
      "Final val acc for lr=1.515914167041141e-06, wd=0.0004609389889268166: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.515914167041141e-06, wd=0.0004609389889268166, betas=(0.85, 0.995)\n",
      "iteration: 1/9   loss = 0.670932412147522   train_acc = 0.47761194029850745   val_acc = 0.35714285714285715\n",
      "iteration: 2/9   loss = 0.7358102798461914   train_acc = 0.5597014925373134   val_acc = 0.35714285714285715\n",
      "iteration: 3/9   loss = 0.6551063656806946   train_acc = 0.47761194029850745   val_acc = 0.35714285714285715\n",
      "iteration: 4/9   loss = 0.6689530611038208   train_acc = 0.5074626865671642   val_acc = 0.35714285714285715\n",
      "iteration: 5/9   loss = 0.6808291673660278   train_acc = 0.5   val_acc = 0.35714285714285715\n",
      "iteration: 6/9   loss = 0.7397772669792175   train_acc = 0.5074626865671642   val_acc = 0.35714285714285715\n",
      "iteration: 7/9   loss = 0.6676861643791199   train_acc = 0.5223880597014925   val_acc = 0.35714285714285715\n",
      "iteration: 8/9   loss = 0.7359723448753357   train_acc = 0.5298507462686567   val_acc = 0.35714285714285715\n",
      "iteration: 9/9   loss = 0.7161654829978943   train_acc = 0.44029850746268656   val_acc = 0.2857142857142857\n",
      "Final val acc for lr=1.515914167041141e-06, wd=0.0004609389889268166: 0.2857\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.515914167041141e-06, wd=0.0004609389889268166, betas=(0.9, 0.98)\n",
      "iteration: 1/9   loss = 0.775318443775177   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.8040677905082703   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.6970224380493164   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.8114045858383179   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.737862765789032   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.6011404395103455   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.8239936828613281   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7617383599281311   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 9/9   loss = 1.0124565362930298   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "Final val acc for lr=1.515914167041141e-06, wd=0.0004609389889268166: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.515914167041141e-06, wd=0.0004609389889268166, betas=(0.9, 0.99)\n",
      "iteration: 1/9   loss = 0.7293107509613037   train_acc = 0.44776119402985076   val_acc = 0.5714285714285714\n",
      "iteration: 2/9   loss = 0.685791015625   train_acc = 0.47761194029850745   val_acc = 0.35714285714285715\n",
      "iteration: 3/9   loss = 0.9749979972839355   train_acc = 0.44776119402985076   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.7545520067214966   train_acc = 0.4925373134328358   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.7352014780044556   train_acc = 0.4626865671641791   val_acc = 0.35714285714285715\n",
      "iteration: 6/9   loss = 0.7469450235366821   train_acc = 0.4626865671641791   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.6979601383209229   train_acc = 0.5074626865671642   val_acc = 0.35714285714285715\n",
      "iteration: 8/9   loss = 0.7842121124267578   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.701543927192688   train_acc = 0.48507462686567165   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=1.515914167041141e-06, wd=0.0004609389889268166: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.515914167041141e-06, wd=0.0004609389889268166, betas=(0.9, 0.995)\n",
      "iteration: 1/9   loss = 0.7332452535629272   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7866374254226685   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.5925965309143066   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7468550205230713   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7752264142036438   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.6670838594436646   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7506265044212341   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7179543972015381   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.8629617691040039   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "Final val acc for lr=1.515914167041141e-06, wd=0.0004609389889268166: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.515914167041141e-06, wd=0.0004609389889268166, betas=(0.9, 0.999)\n",
      "iteration: 1/9   loss = 0.778461217880249   train_acc = 0.5   val_acc = 0.6428571428571429\n",
      "iteration: 2/9   loss = 0.7032114267349243   train_acc = 0.4626865671641791   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.6798750758171082   train_acc = 0.4701492537313433   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7597941756248474   train_acc = 0.4701492537313433   val_acc = 0.7142857142857143\n",
      "iteration: 5/9   loss = 0.7543703317642212   train_acc = 0.47761194029850745   val_acc = 0.6428571428571429\n",
      "iteration: 6/9   loss = 0.6861786842346191   train_acc = 0.4701492537313433   val_acc = 0.2857142857142857\n",
      "iteration: 7/9   loss = 0.7568919062614441   train_acc = 0.48507462686567165   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.7126975059509277   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.7056559920310974   train_acc = 0.4626865671641791   val_acc = 0.7142857142857143\n",
      "Final val acc for lr=1.515914167041141e-06, wd=0.0004609389889268166: 0.7143\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.515914167041141e-06, wd=0.0004609389889268166, betas=(0.95, 0.98)\n",
      "iteration: 1/9   loss = 0.6641950011253357   train_acc = 0.5447761194029851   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.7396026849746704   train_acc = 0.5671641791044776   val_acc = 0.35714285714285715\n",
      "iteration: 3/9   loss = 0.7371313571929932   train_acc = 0.5447761194029851   val_acc = 0.35714285714285715\n",
      "iteration: 4/9   loss = 0.6785039901733398   train_acc = 0.5970149253731343   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.6725537776947021   train_acc = 0.5746268656716418   val_acc = 0.35714285714285715\n",
      "iteration: 6/9   loss = 0.6615331172943115   train_acc = 0.5746268656716418   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.7010729312896729   train_acc = 0.5447761194029851   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.7078133225440979   train_acc = 0.5895522388059702   val_acc = 0.35714285714285715\n",
      "iteration: 9/9   loss = 0.8342185020446777   train_acc = 0.5298507462686567   val_acc = 0.5\n",
      "Final val acc for lr=1.515914167041141e-06, wd=0.0004609389889268166: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.515914167041141e-06, wd=0.0004609389889268166, betas=(0.95, 0.99)\n",
      "iteration: 1/9   loss = 0.7478237152099609   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.6900376081466675   train_acc = 0.5074626865671642   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.9276474714279175   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.6665199995040894   train_acc = 0.5074626865671642   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.804640531539917   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.9674667716026306   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7023435831069946   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7179949283599854   train_acc = 0.5074626865671642   val_acc = 0.42857142857142855\n",
      "iteration: 9/9   loss = 0.837232768535614   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "Final val acc for lr=1.515914167041141e-06, wd=0.0004609389889268166: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.515914167041141e-06, wd=0.0004609389889268166, betas=(0.95, 0.995)\n",
      "iteration: 1/9   loss = 0.6907461285591125   train_acc = 0.5223880597014925   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.6603989005088806   train_acc = 0.5298507462686567   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.7766451239585876   train_acc = 0.5074626865671642   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.7262585163116455   train_acc = 0.4925373134328358   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.6753826141357422   train_acc = 0.4925373134328358   val_acc = 0.5714285714285714\n",
      "iteration: 6/9   loss = 0.6524815559387207   train_acc = 0.5   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.6849783658981323   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7288700342178345   train_acc = 0.5298507462686567   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.8206397891044617   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "Final val acc for lr=1.515914167041141e-06, wd=0.0004609389889268166: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.515914167041141e-06, wd=0.0004609389889268166, betas=(0.95, 0.999)\n",
      "iteration: 1/9   loss = 0.7148375511169434   train_acc = 0.41044776119402987   val_acc = 0.35714285714285715\n",
      "iteration: 2/9   loss = 0.7640680074691772   train_acc = 0.40298507462686567   val_acc = 0.2857142857142857\n",
      "iteration: 3/9   loss = 0.6465184688568115   train_acc = 0.39552238805970147   val_acc = 0.2857142857142857\n",
      "iteration: 4/9   loss = 0.6902529001235962   train_acc = 0.41044776119402987   val_acc = 0.35714285714285715\n",
      "iteration: 5/9   loss = 0.800426721572876   train_acc = 0.4253731343283582   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.8440043330192566   train_acc = 0.3880597014925373   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7250853776931763   train_acc = 0.3805970149253731   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.7208625078201294   train_acc = 0.43283582089552236   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.5779581069946289   train_acc = 0.40298507462686567   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=1.515914167041141e-06, wd=0.0004609389889268166: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.515914167041141e-06, wd=0.0002300292096821906, betas=(0.85, 0.99)\n",
      "iteration: 1/9   loss = 0.7141503691673279   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7623765468597412   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.9444224238395691   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7310925722122192   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7177168130874634   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.8335256576538086   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7877711653709412   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7072080373764038   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.7804858088493347   train_acc = 0.5   val_acc = 0.5\n",
      "Final val acc for lr=1.515914167041141e-06, wd=0.0002300292096821906: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.515914167041141e-06, wd=0.0002300292096821906, betas=(0.85, 0.995)\n",
      "iteration: 1/9   loss = 0.7546727657318115   train_acc = 0.47761194029850745   val_acc = 0.9285714285714286\n",
      "iteration: 2/9   loss = 0.6748292446136475   train_acc = 0.5223880597014925   val_acc = 0.8571428571428571\n",
      "iteration: 3/9   loss = 0.7669195532798767   train_acc = 0.5447761194029851   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.6912304759025574   train_acc = 0.48507462686567165   val_acc = 0.6428571428571429\n",
      "iteration: 5/9   loss = 0.6926450133323669   train_acc = 0.5373134328358209   val_acc = 0.7857142857142857\n",
      "iteration: 6/9   loss = 0.7851052284240723   train_acc = 0.5522388059701493   val_acc = 0.7142857142857143\n",
      "iteration: 7/9   loss = 0.7416528463363647   train_acc = 0.6044776119402985   val_acc = 0.7142857142857143\n",
      "iteration: 8/9   loss = 0.7007414102554321   train_acc = 0.5746268656716418   val_acc = 0.7142857142857143\n",
      "iteration: 9/9   loss = 0.6708519458770752   train_acc = 0.5522388059701493   val_acc = 0.7142857142857143\n",
      "Final val acc for lr=1.515914167041141e-06, wd=0.0002300292096821906: 0.7143\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.515914167041141e-06, wd=0.0002300292096821906, betas=(0.9, 0.98)\n",
      "iteration: 1/9   loss = 0.7574214935302734   train_acc = 0.5373134328358209   val_acc = 0.5714285714285714\n",
      "iteration: 2/9   loss = 0.7122668027877808   train_acc = 0.5298507462686567   val_acc = 0.5714285714285714\n",
      "iteration: 3/9   loss = 0.8224813342094421   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.712016224861145   train_acc = 0.48507462686567165   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.6897387504577637   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 6/9   loss = 1.0061978101730347   train_acc = 0.47761194029850745   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.7049338817596436   train_acc = 0.4626865671641791   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7405894994735718   train_acc = 0.5223880597014925   val_acc = 0.42857142857142855\n",
      "iteration: 9/9   loss = 0.6988734006881714   train_acc = 0.5149253731343284   val_acc = 0.35714285714285715\n",
      "Final val acc for lr=1.515914167041141e-06, wd=0.0002300292096821906: 0.3571\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.515914167041141e-06, wd=0.0002300292096821906, betas=(0.9, 0.99)\n",
      "iteration: 1/9   loss = 0.703648567199707   train_acc = 0.5522388059701493   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.728582501411438   train_acc = 0.5   val_acc = 0.35714285714285715\n",
      "iteration: 3/9   loss = 0.6093364357948303   train_acc = 0.48507462686567165   val_acc = 0.35714285714285715\n",
      "iteration: 4/9   loss = 0.6781193614006042   train_acc = 0.5   val_acc = 0.2857142857142857\n",
      "iteration: 5/9   loss = 0.7522614002227783   train_acc = 0.5298507462686567   val_acc = 0.35714285714285715\n",
      "iteration: 6/9   loss = 0.8415923118591309   train_acc = 0.5298507462686567   val_acc = 0.35714285714285715\n",
      "iteration: 7/9   loss = 0.6928701400756836   train_acc = 0.5373134328358209   val_acc = 0.35714285714285715\n",
      "iteration: 8/9   loss = 0.6910954117774963   train_acc = 0.5223880597014925   val_acc = 0.35714285714285715\n",
      "iteration: 9/9   loss = 0.6318742036819458   train_acc = 0.5298507462686567   val_acc = 0.35714285714285715\n",
      "Final val acc for lr=1.515914167041141e-06, wd=0.0002300292096821906: 0.3571\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.515914167041141e-06, wd=0.0002300292096821906, betas=(0.9, 0.995)\n",
      "iteration: 1/9   loss = 0.7319628000259399   train_acc = 0.5373134328358209   val_acc = 0.7857142857142857\n",
      "iteration: 2/9   loss = 0.6709702014923096   train_acc = 0.5   val_acc = 0.6428571428571429\n",
      "iteration: 3/9   loss = 0.731757640838623   train_acc = 0.4925373134328358   val_acc = 0.8571428571428571\n",
      "iteration: 4/9   loss = 0.7609901428222656   train_acc = 0.5298507462686567   val_acc = 0.8571428571428571\n",
      "iteration: 5/9   loss = 0.7393256425857544   train_acc = 0.5   val_acc = 0.7857142857142857\n",
      "iteration: 6/9   loss = 0.7798318862915039   train_acc = 0.5223880597014925   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.6826059818267822   train_acc = 0.5373134328358209   val_acc = 0.7142857142857143\n",
      "iteration: 8/9   loss = 0.6695912480354309   train_acc = 0.5597014925373134   val_acc = 0.9285714285714286\n",
      "iteration: 9/9   loss = 0.8384873270988464   train_acc = 0.5373134328358209   val_acc = 0.8571428571428571\n",
      "Final val acc for lr=1.515914167041141e-06, wd=0.0002300292096821906: 0.8571\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.515914167041141e-06, wd=0.0002300292096821906, betas=(0.9, 0.999)\n",
      "iteration: 1/9   loss = 0.7690796852111816   train_acc = 0.39552238805970147   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.715617299079895   train_acc = 0.4552238805970149   val_acc = 0.35714285714285715\n",
      "iteration: 3/9   loss = 0.6933068633079529   train_acc = 0.4626865671641791   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.7138266563415527   train_acc = 0.44776119402985076   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.7446793913841248   train_acc = 0.41044776119402987   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.7565681338310242   train_acc = 0.43283582089552236   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.7246333956718445   train_acc = 0.40298507462686567   val_acc = 0.6428571428571429\n",
      "iteration: 8/9   loss = 0.7269530296325684   train_acc = 0.4626865671641791   val_acc = 0.42857142857142855\n",
      "iteration: 9/9   loss = 0.7536234855651855   train_acc = 0.41044776119402987   val_acc = 0.5\n",
      "Final val acc for lr=1.515914167041141e-06, wd=0.0002300292096821906: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.515914167041141e-06, wd=0.0002300292096821906, betas=(0.95, 0.98)\n",
      "iteration: 1/9   loss = 0.7160581946372986   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.6787817478179932   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.7282044887542725   train_acc = 0.4925373134328358   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.7300609946250916   train_acc = 0.47761194029850745   val_acc = 0.5714285714285714\n",
      "iteration: 5/9   loss = 0.7614768147468567   train_acc = 0.48507462686567165   val_acc = 0.6428571428571429\n",
      "iteration: 6/9   loss = 0.7158681750297546   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.730717658996582   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7214581966400146   train_acc = 0.4925373134328358   val_acc = 0.42857142857142855\n",
      "iteration: 9/9   loss = 0.7055209875106812   train_acc = 0.44776119402985076   val_acc = 0.5\n",
      "Final val acc for lr=1.515914167041141e-06, wd=0.0002300292096821906: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.515914167041141e-06, wd=0.0002300292096821906, betas=(0.95, 0.99)\n",
      "iteration: 1/9   loss = 0.6789078712463379   train_acc = 0.4626865671641791   val_acc = 0.2857142857142857\n",
      "iteration: 2/9   loss = 0.7335867285728455   train_acc = 0.4253731343283582   val_acc = 0.2857142857142857\n",
      "iteration: 3/9   loss = 0.8024990558624268   train_acc = 0.44029850746268656   val_acc = 0.35714285714285715\n",
      "iteration: 4/9   loss = 0.6819213628768921   train_acc = 0.44776119402985076   val_acc = 0.35714285714285715\n",
      "iteration: 5/9   loss = 0.7150313854217529   train_acc = 0.43283582089552236   val_acc = 0.35714285714285715\n",
      "iteration: 6/9   loss = 0.665378212928772   train_acc = 0.4626865671641791   val_acc = 0.2857142857142857\n",
      "iteration: 7/9   loss = 0.6965845227241516   train_acc = 0.4925373134328358   val_acc = 0.35714285714285715\n",
      "iteration: 8/9   loss = 0.6905405521392822   train_acc = 0.4626865671641791   val_acc = 0.35714285714285715\n",
      "iteration: 9/9   loss = 0.8672803640365601   train_acc = 0.4253731343283582   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=1.515914167041141e-06, wd=0.0002300292096821906: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.515914167041141e-06, wd=0.0002300292096821906, betas=(0.95, 0.995)\n",
      "iteration: 1/9   loss = 0.7648544311523438   train_acc = 0.44029850746268656   val_acc = 0.2857142857142857\n",
      "iteration: 2/9   loss = 0.6905321478843689   train_acc = 0.4626865671641791   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.6533764600753784   train_acc = 0.4925373134328358   val_acc = 0.35714285714285715\n",
      "iteration: 4/9   loss = 0.7635281085968018   train_acc = 0.4701492537313433   val_acc = 0.35714285714285715\n",
      "iteration: 5/9   loss = 0.6563605070114136   train_acc = 0.4701492537313433   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.81681227684021   train_acc = 0.43283582089552236   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.7198541164398193   train_acc = 0.48507462686567165   val_acc = 0.35714285714285715\n",
      "iteration: 8/9   loss = 0.7330092191696167   train_acc = 0.4626865671641791   val_acc = 0.2857142857142857\n",
      "iteration: 9/9   loss = 0.7569625377655029   train_acc = 0.4925373134328358   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=1.515914167041141e-06, wd=0.0002300292096821906: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.515914167041141e-06, wd=0.0002300292096821906, betas=(0.95, 0.999)\n",
      "iteration: 1/9   loss = 0.7101240158081055   train_acc = 0.5149253731343284   val_acc = 0.5714285714285714\n",
      "iteration: 2/9   loss = 0.73628830909729   train_acc = 0.5074626865671642   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.7952319383621216   train_acc = 0.4925373134328358   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.7191174030303955   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7134888172149658   train_acc = 0.4701492537313433   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.7010364532470703   train_acc = 0.5373134328358209   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.7476168870925903   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7500126361846924   train_acc = 0.4925373134328358   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.7583764791488647   train_acc = 0.5   val_acc = 0.5\n",
      "Final val acc for lr=1.515914167041141e-06, wd=0.0002300292096821906: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.515914167041141e-06, wd=0.00022555743977178082, betas=(0.85, 0.99)\n",
      "iteration: 1/9   loss = 0.7615454792976379   train_acc = 0.47761194029850745   val_acc = 0.5714285714285714\n",
      "iteration: 2/9   loss = 0.7914043068885803   train_acc = 0.4626865671641791   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.6922451853752136   train_acc = 0.4626865671641791   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.755501389503479   train_acc = 0.4701492537313433   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.6784334182739258   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.5603839159011841   train_acc = 0.47761194029850745   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.7434530258178711   train_acc = 0.47761194029850745   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7191153168678284   train_acc = 0.44029850746268656   val_acc = 0.42857142857142855\n",
      "iteration: 9/9   loss = 0.7165367007255554   train_acc = 0.4552238805970149   val_acc = 0.5\n",
      "Final val acc for lr=1.515914167041141e-06, wd=0.00022555743977178082: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.515914167041141e-06, wd=0.00022555743977178082, betas=(0.85, 0.995)\n",
      "iteration: 1/9   loss = 0.7583218216896057   train_acc = 0.4925373134328358   val_acc = 0.6428571428571429\n",
      "iteration: 2/9   loss = 0.7030918598175049   train_acc = 0.5671641791044776   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.8229255676269531   train_acc = 0.5373134328358209   val_acc = 0.7142857142857143\n",
      "iteration: 4/9   loss = 0.684861958026886   train_acc = 0.5522388059701493   val_acc = 0.5714285714285714\n",
      "iteration: 5/9   loss = 0.7189189791679382   train_acc = 0.5298507462686567   val_acc = 0.5714285714285714\n",
      "iteration: 6/9   loss = 0.8103024959564209   train_acc = 0.5149253731343284   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.7020743489265442   train_acc = 0.5373134328358209   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.709383487701416   train_acc = 0.5074626865671642   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.9097824096679688   train_acc = 0.5746268656716418   val_acc = 0.7142857142857143\n",
      "Final val acc for lr=1.515914167041141e-06, wd=0.00022555743977178082: 0.7143\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.515914167041141e-06, wd=0.00022555743977178082, betas=(0.9, 0.98)\n",
      "iteration: 1/9   loss = 0.7208236455917358   train_acc = 0.48507462686567165   val_acc = 0.5714285714285714\n",
      "iteration: 2/9   loss = 0.6983808279037476   train_acc = 0.44029850746268656   val_acc = 0.6428571428571429\n",
      "iteration: 3/9   loss = 0.6346325874328613   train_acc = 0.4552238805970149   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.7103478312492371   train_acc = 0.47761194029850745   val_acc = 0.6428571428571429\n",
      "iteration: 5/9   loss = 0.7074109315872192   train_acc = 0.4552238805970149   val_acc = 0.7857142857142857\n",
      "iteration: 6/9   loss = 0.7620261907577515   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.729926586151123   train_acc = 0.4701492537313433   val_acc = 0.6428571428571429\n",
      "iteration: 8/9   loss = 0.6683881282806396   train_acc = 0.4925373134328358   val_acc = 0.7142857142857143\n",
      "iteration: 9/9   loss = 0.6334215402603149   train_acc = 0.48507462686567165   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=1.515914167041141e-06, wd=0.00022555743977178082: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.515914167041141e-06, wd=0.00022555743977178082, betas=(0.9, 0.99)\n",
      "iteration: 1/9   loss = 0.7688134908676147   train_acc = 0.39552238805970147   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7362488508224487   train_acc = 0.417910447761194   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.8662670850753784   train_acc = 0.417910447761194   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.7109498977661133   train_acc = 0.417910447761194   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7643358111381531   train_acc = 0.44029850746268656   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.5814570784568787   train_acc = 0.4253731343283582   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.7136343717575073   train_acc = 0.39552238805970147   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.779515266418457   train_acc = 0.4253731343283582   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.8486860990524292   train_acc = 0.4253731343283582   val_acc = 0.5\n",
      "Final val acc for lr=1.515914167041141e-06, wd=0.00022555743977178082: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.515914167041141e-06, wd=0.00022555743977178082, betas=(0.9, 0.995)\n",
      "iteration: 1/9   loss = 0.7240966558456421   train_acc = 0.4626865671641791   val_acc = 0.6428571428571429\n",
      "iteration: 2/9   loss = 0.7034362554550171   train_acc = 0.47761194029850745   val_acc = 0.5714285714285714\n",
      "iteration: 3/9   loss = 0.587960958480835   train_acc = 0.4701492537313433   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.7213641405105591   train_acc = 0.5074626865671642   val_acc = 0.5714285714285714\n",
      "iteration: 5/9   loss = 0.7118050456047058   train_acc = 0.47761194029850745   val_acc = 0.5714285714285714\n",
      "iteration: 6/9   loss = 0.6913795471191406   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7433842420578003   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7009248733520508   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.7250069379806519   train_acc = 0.4626865671641791   val_acc = 0.5714285714285714\n",
      "Final val acc for lr=1.515914167041141e-06, wd=0.00022555743977178082: 0.5714\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.515914167041141e-06, wd=0.00022555743977178082, betas=(0.9, 0.999)\n",
      "iteration: 1/9   loss = 0.7153204679489136   train_acc = 0.48507462686567165   val_acc = 0.2857142857142857\n",
      "iteration: 2/9   loss = 0.7544229030609131   train_acc = 0.48507462686567165   val_acc = 0.14285714285714285\n",
      "iteration: 3/9   loss = 0.6168782711029053   train_acc = 0.5074626865671642   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.7488877773284912   train_acc = 0.47761194029850745   val_acc = 0.35714285714285715\n",
      "iteration: 5/9   loss = 0.7064104080200195   train_acc = 0.47761194029850745   val_acc = 0.35714285714285715\n",
      "iteration: 6/9   loss = 0.6028976440429688   train_acc = 0.47761194029850745   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.7322995662689209   train_acc = 0.5149253731343284   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.6889090538024902   train_acc = 0.47761194029850745   val_acc = 0.35714285714285715\n",
      "iteration: 9/9   loss = 0.9144285917282104   train_acc = 0.47761194029850745   val_acc = 0.35714285714285715\n",
      "Final val acc for lr=1.515914167041141e-06, wd=0.00022555743977178082: 0.3571\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.515914167041141e-06, wd=0.00022555743977178082, betas=(0.95, 0.98)\n",
      "iteration: 1/9   loss = 0.7052797079086304   train_acc = 0.5298507462686567   val_acc = 0.7857142857142857\n",
      "iteration: 2/9   loss = 0.7496607303619385   train_acc = 0.5597014925373134   val_acc = 0.7857142857142857\n",
      "iteration: 3/9   loss = 0.7129859328269958   train_acc = 0.5149253731343284   val_acc = 0.7142857142857143\n",
      "iteration: 4/9   loss = 0.6756194829940796   train_acc = 0.4925373134328358   val_acc = 0.8571428571428571\n",
      "iteration: 5/9   loss = 0.7180664539337158   train_acc = 0.5373134328358209   val_acc = 0.7857142857142857\n",
      "iteration: 6/9   loss = 0.7757353782653809   train_acc = 0.48507462686567165   val_acc = 0.8571428571428571\n",
      "iteration: 7/9   loss = 0.7190271019935608   train_acc = 0.5447761194029851   val_acc = 0.7142857142857143\n",
      "iteration: 8/9   loss = 0.7812364101409912   train_acc = 0.5298507462686567   val_acc = 0.7857142857142857\n",
      "iteration: 9/9   loss = 0.7141554355621338   train_acc = 0.5597014925373134   val_acc = 0.7142857142857143\n",
      "Final val acc for lr=1.515914167041141e-06, wd=0.00022555743977178082: 0.7143\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.515914167041141e-06, wd=0.00022555743977178082, betas=(0.95, 0.99)\n",
      "iteration: 1/9   loss = 0.7250686883926392   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7514331340789795   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.7012012004852295   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7337297201156616   train_acc = 0.4701492537313433   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.7430626749992371   train_acc = 0.5   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.6765875816345215   train_acc = 0.4925373134328358   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.6979168653488159   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7618509531021118   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.769372820854187   train_acc = 0.47761194029850745   val_acc = 0.5\n",
      "Final val acc for lr=1.515914167041141e-06, wd=0.00022555743977178082: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.515914167041141e-06, wd=0.00022555743977178082, betas=(0.95, 0.995)\n",
      "iteration: 1/9   loss = 0.6833047866821289   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.6974978446960449   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.8594791889190674   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7185646891593933   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.6880306005477905   train_acc = 0.5074626865671642   val_acc = 0.5714285714285714\n",
      "iteration: 6/9   loss = 0.7849151492118835   train_acc = 0.5074626865671642   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.6973057985305786   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7549341917037964   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.6908189654350281   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "Final val acc for lr=1.515914167041141e-06, wd=0.00022555743977178082: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.515914167041141e-06, wd=0.00022555743977178082, betas=(0.95, 0.999)\n",
      "iteration: 1/9   loss = 0.7424399852752686   train_acc = 0.5223880597014925   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7597159147262573   train_acc = 0.5522388059701493   val_acc = 0.5714285714285714\n",
      "iteration: 3/9   loss = 0.7835167646408081   train_acc = 0.5447761194029851   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.6880418062210083   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.743328869342804   train_acc = 0.5298507462686567   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.7154064178466797   train_acc = 0.5298507462686567   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.7236155867576599   train_acc = 0.5373134328358209   val_acc = 0.7142857142857143\n",
      "iteration: 8/9   loss = 0.7347861528396606   train_acc = 0.5074626865671642   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.8408201932907104   train_acc = 0.5298507462686567   val_acc = 0.5\n",
      "Final val acc for lr=1.515914167041141e-06, wd=0.00022555743977178082: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.515914167041141e-06, wd=0.00011822143451838108, betas=(0.85, 0.99)\n",
      "iteration: 1/9   loss = 0.718217134475708   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.704533576965332   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.8750559687614441   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7256395816802979   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7715017199516296   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.6677411794662476   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7333536148071289   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7187226414680481   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.8321561813354492   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "Final val acc for lr=1.515914167041141e-06, wd=0.00011822143451838108: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.515914167041141e-06, wd=0.00011822143451838108, betas=(0.85, 0.995)\n",
      "iteration: 1/9   loss = 0.7225809693336487   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7814351916313171   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.6599429249763489   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.697873055934906   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7605390548706055   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.7883819937705994   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7740471363067627   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7885070443153381   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.7924842834472656   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "Final val acc for lr=1.515914167041141e-06, wd=0.00011822143451838108: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.515914167041141e-06, wd=0.00011822143451838108, betas=(0.9, 0.98)\n",
      "iteration: 1/9   loss = 0.6878651976585388   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7030201554298401   train_acc = 0.4925373134328358   val_acc = 0.5714285714285714\n",
      "iteration: 3/9   loss = 0.8960794806480408   train_acc = 0.5671641791044776   val_acc = 0.6428571428571429\n",
      "iteration: 4/9   loss = 0.7154441475868225   train_acc = 0.5447761194029851   val_acc = 0.5714285714285714\n",
      "iteration: 5/9   loss = 0.7326209545135498   train_acc = 0.48507462686567165   val_acc = 0.6428571428571429\n",
      "iteration: 6/9   loss = 0.7271664142608643   train_acc = 0.44776119402985076   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.7044320702552795   train_acc = 0.48507462686567165   val_acc = 0.7142857142857143\n",
      "iteration: 8/9   loss = 0.7332873344421387   train_acc = 0.5223880597014925   val_acc = 0.7142857142857143\n",
      "iteration: 9/9   loss = 0.719838559627533   train_acc = 0.48507462686567165   val_acc = 0.5714285714285714\n",
      "Final val acc for lr=1.515914167041141e-06, wd=0.00011822143451838108: 0.5714\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.515914167041141e-06, wd=0.00011822143451838108, betas=(0.9, 0.99)\n",
      "iteration: 1/9   loss = 0.715965986251831   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7065533399581909   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.6546911597251892   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.728916585445404   train_acc = 0.5074626865671642   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.7792223691940308   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.5758572220802307   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7511310577392578   train_acc = 0.48507462686567165   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.7150254249572754   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.6857836246490479   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "Final val acc for lr=1.515914167041141e-06, wd=0.00011822143451838108: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.515914167041141e-06, wd=0.00011822143451838108, betas=(0.9, 0.995)\n",
      "iteration: 1/9   loss = 0.6718798875808716   train_acc = 0.5223880597014925   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.6925644874572754   train_acc = 0.44029850746268656   val_acc = 0.5714285714285714\n",
      "iteration: 3/9   loss = 0.5701032280921936   train_acc = 0.43283582089552236   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.7733689546585083   train_acc = 0.4552238805970149   val_acc = 0.5714285714285714\n",
      "iteration: 5/9   loss = 0.7229365706443787   train_acc = 0.44776119402985076   val_acc = 0.5714285714285714\n",
      "iteration: 6/9   loss = 0.8278719186782837   train_acc = 0.43283582089552236   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7613584995269775   train_acc = 0.4626865671641791   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.7192578315734863   train_acc = 0.4701492537313433   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.8882224559783936   train_acc = 0.4552238805970149   val_acc = 0.35714285714285715\n",
      "Final val acc for lr=1.515914167041141e-06, wd=0.00011822143451838108: 0.3571\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.515914167041141e-06, wd=0.00011822143451838108, betas=(0.9, 0.999)\n",
      "iteration: 1/9   loss = 0.7619678378105164   train_acc = 0.48507462686567165   val_acc = 0.5714285714285714\n",
      "iteration: 2/9   loss = 0.7941070795059204   train_acc = 0.5298507462686567   val_acc = 0.6428571428571429\n",
      "iteration: 3/9   loss = 0.6677560806274414   train_acc = 0.48507462686567165   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.6976314783096313   train_acc = 0.4925373134328358   val_acc = 0.7142857142857143\n",
      "iteration: 5/9   loss = 0.7369104623794556   train_acc = 0.47761194029850745   val_acc = 0.5714285714285714\n",
      "iteration: 6/9   loss = 0.7283213138580322   train_acc = 0.5074626865671642   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.7004026174545288   train_acc = 0.47761194029850745   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.7289915084838867   train_acc = 0.4552238805970149   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.6960180997848511   train_acc = 0.4552238805970149   val_acc = 0.6428571428571429\n",
      "Final val acc for lr=1.515914167041141e-06, wd=0.00011822143451838108: 0.6429\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.515914167041141e-06, wd=0.00011822143451838108, betas=(0.95, 0.98)\n",
      "iteration: 1/9   loss = 0.7279932498931885   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7170035243034363   train_acc = 0.582089552238806   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.6501988768577576   train_acc = 0.5447761194029851   val_acc = 0.6428571428571429\n",
      "iteration: 4/9   loss = 0.7247179746627808   train_acc = 0.4925373134328358   val_acc = 0.5714285714285714\n",
      "iteration: 5/9   loss = 0.70493483543396   train_acc = 0.5298507462686567   val_acc = 0.35714285714285715\n",
      "iteration: 6/9   loss = 0.728855311870575   train_acc = 0.5223880597014925   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.7356958985328674   train_acc = 0.5149253731343284   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.6866893768310547   train_acc = 0.5149253731343284   val_acc = 0.42857142857142855\n",
      "iteration: 9/9   loss = 0.7099051475524902   train_acc = 0.5074626865671642   val_acc = 0.6428571428571429\n",
      "Final val acc for lr=1.515914167041141e-06, wd=0.00011822143451838108: 0.6429\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.515914167041141e-06, wd=0.00011822143451838108, betas=(0.95, 0.99)\n",
      "iteration: 1/9   loss = 0.7612130641937256   train_acc = 0.5970149253731343   val_acc = 0.6428571428571429\n",
      "iteration: 2/9   loss = 0.6595640778541565   train_acc = 0.5447761194029851   val_acc = 0.6428571428571429\n",
      "iteration: 3/9   loss = 0.6790508031845093   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7085071802139282   train_acc = 0.5671641791044776   val_acc = 0.6428571428571429\n",
      "iteration: 5/9   loss = 0.7104905843734741   train_acc = 0.5373134328358209   val_acc = 0.5714285714285714\n",
      "iteration: 6/9   loss = 0.7528488636016846   train_acc = 0.5746268656716418   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.7160124778747559   train_acc = 0.5671641791044776   val_acc = 0.7142857142857143\n",
      "iteration: 8/9   loss = 0.6824923157691956   train_acc = 0.5298507462686567   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.6692957878112793   train_acc = 0.5447761194029851   val_acc = 0.5714285714285714\n",
      "Final val acc for lr=1.515914167041141e-06, wd=0.00011822143451838108: 0.5714\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.515914167041141e-06, wd=0.00011822143451838108, betas=(0.95, 0.995)\n",
      "iteration: 1/9   loss = 0.7740728259086609   train_acc = 0.5597014925373134   val_acc = 0.35714285714285715\n",
      "iteration: 2/9   loss = 0.7259140014648438   train_acc = 0.5597014925373134   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.857439398765564   train_acc = 0.5373134328358209   val_acc = 0.21428571428571427\n",
      "iteration: 4/9   loss = 0.6941088438034058   train_acc = 0.5522388059701493   val_acc = 0.14285714285714285\n",
      "iteration: 5/9   loss = 0.7043689489364624   train_acc = 0.5671641791044776   val_acc = 0.14285714285714285\n",
      "iteration: 6/9   loss = 0.5573866367340088   train_acc = 0.5597014925373134   val_acc = 0.2857142857142857\n",
      "iteration: 7/9   loss = 0.6909144520759583   train_acc = 0.5970149253731343   val_acc = 0.14285714285714285\n",
      "iteration: 8/9   loss = 0.7428651452064514   train_acc = 0.5597014925373134   val_acc = 0.21428571428571427\n",
      "iteration: 9/9   loss = 0.8061575293540955   train_acc = 0.6194029850746269   val_acc = 0.35714285714285715\n",
      "Final val acc for lr=1.515914167041141e-06, wd=0.00011822143451838108: 0.3571\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.515914167041141e-06, wd=0.00011822143451838108, betas=(0.95, 0.999)\n",
      "iteration: 1/9   loss = 0.6934912204742432   train_acc = 0.4925373134328358   val_acc = 0.5714285714285714\n",
      "iteration: 2/9   loss = 0.68816077709198   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.7871774435043335   train_acc = 0.5074626865671642   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.7428432703018188   train_acc = 0.5074626865671642   val_acc = 0.5714285714285714\n",
      "iteration: 5/9   loss = 0.6979688405990601   train_acc = 0.5223880597014925   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.713566780090332   train_acc = 0.5298507462686567   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7303018569946289   train_acc = 0.5074626865671642   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.7082679867744446   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.7776274085044861   train_acc = 0.5447761194029851   val_acc = 0.5714285714285714\n",
      "Final val acc for lr=1.515914167041141e-06, wd=0.00011822143451838108: 0.5714\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.515914167041141e-06, wd=0.00032736198184168545, betas=(0.85, 0.99)\n",
      "iteration: 1/9   loss = 0.7607965469360352   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.729397177696228   train_acc = 0.48507462686567165   val_acc = 0.5714285714285714\n",
      "iteration: 3/9   loss = 0.5260711908340454   train_acc = 0.5298507462686567   val_acc = 0.2857142857142857\n",
      "iteration: 4/9   loss = 0.6927764415740967   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7268895506858826   train_acc = 0.5149253731343284   val_acc = 0.35714285714285715\n",
      "iteration: 6/9   loss = 0.9842538833618164   train_acc = 0.47761194029850745   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.7807259559631348   train_acc = 0.5149253731343284   val_acc = 0.35714285714285715\n",
      "iteration: 8/9   loss = 0.7313989400863647   train_acc = 0.5298507462686567   val_acc = 0.42857142857142855\n",
      "iteration: 9/9   loss = 0.7262101173400879   train_acc = 0.5298507462686567   val_acc = 0.5\n",
      "Final val acc for lr=1.515914167041141e-06, wd=0.00032736198184168545: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.515914167041141e-06, wd=0.00032736198184168545, betas=(0.85, 0.995)\n",
      "iteration: 1/9   loss = 0.7522684335708618   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.8281804323196411   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.763521134853363   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7645219564437866   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7733030319213867   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.8185297846794128   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7269008159637451   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7579295635223389   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.6881367564201355   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "Final val acc for lr=1.515914167041141e-06, wd=0.00032736198184168545: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.515914167041141e-06, wd=0.00032736198184168545, betas=(0.9, 0.98)\n",
      "iteration: 1/9   loss = 0.7725735902786255   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7219284772872925   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.6651745438575745   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7060655951499939   train_acc = 0.48507462686567165   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.8438190221786499   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.6297595500946045   train_acc = 0.47761194029850745   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.700406014919281   train_acc = 0.5074626865671642   val_acc = 0.35714285714285715\n",
      "iteration: 8/9   loss = 0.7917246222496033   train_acc = 0.4925373134328358   val_acc = 0.42857142857142855\n",
      "iteration: 9/9   loss = 0.744897723197937   train_acc = 0.4925373134328358   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=1.515914167041141e-06, wd=0.00032736198184168545: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.515914167041141e-06, wd=0.00032736198184168545, betas=(0.9, 0.99)\n",
      "iteration: 1/9   loss = 0.683056116104126   train_acc = 0.5597014925373134   val_acc = 0.5714285714285714\n",
      "iteration: 2/9   loss = 0.7111316919326782   train_acc = 0.5671641791044776   val_acc = 0.6428571428571429\n",
      "iteration: 3/9   loss = 0.6764786243438721   train_acc = 0.5597014925373134   val_acc = 0.6428571428571429\n",
      "iteration: 4/9   loss = 0.7373827695846558   train_acc = 0.5671641791044776   val_acc = 0.5714285714285714\n",
      "iteration: 5/9   loss = 0.7084975242614746   train_acc = 0.5970149253731343   val_acc = 0.6428571428571429\n",
      "iteration: 6/9   loss = 0.6920196413993835   train_acc = 0.5970149253731343   val_acc = 0.6428571428571429\n",
      "iteration: 7/9   loss = 0.7107996344566345   train_acc = 0.5895522388059702   val_acc = 0.6428571428571429\n",
      "iteration: 8/9   loss = 0.738141655921936   train_acc = 0.5895522388059702   val_acc = 0.7142857142857143\n",
      "iteration: 9/9   loss = 0.6784607172012329   train_acc = 0.5447761194029851   val_acc = 0.6428571428571429\n",
      "Final val acc for lr=1.515914167041141e-06, wd=0.00032736198184168545: 0.6429\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.515914167041141e-06, wd=0.00032736198184168545, betas=(0.9, 0.995)\n",
      "iteration: 1/9   loss = 0.6816352009773254   train_acc = 0.5373134328358209   val_acc = 0.6428571428571429\n",
      "iteration: 2/9   loss = 0.7034136652946472   train_acc = 0.5373134328358209   val_acc = 0.6428571428571429\n",
      "iteration: 3/9   loss = 0.729906439781189   train_acc = 0.5597014925373134   val_acc = 0.7142857142857143\n",
      "iteration: 4/9   loss = 0.6866554021835327   train_acc = 0.5223880597014925   val_acc = 0.6428571428571429\n",
      "iteration: 5/9   loss = 0.7377668619155884   train_acc = 0.5746268656716418   val_acc = 0.6428571428571429\n",
      "iteration: 6/9   loss = 0.8586164712905884   train_acc = 0.5522388059701493   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7304850816726685   train_acc = 0.5373134328358209   val_acc = 0.6428571428571429\n",
      "iteration: 8/9   loss = 0.7021417021751404   train_acc = 0.5746268656716418   val_acc = 0.6428571428571429\n",
      "iteration: 9/9   loss = 0.7775653600692749   train_acc = 0.5522388059701493   val_acc = 0.7142857142857143\n",
      "Final val acc for lr=1.515914167041141e-06, wd=0.00032736198184168545: 0.7143\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.515914167041141e-06, wd=0.00032736198184168545, betas=(0.9, 0.999)\n",
      "iteration: 1/9   loss = 0.7350428104400635   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7762177586555481   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.806348979473114   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7553341388702393   train_acc = 0.5074626865671642   val_acc = 0.5714285714285714\n",
      "iteration: 5/9   loss = 0.6862374544143677   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.6771587133407593   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7080527544021606   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.769080400466919   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.7826287150382996   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "Final val acc for lr=1.515914167041141e-06, wd=0.00032736198184168545: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.515914167041141e-06, wd=0.00032736198184168545, betas=(0.95, 0.98)\n",
      "iteration: 1/9   loss = 0.7237802743911743   train_acc = 0.4626865671641791   val_acc = 0.35714285714285715\n",
      "iteration: 2/9   loss = 0.7474306225776672   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.7137970924377441   train_acc = 0.5074626865671642   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.7126778364181519   train_acc = 0.48507462686567165   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.8076877593994141   train_acc = 0.4925373134328358   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.5472614765167236   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7543752193450928   train_acc = 0.5   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.7429831027984619   train_acc = 0.48507462686567165   val_acc = 0.42857142857142855\n",
      "iteration: 9/9   loss = 0.5534381866455078   train_acc = 0.5   val_acc = 0.2857142857142857\n",
      "Final val acc for lr=1.515914167041141e-06, wd=0.00032736198184168545: 0.2857\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.515914167041141e-06, wd=0.00032736198184168545, betas=(0.95, 0.99)\n",
      "iteration: 1/9   loss = 0.745714008808136   train_acc = 0.5298507462686567   val_acc = 0.35714285714285715\n",
      "iteration: 2/9   loss = 0.6945493221282959   train_acc = 0.5223880597014925   val_acc = 0.2857142857142857\n",
      "iteration: 3/9   loss = 0.7277274131774902   train_acc = 0.4925373134328358   val_acc = 0.2857142857142857\n",
      "iteration: 4/9   loss = 0.7226177453994751   train_acc = 0.48507462686567165   val_acc = 0.35714285714285715\n",
      "iteration: 5/9   loss = 0.7135871648788452   train_acc = 0.5447761194029851   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.8417035341262817   train_acc = 0.5223880597014925   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7239359617233276   train_acc = 0.4701492537313433   val_acc = 0.2857142857142857\n",
      "iteration: 8/9   loss = 0.6808499693870544   train_acc = 0.48507462686567165   val_acc = 0.35714285714285715\n",
      "iteration: 9/9   loss = 0.6152219176292419   train_acc = 0.5223880597014925   val_acc = 0.2857142857142857\n",
      "Final val acc for lr=1.515914167041141e-06, wd=0.00032736198184168545: 0.2857\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.515914167041141e-06, wd=0.00032736198184168545, betas=(0.95, 0.995)\n",
      "iteration: 1/9   loss = 0.7321985960006714   train_acc = 0.47761194029850745   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7406463027000427   train_acc = 0.4925373134328358   val_acc = 0.5714285714285714\n",
      "iteration: 3/9   loss = 0.8842467665672302   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7265224456787109   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7603054642677307   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.7239747047424316   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7495245337486267   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.6882033348083496   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.6657466888427734   train_acc = 0.5   val_acc = 0.5\n",
      "Final val acc for lr=1.515914167041141e-06, wd=0.00032736198184168545: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.515914167041141e-06, wd=0.00032736198184168545, betas=(0.95, 0.999)\n",
      "iteration: 1/9   loss = 0.7439256906509399   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.6523273587226868   train_acc = 0.4925373134328358   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.7117305994033813   train_acc = 0.44776119402985076   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.7271338701248169   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7437794208526611   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.8243594169616699   train_acc = 0.48507462686567165   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.709976315498352   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7089182138442993   train_acc = 0.43283582089552236   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.6354176998138428   train_acc = 0.5447761194029851   val_acc = 0.7142857142857143\n",
      "Final val acc for lr=1.515914167041141e-06, wd=0.00032736198184168545: 0.7143\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.6664867868002596e-05, wd=0.0004609389889268166, betas=(0.85, 0.99)\n",
      "iteration: 1/9   loss = 0.7275038957595825   train_acc = 0.5223880597014925   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7256696224212646   train_acc = 0.5298507462686567   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.71781986951828   train_acc = 0.5895522388059702   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.6707345247268677   train_acc = 0.5597014925373134   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.7095766067504883   train_acc = 0.6343283582089553   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.7644634246826172   train_acc = 0.6044776119402985   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.743098258972168   train_acc = 0.6268656716417911   val_acc = 0.35714285714285715\n",
      "iteration: 8/9   loss = 0.6896188259124756   train_acc = 0.6791044776119403   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.5816152691841125   train_acc = 0.6417910447761194   val_acc = 0.5\n",
      "Final val acc for lr=1.6664867868002596e-05, wd=0.0004609389889268166: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.6664867868002596e-05, wd=0.0004609389889268166, betas=(0.85, 0.995)\n",
      "iteration: 1/9   loss = 0.6941462755203247   train_acc = 0.5074626865671642   val_acc = 0.6428571428571429\n",
      "iteration: 2/9   loss = 0.709159791469574   train_acc = 0.5373134328358209   val_acc = 0.5714285714285714\n",
      "iteration: 3/9   loss = 0.7042859792709351   train_acc = 0.5447761194029851   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.756720244884491   train_acc = 0.5597014925373134   val_acc = 0.6428571428571429\n",
      "iteration: 5/9   loss = 0.7204912900924683   train_acc = 0.5298507462686567   val_acc = 0.5714285714285714\n",
      "iteration: 6/9   loss = 0.6633397340774536   train_acc = 0.5447761194029851   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.7254413366317749   train_acc = 0.5671641791044776   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.7265093326568604   train_acc = 0.5522388059701493   val_acc = 0.6428571428571429\n",
      "iteration: 9/9   loss = 0.6852819323539734   train_acc = 0.5895522388059702   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=1.6664867868002596e-05, wd=0.0004609389889268166: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.6664867868002596e-05, wd=0.0004609389889268166, betas=(0.9, 0.98)\n",
      "iteration: 1/9   loss = 0.7136116623878479   train_acc = 0.6492537313432836   val_acc = 0.7142857142857143\n",
      "iteration: 2/9   loss = 0.6601402759552002   train_acc = 0.6119402985074627   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.8068339228630066   train_acc = 0.6492537313432836   val_acc = 0.6428571428571429\n",
      "iteration: 4/9   loss = 0.7031514644622803   train_acc = 0.6268656716417911   val_acc = 0.5714285714285714\n",
      "iteration: 5/9   loss = 0.6847829818725586   train_acc = 0.5746268656716418   val_acc = 0.5714285714285714\n",
      "iteration: 6/9   loss = 0.6236180663108826   train_acc = 0.6119402985074627   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.6751807928085327   train_acc = 0.6343283582089553   val_acc = 0.7142857142857143\n",
      "iteration: 8/9   loss = 0.7461770176887512   train_acc = 0.6417910447761194   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.6745823621749878   train_acc = 0.6268656716417911   val_acc = 0.5714285714285714\n",
      "Final val acc for lr=1.6664867868002596e-05, wd=0.0004609389889268166: 0.5714\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.6664867868002596e-05, wd=0.0004609389889268166, betas=(0.9, 0.99)\n",
      "iteration: 1/9   loss = 0.7540867328643799   train_acc = 0.4925373134328358   val_acc = 0.6428571428571429\n",
      "iteration: 2/9   loss = 0.6964564919471741   train_acc = 0.5447761194029851   val_acc = 0.14285714285714285\n",
      "iteration: 3/9   loss = 0.7043949961662292   train_acc = 0.5149253731343284   val_acc = 0.2857142857142857\n",
      "iteration: 4/9   loss = 0.7179813385009766   train_acc = 0.5298507462686567   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.7589818239212036   train_acc = 0.5522388059701493   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.680144190788269   train_acc = 0.6119402985074627   val_acc = 0.35714285714285715\n",
      "iteration: 7/9   loss = 0.7472226619720459   train_acc = 0.6417910447761194   val_acc = 0.35714285714285715\n",
      "iteration: 8/9   loss = 0.6917504668235779   train_acc = 0.6194029850746269   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.8615081310272217   train_acc = 0.5895522388059702   val_acc = 0.5\n",
      "Final val acc for lr=1.6664867868002596e-05, wd=0.0004609389889268166: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.6664867868002596e-05, wd=0.0004609389889268166, betas=(0.9, 0.995)\n",
      "iteration: 1/9   loss = 0.702769935131073   train_acc = 0.6119402985074627   val_acc = 0.6428571428571429\n",
      "iteration: 2/9   loss = 0.7291362285614014   train_acc = 0.6417910447761194   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.7055797576904297   train_acc = 0.6343283582089553   val_acc = 0.35714285714285715\n",
      "iteration: 4/9   loss = 0.7057958841323853   train_acc = 0.6119402985074627   val_acc = 0.35714285714285715\n",
      "iteration: 5/9   loss = 0.7271161079406738   train_acc = 0.6492537313432836   val_acc = 0.6428571428571429\n",
      "iteration: 6/9   loss = 0.6305542588233948   train_acc = 0.6417910447761194   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.6929669380187988   train_acc = 0.6119402985074627   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.722998857498169   train_acc = 0.6194029850746269   val_acc = 0.42857142857142855\n",
      "iteration: 9/9   loss = 0.7337881922721863   train_acc = 0.6492537313432836   val_acc = 0.5\n",
      "Final val acc for lr=1.6664867868002596e-05, wd=0.0004609389889268166: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.6664867868002596e-05, wd=0.0004609389889268166, betas=(0.9, 0.999)\n",
      "iteration: 1/9   loss = 0.7496474385261536   train_acc = 0.5223880597014925   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.7196311950683594   train_acc = 0.5298507462686567   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.7315329909324646   train_acc = 0.5522388059701493   val_acc = 0.35714285714285715\n",
      "iteration: 4/9   loss = 0.7061665058135986   train_acc = 0.5298507462686567   val_acc = 0.35714285714285715\n",
      "iteration: 5/9   loss = 0.6664984226226807   train_acc = 0.5074626865671642   val_acc = 0.35714285714285715\n",
      "iteration: 6/9   loss = 0.7504597902297974   train_acc = 0.5522388059701493   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7471750378608704   train_acc = 0.5074626865671642   val_acc = 0.35714285714285715\n",
      "iteration: 8/9   loss = 0.6866465210914612   train_acc = 0.5597014925373134   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.7831976413726807   train_acc = 0.5522388059701493   val_acc = 0.35714285714285715\n",
      "Final val acc for lr=1.6664867868002596e-05, wd=0.0004609389889268166: 0.3571\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.6664867868002596e-05, wd=0.0004609389889268166, betas=(0.95, 0.98)\n",
      "iteration: 1/9   loss = 0.7468134164810181   train_acc = 0.5522388059701493   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.7358965277671814   train_acc = 0.5522388059701493   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.654218316078186   train_acc = 0.5149253731343284   val_acc = 0.2857142857142857\n",
      "iteration: 4/9   loss = 0.7150566577911377   train_acc = 0.5895522388059702   val_acc = 0.35714285714285715\n",
      "iteration: 5/9   loss = 0.7033941745758057   train_acc = 0.5597014925373134   val_acc = 0.2857142857142857\n",
      "iteration: 6/9   loss = 0.8256918787956238   train_acc = 0.5671641791044776   val_acc = 0.2857142857142857\n",
      "iteration: 7/9   loss = 0.7425222396850586   train_acc = 0.5746268656716418   val_acc = 0.35714285714285715\n",
      "iteration: 8/9   loss = 0.6761826872825623   train_acc = 0.5373134328358209   val_acc = 0.2857142857142857\n",
      "iteration: 9/9   loss = 0.5417305827140808   train_acc = 0.5970149253731343   val_acc = 0.35714285714285715\n",
      "Final val acc for lr=1.6664867868002596e-05, wd=0.0004609389889268166: 0.3571\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.6664867868002596e-05, wd=0.0004609389889268166, betas=(0.95, 0.99)\n",
      "iteration: 1/9   loss = 0.8039408922195435   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.6853910684585571   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.7662533521652222   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7670453190803528   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7234302759170532   train_acc = 0.5223880597014925   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.8213790655136108   train_acc = 0.5373134328358209   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.6748136281967163   train_acc = 0.5298507462686567   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.7584681510925293   train_acc = 0.5298507462686567   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.7124079465866089   train_acc = 0.5298507462686567   val_acc = 0.5\n",
      "Final val acc for lr=1.6664867868002596e-05, wd=0.0004609389889268166: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.6664867868002596e-05, wd=0.0004609389889268166, betas=(0.95, 0.995)\n",
      "iteration: 1/9   loss = 0.795021653175354   train_acc = 0.5522388059701493   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7426258325576782   train_acc = 0.5597014925373134   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.7531919479370117   train_acc = 0.5671641791044776   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.708907425403595   train_acc = 0.5895522388059702   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.7806394100189209   train_acc = 0.5671641791044776   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.872156023979187   train_acc = 0.582089552238806   val_acc = 0.35714285714285715\n",
      "iteration: 7/9   loss = 0.7415434122085571   train_acc = 0.5895522388059702   val_acc = 0.35714285714285715\n",
      "iteration: 8/9   loss = 0.6925745010375977   train_acc = 0.5970149253731343   val_acc = 0.2857142857142857\n",
      "iteration: 9/9   loss = 0.6638327836990356   train_acc = 0.5746268656716418   val_acc = 0.5\n",
      "Final val acc for lr=1.6664867868002596e-05, wd=0.0004609389889268166: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.6664867868002596e-05, wd=0.0004609389889268166, betas=(0.95, 0.999)\n",
      "iteration: 1/9   loss = 0.6824703812599182   train_acc = 0.5447761194029851   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.6767477989196777   train_acc = 0.5298507462686567   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.6826338171958923   train_acc = 0.6119402985074627   val_acc = 0.35714285714285715\n",
      "iteration: 4/9   loss = 0.7149712443351746   train_acc = 0.5970149253731343   val_acc = 0.2857142857142857\n",
      "iteration: 5/9   loss = 0.6778604984283447   train_acc = 0.5970149253731343   val_acc = 0.21428571428571427\n",
      "iteration: 6/9   loss = 0.7664763331413269   train_acc = 0.5895522388059702   val_acc = 0.2857142857142857\n",
      "iteration: 7/9   loss = 0.7087721824645996   train_acc = 0.5895522388059702   val_acc = 0.21428571428571427\n",
      "iteration: 8/9   loss = 0.710379421710968   train_acc = 0.5671641791044776   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.6799758672714233   train_acc = 0.6044776119402985   val_acc = 0.35714285714285715\n",
      "Final val acc for lr=1.6664867868002596e-05, wd=0.0004609389889268166: 0.3571\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.6664867868002596e-05, wd=0.0002300292096821906, betas=(0.85, 0.99)\n",
      "iteration: 1/9   loss = 0.7089812755584717   train_acc = 0.5   val_acc = 0.21428571428571427\n",
      "iteration: 2/9   loss = 0.703214168548584   train_acc = 0.5223880597014925   val_acc = 0.21428571428571427\n",
      "iteration: 3/9   loss = 0.7312571406364441   train_acc = 0.5223880597014925   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.7480747103691101   train_acc = 0.4701492537313433   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.6938897371292114   train_acc = 0.5223880597014925   val_acc = 0.35714285714285715\n",
      "iteration: 6/9   loss = 0.7599450945854187   train_acc = 0.5149253731343284   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.7302359342575073   train_acc = 0.5149253731343284   val_acc = 0.2857142857142857\n",
      "iteration: 8/9   loss = 0.6637541651725769   train_acc = 0.5522388059701493   val_acc = 0.2857142857142857\n",
      "iteration: 9/9   loss = 0.6911250352859497   train_acc = 0.5298507462686567   val_acc = 0.2857142857142857\n",
      "Final val acc for lr=1.6664867868002596e-05, wd=0.0002300292096821906: 0.2857\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.6664867868002596e-05, wd=0.0002300292096821906, betas=(0.85, 0.995)\n",
      "iteration: 1/9   loss = 0.7203012704849243   train_acc = 0.5223880597014925   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7021948099136353   train_acc = 0.5447761194029851   val_acc = 0.5714285714285714\n",
      "iteration: 3/9   loss = 0.5972175598144531   train_acc = 0.5746268656716418   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.7005334496498108   train_acc = 0.5597014925373134   val_acc = 0.5714285714285714\n",
      "iteration: 5/9   loss = 0.6960172057151794   train_acc = 0.5597014925373134   val_acc = 0.7142857142857143\n",
      "iteration: 6/9   loss = 0.9317030310630798   train_acc = 0.6194029850746269   val_acc = 0.7142857142857143\n",
      "iteration: 7/9   loss = 0.6949676275253296   train_acc = 0.5746268656716418   val_acc = 0.7142857142857143\n",
      "iteration: 8/9   loss = 0.7031311392784119   train_acc = 0.6194029850746269   val_acc = 0.7857142857142857\n",
      "iteration: 9/9   loss = 0.6161590814590454   train_acc = 0.6044776119402985   val_acc = 0.7142857142857143\n",
      "Final val acc for lr=1.6664867868002596e-05, wd=0.0002300292096821906: 0.7143\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.6664867868002596e-05, wd=0.0002300292096821906, betas=(0.9, 0.98)\n",
      "iteration: 1/9   loss = 0.696188747882843   train_acc = 0.5895522388059702   val_acc = 0.7142857142857143\n",
      "iteration: 2/9   loss = 0.6974932551383972   train_acc = 0.5895522388059702   val_acc = 0.7142857142857143\n",
      "iteration: 3/9   loss = 0.6919812560081482   train_acc = 0.6268656716417911   val_acc = 0.7857142857142857\n",
      "iteration: 4/9   loss = 0.7106819152832031   train_acc = 0.5746268656716418   val_acc = 0.9285714285714286\n",
      "iteration: 5/9   loss = 0.6336476802825928   train_acc = 0.6716417910447762   val_acc = 0.8571428571428571\n",
      "iteration: 6/9   loss = 0.6303806900978088   train_acc = 0.5746268656716418   val_acc = 0.6428571428571429\n",
      "iteration: 7/9   loss = 0.7115927934646606   train_acc = 0.5970149253731343   val_acc = 0.6428571428571429\n",
      "iteration: 8/9   loss = 0.6457333564758301   train_acc = 0.6194029850746269   val_acc = 0.7857142857142857\n",
      "iteration: 9/9   loss = 0.6555593013763428   train_acc = 0.5970149253731343   val_acc = 0.7857142857142857\n",
      "Final val acc for lr=1.6664867868002596e-05, wd=0.0002300292096821906: 0.7857\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.6664867868002596e-05, wd=0.0002300292096821906, betas=(0.9, 0.99)\n",
      "iteration: 1/9   loss = 0.6999639868736267   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7485374808311462   train_acc = 0.4552238805970149   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.69603031873703   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.709702730178833   train_acc = 0.5447761194029851   val_acc = 0.5714285714285714\n",
      "iteration: 5/9   loss = 0.6646679639816284   train_acc = 0.5223880597014925   val_acc = 0.5714285714285714\n",
      "iteration: 6/9   loss = 0.7539069056510925   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7113139033317566   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.6908861994743347   train_acc = 0.5597014925373134   val_acc = 0.7142857142857143\n",
      "iteration: 9/9   loss = 0.6598595380783081   train_acc = 0.5895522388059702   val_acc = 0.5714285714285714\n",
      "Final val acc for lr=1.6664867868002596e-05, wd=0.0002300292096821906: 0.5714\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.6664867868002596e-05, wd=0.0002300292096821906, betas=(0.9, 0.995)\n",
      "iteration: 1/9   loss = 0.7264997959136963   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7121148109436035   train_acc = 0.5597014925373134   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.8038109540939331   train_acc = 0.5522388059701493   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.7379153370857239   train_acc = 0.5522388059701493   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.706091046333313   train_acc = 0.5895522388059702   val_acc = 0.5714285714285714\n",
      "iteration: 6/9   loss = 0.5416629314422607   train_acc = 0.5373134328358209   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.63339763879776   train_acc = 0.6343283582089553   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.730724573135376   train_acc = 0.6044776119402985   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.6513866782188416   train_acc = 0.5671641791044776   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=1.6664867868002596e-05, wd=0.0002300292096821906: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.6664867868002596e-05, wd=0.0002300292096821906, betas=(0.9, 0.999)\n",
      "iteration: 1/9   loss = 0.6967349648475647   train_acc = 0.5298507462686567   val_acc = 0.5714285714285714\n",
      "iteration: 2/9   loss = 0.7429916858673096   train_acc = 0.5223880597014925   val_acc = 0.7142857142857143\n",
      "iteration: 3/9   loss = 0.5522257089614868   train_acc = 0.6119402985074627   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.703954815864563   train_acc = 0.5746268656716418   val_acc = 0.5714285714285714\n",
      "iteration: 5/9   loss = 0.702751874923706   train_acc = 0.6194029850746269   val_acc = 0.6428571428571429\n",
      "iteration: 6/9   loss = 0.9691241979598999   train_acc = 0.5895522388059702   val_acc = 0.6428571428571429\n",
      "iteration: 7/9   loss = 0.7045694589614868   train_acc = 0.5671641791044776   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.6903181076049805   train_acc = 0.6268656716417911   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.8158416748046875   train_acc = 0.6417910447761194   val_acc = 0.5714285714285714\n",
      "Final val acc for lr=1.6664867868002596e-05, wd=0.0002300292096821906: 0.5714\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.6664867868002596e-05, wd=0.0002300292096821906, betas=(0.95, 0.98)\n",
      "iteration: 1/9   loss = 0.6909143924713135   train_acc = 0.5   val_acc = 0.35714285714285715\n",
      "iteration: 2/9   loss = 0.7272274494171143   train_acc = 0.5074626865671642   val_acc = 0.35714285714285715\n",
      "iteration: 3/9   loss = 0.7949585914611816   train_acc = 0.47761194029850745   val_acc = 0.35714285714285715\n",
      "iteration: 4/9   loss = 0.7287163734436035   train_acc = 0.44029850746268656   val_acc = 0.2857142857142857\n",
      "iteration: 5/9   loss = 0.6953761577606201   train_acc = 0.47761194029850745   val_acc = 0.2857142857142857\n",
      "iteration: 6/9   loss = 0.766119122505188   train_acc = 0.5074626865671642   val_acc = 0.21428571428571427\n",
      "iteration: 7/9   loss = 0.7110744118690491   train_acc = 0.4925373134328358   val_acc = 0.21428571428571427\n",
      "iteration: 8/9   loss = 0.704078197479248   train_acc = 0.4701492537313433   val_acc = 0.21428571428571427\n",
      "iteration: 9/9   loss = 0.8358159065246582   train_acc = 0.5149253731343284   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=1.6664867868002596e-05, wd=0.0002300292096821906: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.6664867868002596e-05, wd=0.0002300292096821906, betas=(0.95, 0.99)\n",
      "iteration: 1/9   loss = 0.6966699361801147   train_acc = 0.5074626865671642   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.6949124336242676   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.6835846900939941   train_acc = 0.5074626865671642   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.7569551467895508   train_acc = 0.5447761194029851   val_acc = 0.35714285714285715\n",
      "iteration: 5/9   loss = 0.6791205406188965   train_acc = 0.5522388059701493   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.6777782440185547   train_acc = 0.5597014925373134   val_acc = 0.6428571428571429\n",
      "iteration: 7/9   loss = 0.7070452570915222   train_acc = 0.582089552238806   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.7378221154212952   train_acc = 0.5522388059701493   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.7682403326034546   train_acc = 0.6119402985074627   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=1.6664867868002596e-05, wd=0.0002300292096821906: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.6664867868002596e-05, wd=0.0002300292096821906, betas=(0.95, 0.995)\n",
      "iteration: 1/9   loss = 0.7299730777740479   train_acc = 0.5522388059701493   val_acc = 0.5714285714285714\n",
      "iteration: 2/9   loss = 0.7174654603004456   train_acc = 0.5373134328358209   val_acc = 0.5714285714285714\n",
      "iteration: 3/9   loss = 0.676217257976532   train_acc = 0.6044776119402985   val_acc = 0.7142857142857143\n",
      "iteration: 4/9   loss = 0.6806357502937317   train_acc = 0.5970149253731343   val_acc = 0.5714285714285714\n",
      "iteration: 5/9   loss = 0.7385251522064209   train_acc = 0.5671641791044776   val_acc = 0.6428571428571429\n",
      "iteration: 6/9   loss = 0.6523416042327881   train_acc = 0.5970149253731343   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.6973538398742676   train_acc = 0.6044776119402985   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.6669765710830688   train_acc = 0.5970149253731343   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.7431086301803589   train_acc = 0.5746268656716418   val_acc = 0.5714285714285714\n",
      "Final val acc for lr=1.6664867868002596e-05, wd=0.0002300292096821906: 0.5714\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.6664867868002596e-05, wd=0.0002300292096821906, betas=(0.95, 0.999)\n",
      "iteration: 1/9   loss = 0.7749834656715393   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7331705689430237   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.6068587303161621   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7496049404144287   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.6728192567825317   train_acc = 0.47761194029850745   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.6775015592575073   train_acc = 0.4925373134328358   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.7457325458526611   train_acc = 0.5   val_acc = 0.6428571428571429\n",
      "iteration: 8/9   loss = 0.7256191968917847   train_acc = 0.5298507462686567   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.7305135726928711   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "Final val acc for lr=1.6664867868002596e-05, wd=0.0002300292096821906: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.6664867868002596e-05, wd=0.00022555743977178082, betas=(0.85, 0.99)\n",
      "iteration: 1/9   loss = 0.7359428405761719   train_acc = 0.5149253731343284   val_acc = 0.35714285714285715\n",
      "iteration: 2/9   loss = 0.7361986637115479   train_acc = 0.5522388059701493   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.736102283000946   train_acc = 0.582089552238806   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7159460783004761   train_acc = 0.5373134328358209   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7097008228302002   train_acc = 0.582089552238806   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.5347145795822144   train_acc = 0.5522388059701493   val_acc = 0.6428571428571429\n",
      "iteration: 7/9   loss = 0.6769598722457886   train_acc = 0.6268656716417911   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.6708825826644897   train_acc = 0.6194029850746269   val_acc = 0.42857142857142855\n",
      "iteration: 9/9   loss = 0.6815902590751648   train_acc = 0.6417910447761194   val_acc = 0.5714285714285714\n",
      "Final val acc for lr=1.6664867868002596e-05, wd=0.00022555743977178082: 0.5714\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.6664867868002596e-05, wd=0.00022555743977178082, betas=(0.85, 0.995)\n",
      "iteration: 1/9   loss = 0.7182788848876953   train_acc = 0.5   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.7700892686843872   train_acc = 0.4925373134328358   val_acc = 0.5714285714285714\n",
      "iteration: 3/9   loss = 0.7191598415374756   train_acc = 0.4925373134328358   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.7821178436279297   train_acc = 0.5597014925373134   val_acc = 0.2857142857142857\n",
      "iteration: 5/9   loss = 0.6930608749389648   train_acc = 0.5223880597014925   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.8787156939506531   train_acc = 0.5373134328358209   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7023921608924866   train_acc = 0.5671641791044776   val_acc = 0.6428571428571429\n",
      "iteration: 8/9   loss = 0.7662351727485657   train_acc = 0.5447761194029851   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.7263681292533875   train_acc = 0.5746268656716418   val_acc = 0.5\n",
      "Final val acc for lr=1.6664867868002596e-05, wd=0.00022555743977178082: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.6664867868002596e-05, wd=0.00022555743977178082, betas=(0.9, 0.98)\n",
      "iteration: 1/9   loss = 0.6814563274383545   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.8305336833000183   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.8116130828857422   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7552188634872437   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7827054262161255   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.6113157868385315   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7188220024108887   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7422761917114258   train_acc = 0.5223880597014925   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.7670807838439941   train_acc = 0.5298507462686567   val_acc = 0.5\n",
      "Final val acc for lr=1.6664867868002596e-05, wd=0.00022555743977178082: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.6664867868002596e-05, wd=0.00022555743977178082, betas=(0.9, 0.99)\n",
      "iteration: 1/9   loss = 0.7820678949356079   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7263802289962769   train_acc = 0.47761194029850745   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.7877821922302246   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.794134259223938   train_acc = 0.47761194029850745   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7328637838363647   train_acc = 0.4701492537313433   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.6762310266494751   train_acc = 0.4626865671641791   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7470607757568359   train_acc = 0.47761194029850745   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7491044998168945   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.7338818311691284   train_acc = 0.44776119402985076   val_acc = 0.35714285714285715\n",
      "Final val acc for lr=1.6664867868002596e-05, wd=0.00022555743977178082: 0.3571\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.6664867868002596e-05, wd=0.00022555743977178082, betas=(0.9, 0.995)\n",
      "iteration: 1/9   loss = 0.7793952226638794   train_acc = 0.5298507462686567   val_acc = 0.5714285714285714\n",
      "iteration: 2/9   loss = 0.6617420315742493   train_acc = 0.5597014925373134   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.6365597248077393   train_acc = 0.5895522388059702   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.69923996925354   train_acc = 0.6343283582089553   val_acc = 0.7142857142857143\n",
      "iteration: 5/9   loss = 0.67266845703125   train_acc = 0.6194029850746269   val_acc = 0.7857142857142857\n",
      "iteration: 6/9   loss = 0.7279402017593384   train_acc = 0.6119402985074627   val_acc = 0.8571428571428571\n",
      "iteration: 7/9   loss = 0.7080385684967041   train_acc = 0.6119402985074627   val_acc = 0.7142857142857143\n",
      "iteration: 8/9   loss = 0.6610544919967651   train_acc = 0.6567164179104478   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.6813980340957642   train_acc = 0.6194029850746269   val_acc = 0.5\n",
      "Final val acc for lr=1.6664867868002596e-05, wd=0.00022555743977178082: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.6664867868002596e-05, wd=0.00022555743977178082, betas=(0.9, 0.999)\n",
      "iteration: 1/9   loss = 0.7440230846405029   train_acc = 0.5970149253731343   val_acc = 0.5714285714285714\n",
      "iteration: 2/9   loss = 0.6839022040367126   train_acc = 0.5671641791044776   val_acc = 0.6428571428571429\n",
      "iteration: 3/9   loss = 0.6772429347038269   train_acc = 0.5895522388059702   val_acc = 0.6428571428571429\n",
      "iteration: 4/9   loss = 0.6541755199432373   train_acc = 0.5895522388059702   val_acc = 0.5714285714285714\n",
      "iteration: 5/9   loss = 0.7034090161323547   train_acc = 0.6343283582089553   val_acc = 0.6428571428571429\n",
      "iteration: 6/9   loss = 0.6313920021057129   train_acc = 0.5970149253731343   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.7177901864051819   train_acc = 0.6119402985074627   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.7438074350357056   train_acc = 0.6268656716417911   val_acc = 0.42857142857142855\n",
      "iteration: 9/9   loss = 0.6446071863174438   train_acc = 0.6119402985074627   val_acc = 0.5714285714285714\n",
      "Final val acc for lr=1.6664867868002596e-05, wd=0.00022555743977178082: 0.5714\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.6664867868002596e-05, wd=0.00022555743977178082, betas=(0.95, 0.98)\n",
      "iteration: 1/9   loss = 0.7268383502960205   train_acc = 0.5149253731343284   val_acc = 0.7142857142857143\n",
      "iteration: 2/9   loss = 0.7546593546867371   train_acc = 0.5298507462686567   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.6628386974334717   train_acc = 0.5298507462686567   val_acc = 0.7142857142857143\n",
      "iteration: 4/9   loss = 0.7496297359466553   train_acc = 0.5522388059701493   val_acc = 0.5714285714285714\n",
      "iteration: 5/9   loss = 0.7240231037139893   train_acc = 0.5597014925373134   val_acc = 0.5714285714285714\n",
      "iteration: 6/9   loss = 0.8063792586326599   train_acc = 0.582089552238806   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.6968908309936523   train_acc = 0.5447761194029851   val_acc = 0.6428571428571429\n",
      "iteration: 8/9   loss = 0.6683923602104187   train_acc = 0.582089552238806   val_acc = 0.7142857142857143\n",
      "iteration: 9/9   loss = 0.861021876335144   train_acc = 0.6119402985074627   val_acc = 0.7142857142857143\n",
      "Final val acc for lr=1.6664867868002596e-05, wd=0.00022555743977178082: 0.7143\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.6664867868002596e-05, wd=0.00022555743977178082, betas=(0.95, 0.99)\n",
      "iteration: 1/9   loss = 0.6779015064239502   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7413102388381958   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.8692973852157593   train_acc = 0.5373134328358209   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7392921447753906   train_acc = 0.5223880597014925   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7378919124603271   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.5489026308059692   train_acc = 0.5298507462686567   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.6850138306617737   train_acc = 0.5746268656716418   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.6929370164871216   train_acc = 0.5522388059701493   val_acc = 0.42857142857142855\n",
      "iteration: 9/9   loss = 0.5602357387542725   train_acc = 0.5671641791044776   val_acc = 0.5714285714285714\n",
      "Final val acc for lr=1.6664867868002596e-05, wd=0.00022555743977178082: 0.5714\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.6664867868002596e-05, wd=0.00022555743977178082, betas=(0.95, 0.995)\n",
      "iteration: 1/9   loss = 0.7392126321792603   train_acc = 0.43283582089552236   val_acc = 0.35714285714285715\n",
      "iteration: 2/9   loss = 0.7545645236968994   train_acc = 0.44776119402985076   val_acc = 0.35714285714285715\n",
      "iteration: 3/9   loss = 0.6742720603942871   train_acc = 0.4701492537313433   val_acc = 0.35714285714285715\n",
      "iteration: 4/9   loss = 0.7496927976608276   train_acc = 0.5223880597014925   val_acc = 0.5714285714285714\n",
      "iteration: 5/9   loss = 0.7353370189666748   train_acc = 0.4925373134328358   val_acc = 0.35714285714285715\n",
      "iteration: 6/9   loss = 0.6155440807342529   train_acc = 0.5223880597014925   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.7142883539199829   train_acc = 0.5746268656716418   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.6879804134368896   train_acc = 0.5522388059701493   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.6374160051345825   train_acc = 0.5597014925373134   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=1.6664867868002596e-05, wd=0.00022555743977178082: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.6664867868002596e-05, wd=0.00022555743977178082, betas=(0.95, 0.999)\n",
      "iteration: 1/9   loss = 0.7983283400535583   train_acc = 0.4925373134328358   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.7206557989120483   train_acc = 0.5223880597014925   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.93352210521698   train_acc = 0.4925373134328358   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.7118076682090759   train_acc = 0.4626865671641791   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.7924407720565796   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.9984287023544312   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.6875858902931213   train_acc = 0.5373134328358209   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7499212026596069   train_acc = 0.5373134328358209   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.8150763511657715   train_acc = 0.5746268656716418   val_acc = 0.5714285714285714\n",
      "Final val acc for lr=1.6664867868002596e-05, wd=0.00022555743977178082: 0.5714\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.6664867868002596e-05, wd=0.00011822143451838108, betas=(0.85, 0.99)\n",
      "iteration: 1/9   loss = 0.7197138667106628   train_acc = 0.48507462686567165   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.7641025185585022   train_acc = 0.5223880597014925   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.694614827632904   train_acc = 0.4925373134328358   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.7181073427200317   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7118690013885498   train_acc = 0.48507462686567165   val_acc = 0.35714285714285715\n",
      "iteration: 6/9   loss = 0.6959211826324463   train_acc = 0.5373134328358209   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.6680230498313904   train_acc = 0.5   val_acc = 0.35714285714285715\n",
      "iteration: 8/9   loss = 0.6698599457740784   train_acc = 0.5074626865671642   val_acc = 0.42857142857142855\n",
      "iteration: 9/9   loss = 0.6536924242973328   train_acc = 0.5223880597014925   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=1.6664867868002596e-05, wd=0.00011822143451838108: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.6664867868002596e-05, wd=0.00011822143451838108, betas=(0.85, 0.995)\n",
      "iteration: 1/9   loss = 0.7145035266876221   train_acc = 0.5447761194029851   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.7234251499176025   train_acc = 0.5671641791044776   val_acc = 0.5714285714285714\n",
      "iteration: 3/9   loss = 0.7366244196891785   train_acc = 0.6044776119402985   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.6704037189483643   train_acc = 0.6119402985074627   val_acc = 0.5714285714285714\n",
      "iteration: 5/9   loss = 0.7104429006576538   train_acc = 0.5522388059701493   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.712509274482727   train_acc = 0.6194029850746269   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7067400217056274   train_acc = 0.5895522388059702   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.6932123899459839   train_acc = 0.6268656716417911   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.6142959594726562   train_acc = 0.5895522388059702   val_acc = 0.5714285714285714\n",
      "Final val acc for lr=1.6664867868002596e-05, wd=0.00011822143451838108: 0.5714\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.6664867868002596e-05, wd=0.00011822143451838108, betas=(0.9, 0.98)\n",
      "iteration: 1/9   loss = 0.7098870277404785   train_acc = 0.5149253731343284   val_acc = 0.5714285714285714\n",
      "iteration: 2/9   loss = 0.7813477516174316   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.7567488551139832   train_acc = 0.5447761194029851   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.6789995431900024   train_acc = 0.5671641791044776   val_acc = 0.5714285714285714\n",
      "iteration: 5/9   loss = 0.6952537894248962   train_acc = 0.5522388059701493   val_acc = 0.7142857142857143\n",
      "iteration: 6/9   loss = 0.7520418167114258   train_acc = 0.5895522388059702   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.6445234417915344   train_acc = 0.5597014925373134   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.660443902015686   train_acc = 0.5970149253731343   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.5984193086624146   train_acc = 0.5970149253731343   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=1.6664867868002596e-05, wd=0.00011822143451838108: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.6664867868002596e-05, wd=0.00011822143451838108, betas=(0.9, 0.99)\n",
      "iteration: 1/9   loss = 0.6800971031188965   train_acc = 0.5597014925373134   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7167584896087646   train_acc = 0.5671641791044776   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.6082854270935059   train_acc = 0.5970149253731343   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7489229440689087   train_acc = 0.6268656716417911   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.6811432838439941   train_acc = 0.6194029850746269   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.6617822647094727   train_acc = 0.582089552238806   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.7354269027709961   train_acc = 0.6268656716417911   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.6732219457626343   train_acc = 0.6343283582089553   val_acc = 0.42857142857142855\n",
      "iteration: 9/9   loss = 0.6378228068351746   train_acc = 0.6194029850746269   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=1.6664867868002596e-05, wd=0.00011822143451838108: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.6664867868002596e-05, wd=0.00011822143451838108, betas=(0.9, 0.995)\n",
      "iteration: 1/9   loss = 0.7219612002372742   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7603163719177246   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.7421897649765015   train_acc = 0.4925373134328358   val_acc = 0.6428571428571429\n",
      "iteration: 4/9   loss = 0.6948276162147522   train_acc = 0.5522388059701493   val_acc = 0.6428571428571429\n",
      "iteration: 5/9   loss = 0.7403367161750793   train_acc = 0.5149253731343284   val_acc = 0.6428571428571429\n",
      "iteration: 6/9   loss = 0.808368444442749   train_acc = 0.5149253731343284   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.7227732539176941   train_acc = 0.5223880597014925   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7218450307846069   train_acc = 0.5298507462686567   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.735764741897583   train_acc = 0.5447761194029851   val_acc = 0.5714285714285714\n",
      "Final val acc for lr=1.6664867868002596e-05, wd=0.00011822143451838108: 0.5714\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.6664867868002596e-05, wd=0.00011822143451838108, betas=(0.9, 0.999)\n",
      "iteration: 1/9   loss = 0.7525995373725891   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7302566170692444   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.636494517326355   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7229364514350891   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.6471554040908813   train_acc = 0.5223880597014925   val_acc = 0.6428571428571429\n",
      "iteration: 6/9   loss = 0.6444668769836426   train_acc = 0.5373134328358209   val_acc = 0.6428571428571429\n",
      "iteration: 7/9   loss = 0.6639055013656616   train_acc = 0.5447761194029851   val_acc = 0.6428571428571429\n",
      "iteration: 8/9   loss = 0.7647565603256226   train_acc = 0.5149253731343284   val_acc = 0.7142857142857143\n",
      "iteration: 9/9   loss = 0.6386109590530396   train_acc = 0.5223880597014925   val_acc = 0.7142857142857143\n",
      "Final val acc for lr=1.6664867868002596e-05, wd=0.00011822143451838108: 0.7143\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.6664867868002596e-05, wd=0.00011822143451838108, betas=(0.95, 0.98)\n",
      "iteration: 1/9   loss = 0.6597684621810913   train_acc = 0.5671641791044776   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.6570818424224854   train_acc = 0.6044776119402985   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.7925704717636108   train_acc = 0.5746268656716418   val_acc = 0.6428571428571429\n",
      "iteration: 4/9   loss = 0.7084823846817017   train_acc = 0.6044776119402985   val_acc = 0.6428571428571429\n",
      "iteration: 5/9   loss = 0.6979274749755859   train_acc = 0.6119402985074627   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.689595639705658   train_acc = 0.5895522388059702   val_acc = 0.35714285714285715\n",
      "iteration: 7/9   loss = 0.6827012300491333   train_acc = 0.6343283582089553   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.7105764746665955   train_acc = 0.5597014925373134   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.645564615726471   train_acc = 0.5597014925373134   val_acc = 0.5\n",
      "Final val acc for lr=1.6664867868002596e-05, wd=0.00011822143451838108: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.6664867868002596e-05, wd=0.00011822143451838108, betas=(0.95, 0.99)\n",
      "iteration: 1/9   loss = 0.8436718583106995   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.697934091091156   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.7804300785064697   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7200464606285095   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7377371788024902   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.7614242434501648   train_acc = 0.5597014925373134   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7879247665405273   train_acc = 0.5447761194029851   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7350888252258301   train_acc = 0.582089552238806   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.7196440100669861   train_acc = 0.5895522388059702   val_acc = 0.5714285714285714\n",
      "Final val acc for lr=1.6664867868002596e-05, wd=0.00011822143451838108: 0.5714\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.6664867868002596e-05, wd=0.00011822143451838108, betas=(0.95, 0.995)\n",
      "iteration: 1/9   loss = 0.7337179780006409   train_acc = 0.4925373134328358   val_acc = 0.5714285714285714\n",
      "iteration: 2/9   loss = 0.7028822898864746   train_acc = 0.5   val_acc = 0.5714285714285714\n",
      "iteration: 3/9   loss = 0.5352532267570496   train_acc = 0.5   val_acc = 0.6428571428571429\n",
      "iteration: 4/9   loss = 0.6960810422897339   train_acc = 0.5298507462686567   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.6891803741455078   train_acc = 0.5149253731343284   val_acc = 0.5714285714285714\n",
      "iteration: 6/9   loss = 0.8073693513870239   train_acc = 0.5447761194029851   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.6792040467262268   train_acc = 0.5373134328358209   val_acc = 0.35714285714285715\n",
      "iteration: 8/9   loss = 0.7513566017150879   train_acc = 0.5522388059701493   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.616041362285614   train_acc = 0.582089552238806   val_acc = 0.5714285714285714\n",
      "Final val acc for lr=1.6664867868002596e-05, wd=0.00011822143451838108: 0.5714\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.6664867868002596e-05, wd=0.00011822143451838108, betas=(0.95, 0.999)\n",
      "iteration: 1/9   loss = 0.713520348072052   train_acc = 0.5746268656716418   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7214102745056152   train_acc = 0.5447761194029851   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.8753092288970947   train_acc = 0.5223880597014925   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7124698162078857   train_acc = 0.5746268656716418   val_acc = 0.35714285714285715\n",
      "iteration: 5/9   loss = 0.6937305927276611   train_acc = 0.5895522388059702   val_acc = 0.35714285714285715\n",
      "iteration: 6/9   loss = 0.5532007813453674   train_acc = 0.5970149253731343   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.7085195779800415   train_acc = 0.582089552238806   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.6947466135025024   train_acc = 0.6044776119402985   val_acc = 0.42857142857142855\n",
      "iteration: 9/9   loss = 0.8254811763763428   train_acc = 0.5895522388059702   val_acc = 0.5714285714285714\n",
      "Final val acc for lr=1.6664867868002596e-05, wd=0.00011822143451838108: 0.5714\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.6664867868002596e-05, wd=0.00032736198184168545, betas=(0.85, 0.99)\n",
      "iteration: 1/9   loss = 0.7323647737503052   train_acc = 0.47761194029850745   val_acc = 0.6428571428571429\n",
      "iteration: 2/9   loss = 0.7387819886207581   train_acc = 0.5447761194029851   val_acc = 0.5714285714285714\n",
      "iteration: 3/9   loss = 0.71930330991745   train_acc = 0.5373134328358209   val_acc = 0.6428571428571429\n",
      "iteration: 4/9   loss = 0.6729083061218262   train_acc = 0.6119402985074627   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7146403193473816   train_acc = 0.6044776119402985   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.7019646167755127   train_acc = 0.5671641791044776   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.7001322507858276   train_acc = 0.582089552238806   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.6836031675338745   train_acc = 0.5746268656716418   val_acc = 0.6428571428571429\n",
      "iteration: 9/9   loss = 0.6069389581680298   train_acc = 0.6268656716417911   val_acc = 0.5714285714285714\n",
      "Final val acc for lr=1.6664867868002596e-05, wd=0.00032736198184168545: 0.5714\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.6664867868002596e-05, wd=0.00032736198184168545, betas=(0.85, 0.995)\n",
      "iteration: 1/9   loss = 0.7204843163490295   train_acc = 0.5   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.7520135641098022   train_acc = 0.43283582089552236   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.739220142364502   train_acc = 0.48507462686567165   val_acc = 0.35714285714285715\n",
      "iteration: 4/9   loss = 0.7338367700576782   train_acc = 0.4925373134328358   val_acc = 0.5714285714285714\n",
      "iteration: 5/9   loss = 0.7415326833724976   train_acc = 0.4701492537313433   val_acc = 0.35714285714285715\n",
      "iteration: 6/9   loss = 0.6858224868774414   train_acc = 0.5223880597014925   val_acc = 0.21428571428571427\n",
      "iteration: 7/9   loss = 0.6747144460678101   train_acc = 0.5522388059701493   val_acc = 0.35714285714285715\n",
      "iteration: 8/9   loss = 0.710537314414978   train_acc = 0.5597014925373134   val_acc = 0.42857142857142855\n",
      "iteration: 9/9   loss = 0.691382110118866   train_acc = 0.5671641791044776   val_acc = 0.35714285714285715\n",
      "Final val acc for lr=1.6664867868002596e-05, wd=0.00032736198184168545: 0.3571\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.6664867868002596e-05, wd=0.00032736198184168545, betas=(0.9, 0.98)\n",
      "iteration: 1/9   loss = 0.761164128780365   train_acc = 0.4925373134328358   val_acc = 0.5714285714285714\n",
      "iteration: 2/9   loss = 0.7214789390563965   train_acc = 0.5   val_acc = 0.5714285714285714\n",
      "iteration: 3/9   loss = 0.7683684229850769   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7453818917274475   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7007341384887695   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.627652645111084   train_acc = 0.5447761194029851   val_acc = 0.6428571428571429\n",
      "iteration: 7/9   loss = 0.6942933201789856   train_acc = 0.5373134328358209   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7147116661071777   train_acc = 0.582089552238806   val_acc = 0.7142857142857143\n",
      "iteration: 9/9   loss = 0.6966991424560547   train_acc = 0.582089552238806   val_acc = 0.7142857142857143\n",
      "Final val acc for lr=1.6664867868002596e-05, wd=0.00032736198184168545: 0.7143\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.6664867868002596e-05, wd=0.00032736198184168545, betas=(0.9, 0.99)\n",
      "iteration: 1/9   loss = 0.7275065183639526   train_acc = 0.4925373134328358   val_acc = 0.6428571428571429\n",
      "iteration: 2/9   loss = 0.7180777788162231   train_acc = 0.4552238805970149   val_acc = 0.6428571428571429\n",
      "iteration: 3/9   loss = 0.6214227676391602   train_acc = 0.44776119402985076   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.6791330575942993   train_acc = 0.5223880597014925   val_acc = 0.5714285714285714\n",
      "iteration: 5/9   loss = 0.7470718622207642   train_acc = 0.5223880597014925   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.7087416052818298   train_acc = 0.44776119402985076   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.6904253959655762   train_acc = 0.5298507462686567   val_acc = 0.6428571428571429\n",
      "iteration: 8/9   loss = 0.6978428363800049   train_acc = 0.4925373134328358   val_acc = 0.6428571428571429\n",
      "iteration: 9/9   loss = 0.6445455551147461   train_acc = 0.5074626865671642   val_acc = 0.5714285714285714\n",
      "Final val acc for lr=1.6664867868002596e-05, wd=0.00032736198184168545: 0.5714\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.6664867868002596e-05, wd=0.00032736198184168545, betas=(0.9, 0.995)\n",
      "iteration: 1/9   loss = 0.6824243068695068   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7830886840820312   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.7536373138427734   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7348774671554565   train_acc = 0.5074626865671642   val_acc = 0.5714285714285714\n",
      "iteration: 5/9   loss = 0.7049970626831055   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.49970826506614685   train_acc = 0.582089552238806   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.6673146486282349   train_acc = 0.5522388059701493   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.7706454992294312   train_acc = 0.5522388059701493   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.7077681422233582   train_acc = 0.5746268656716418   val_acc = 0.2857142857142857\n",
      "Final val acc for lr=1.6664867868002596e-05, wd=0.00032736198184168545: 0.2857\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.6664867868002596e-05, wd=0.00032736198184168545, betas=(0.9, 0.999)\n",
      "iteration: 1/9   loss = 0.7222276926040649   train_acc = 0.5074626865671642   val_acc = 0.35714285714285715\n",
      "iteration: 2/9   loss = 0.775910496711731   train_acc = 0.5074626865671642   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.8319104909896851   train_acc = 0.4925373134328358   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.687880277633667   train_acc = 0.5223880597014925   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.7674239873886108   train_acc = 0.5746268656716418   val_acc = 0.35714285714285715\n",
      "iteration: 6/9   loss = 0.6576592326164246   train_acc = 0.4701492537313433   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7356348633766174   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7157125473022461   train_acc = 0.5522388059701493   val_acc = 0.42857142857142855\n",
      "iteration: 9/9   loss = 0.647524893283844   train_acc = 0.5447761194029851   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=1.6664867868002596e-05, wd=0.00032736198184168545: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.6664867868002596e-05, wd=0.00032736198184168545, betas=(0.95, 0.98)\n",
      "iteration: 1/9   loss = 0.7585210800170898   train_acc = 0.5373134328358209   val_acc = 0.35714285714285715\n",
      "iteration: 2/9   loss = 0.725082516670227   train_acc = 0.5447761194029851   val_acc = 0.2857142857142857\n",
      "iteration: 3/9   loss = 0.8268190622329712   train_acc = 0.5   val_acc = 0.2857142857142857\n",
      "iteration: 4/9   loss = 0.6856600046157837   train_acc = 0.5522388059701493   val_acc = 0.35714285714285715\n",
      "iteration: 5/9   loss = 0.6954879760742188   train_acc = 0.5074626865671642   val_acc = 0.35714285714285715\n",
      "iteration: 6/9   loss = 0.6481589078903198   train_acc = 0.5223880597014925   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.7383382320404053   train_acc = 0.5447761194029851   val_acc = 0.35714285714285715\n",
      "iteration: 8/9   loss = 0.7229407429695129   train_acc = 0.5597014925373134   val_acc = 0.35714285714285715\n",
      "iteration: 9/9   loss = 0.8042399287223816   train_acc = 0.5746268656716418   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=1.6664867868002596e-05, wd=0.00032736198184168545: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.6664867868002596e-05, wd=0.00032736198184168545, betas=(0.95, 0.99)\n",
      "iteration: 1/9   loss = 0.7775720953941345   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7312489748001099   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.4960719048976898   train_acc = 0.5074626865671642   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.6619783043861389   train_acc = 0.5223880597014925   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7892060875892639   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.721106231212616   train_acc = 0.5373134328358209   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7130637168884277   train_acc = 0.5671641791044776   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.7017739415168762   train_acc = 0.5671641791044776   val_acc = 0.42857142857142855\n",
      "iteration: 9/9   loss = 0.5822872519493103   train_acc = 0.5746268656716418   val_acc = 0.35714285714285715\n",
      "Final val acc for lr=1.6664867868002596e-05, wd=0.00032736198184168545: 0.3571\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.6664867868002596e-05, wd=0.00032736198184168545, betas=(0.95, 0.995)\n",
      "iteration: 1/9   loss = 0.7688483595848083   train_acc = 0.47761194029850745   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.698815107345581   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.8201531171798706   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.6805758476257324   train_acc = 0.5   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.6932308077812195   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.6472095251083374   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.6686590313911438   train_acc = 0.4925373134328358   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.730255126953125   train_acc = 0.5447761194029851   val_acc = 0.42857142857142855\n",
      "iteration: 9/9   loss = 0.7238869667053223   train_acc = 0.5298507462686567   val_acc = 0.5\n",
      "Final val acc for lr=1.6664867868002596e-05, wd=0.00032736198184168545: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=1.6664867868002596e-05, wd=0.00032736198184168545, betas=(0.95, 0.999)\n",
      "iteration: 1/9   loss = 0.7097088098526001   train_acc = 0.582089552238806   val_acc = 0.2857142857142857\n",
      "iteration: 2/9   loss = 0.7091547250747681   train_acc = 0.5746268656716418   val_acc = 0.2857142857142857\n",
      "iteration: 3/9   loss = 0.6375678181648254   train_acc = 0.582089552238806   val_acc = 0.35714285714285715\n",
      "iteration: 4/9   loss = 0.6922347545623779   train_acc = 0.5597014925373134   val_acc = 0.2857142857142857\n",
      "iteration: 5/9   loss = 0.7408558130264282   train_acc = 0.5671641791044776   val_acc = 0.21428571428571427\n",
      "iteration: 6/9   loss = 0.6085304021835327   train_acc = 0.5895522388059702   val_acc = 0.35714285714285715\n",
      "iteration: 7/9   loss = 0.6680851578712463   train_acc = 0.5895522388059702   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.6536379456520081   train_acc = 0.5895522388059702   val_acc = 0.2857142857142857\n",
      "iteration: 9/9   loss = 0.6708362102508545   train_acc = 0.6492537313432836   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=1.6664867868002596e-05, wd=0.00032736198184168545: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.839252303713852e-06, wd=0.0004609389889268166, betas=(0.85, 0.99)\n",
      "iteration: 1/9   loss = 0.7762757539749146   train_acc = 0.48507462686567165   val_acc = 0.5714285714285714\n",
      "iteration: 2/9   loss = 0.7073365449905396   train_acc = 0.5373134328358209   val_acc = 0.5714285714285714\n",
      "iteration: 3/9   loss = 0.7312137484550476   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7177377939224243   train_acc = 0.4701492537313433   val_acc = 0.5714285714285714\n",
      "iteration: 5/9   loss = 0.7049355506896973   train_acc = 0.4925373134328358   val_acc = 0.6428571428571429\n",
      "iteration: 6/9   loss = 0.6929185390472412   train_acc = 0.5   val_acc = 0.6428571428571429\n",
      "iteration: 7/9   loss = 0.6945996284484863   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.6649232506752014   train_acc = 0.5223880597014925   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.6055198311805725   train_acc = 0.5074626865671642   val_acc = 0.6428571428571429\n",
      "Final val acc for lr=6.839252303713852e-06, wd=0.0004609389889268166: 0.6429\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.839252303713852e-06, wd=0.0004609389889268166, betas=(0.85, 0.995)\n",
      "iteration: 1/9   loss = 0.7671214938163757   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7386295199394226   train_acc = 0.5   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 1.0302302837371826   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7040314078330994   train_acc = 0.5074626865671642   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.79255610704422   train_acc = 0.5   val_acc = 0.5714285714285714\n",
      "iteration: 6/9   loss = 0.6652666926383972   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.733586847782135   train_acc = 0.5149253731343284   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.8335587382316589   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.6600698232650757   train_acc = 0.5074626865671642   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=6.839252303713852e-06, wd=0.0004609389889268166: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.839252303713852e-06, wd=0.0004609389889268166, betas=(0.9, 0.98)\n",
      "iteration: 1/9   loss = 0.7883487939834595   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7163594365119934   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.7786077260971069   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7677127122879028   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7191739082336426   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.6330236792564392   train_acc = 0.4925373134328358   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.6525771021842957   train_acc = 0.48507462686567165   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.7459811568260193   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.46503448486328125   train_acc = 0.5   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=6.839252303713852e-06, wd=0.0004609389889268166: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.839252303713852e-06, wd=0.0004609389889268166, betas=(0.9, 0.99)\n",
      "iteration: 1/9   loss = 0.7867901921272278   train_acc = 0.5223880597014925   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.7085684537887573   train_acc = 0.5298507462686567   val_acc = 0.2857142857142857\n",
      "iteration: 3/9   loss = 0.6110837459564209   train_acc = 0.5223880597014925   val_acc = 0.35714285714285715\n",
      "iteration: 4/9   loss = 0.6777750253677368   train_acc = 0.5298507462686567   val_acc = 0.2857142857142857\n",
      "iteration: 5/9   loss = 0.6649860143661499   train_acc = 0.5149253731343284   val_acc = 0.35714285714285715\n",
      "iteration: 6/9   loss = 0.6340063810348511   train_acc = 0.5298507462686567   val_acc = 0.35714285714285715\n",
      "iteration: 7/9   loss = 0.7097154855728149   train_acc = 0.5149253731343284   val_acc = 0.35714285714285715\n",
      "iteration: 8/9   loss = 0.7365039587020874   train_acc = 0.5149253731343284   val_acc = 0.42857142857142855\n",
      "iteration: 9/9   loss = 0.6443436741828918   train_acc = 0.5373134328358209   val_acc = 0.35714285714285715\n",
      "Final val acc for lr=6.839252303713852e-06, wd=0.0004609389889268166: 0.3571\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.839252303713852e-06, wd=0.0004609389889268166, betas=(0.9, 0.995)\n",
      "iteration: 1/9   loss = 0.7594921588897705   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7572731971740723   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.6921291351318359   train_acc = 0.4925373134328358   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.6793168783187866   train_acc = 0.48507462686567165   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.6955209970474243   train_acc = 0.4925373134328358   val_acc = 0.5714285714285714\n",
      "iteration: 6/9   loss = 0.9661023616790771   train_acc = 0.48507462686567165   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.6778627634048462   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7391960024833679   train_acc = 0.5   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.6736369132995605   train_acc = 0.5074626865671642   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=6.839252303713852e-06, wd=0.0004609389889268166: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.839252303713852e-06, wd=0.0004609389889268166, betas=(0.9, 0.999)\n",
      "iteration: 1/9   loss = 0.7082319259643555   train_acc = 0.5746268656716418   val_acc = 0.7142857142857143\n",
      "iteration: 2/9   loss = 0.7110691070556641   train_acc = 0.5223880597014925   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.8415050506591797   train_acc = 0.6044776119402985   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7162597179412842   train_acc = 0.5522388059701493   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.747811496257782   train_acc = 0.5522388059701493   val_acc = 0.5714285714285714\n",
      "iteration: 6/9   loss = 0.45000457763671875   train_acc = 0.5373134328358209   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.7202047109603882   train_acc = 0.5895522388059702   val_acc = 0.6428571428571429\n",
      "iteration: 8/9   loss = 0.6774446964263916   train_acc = 0.5447761194029851   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.7489686012268066   train_acc = 0.5671641791044776   val_acc = 0.6428571428571429\n",
      "Final val acc for lr=6.839252303713852e-06, wd=0.0004609389889268166: 0.6429\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.839252303713852e-06, wd=0.0004609389889268166, betas=(0.95, 0.98)\n",
      "iteration: 1/9   loss = 0.8036537170410156   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7553117275238037   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.6841672658920288   train_acc = 0.5074626865671642   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.7601714730262756   train_acc = 0.4925373134328358   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.6767712831497192   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.8603824973106384   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7655549645423889   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7016086578369141   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.6276199817657471   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "Final val acc for lr=6.839252303713852e-06, wd=0.0004609389889268166: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.839252303713852e-06, wd=0.0004609389889268166, betas=(0.95, 0.99)\n",
      "iteration: 1/9   loss = 0.7243945598602295   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7310222387313843   train_acc = 0.5223880597014925   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.7004072666168213   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7223677635192871   train_acc = 0.5074626865671642   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.690473735332489   train_acc = 0.5   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.7624403238296509   train_acc = 0.5   val_acc = 0.6428571428571429\n",
      "iteration: 7/9   loss = 0.6898139715194702   train_acc = 0.5223880597014925   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.6441513895988464   train_acc = 0.5298507462686567   val_acc = 0.6428571428571429\n",
      "iteration: 9/9   loss = 0.7173782587051392   train_acc = 0.5074626865671642   val_acc = 0.6428571428571429\n",
      "Final val acc for lr=6.839252303713852e-06, wd=0.0004609389889268166: 0.6429\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.839252303713852e-06, wd=0.0004609389889268166, betas=(0.95, 0.995)\n",
      "iteration: 1/9   loss = 0.7241658568382263   train_acc = 0.48507462686567165   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.7426114082336426   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.781296968460083   train_acc = 0.4701492537313433   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.6800235509872437   train_acc = 0.47761194029850745   val_acc = 0.35714285714285715\n",
      "iteration: 5/9   loss = 0.7518784403800964   train_acc = 0.4925373134328358   val_acc = 0.21428571428571427\n",
      "iteration: 6/9   loss = 0.8140958547592163   train_acc = 0.4701492537313433   val_acc = 0.35714285714285715\n",
      "iteration: 7/9   loss = 0.7051693201065063   train_acc = 0.5223880597014925   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.677377462387085   train_acc = 0.5149253731343284   val_acc = 0.2857142857142857\n",
      "iteration: 9/9   loss = 0.6693694591522217   train_acc = 0.4925373134328358   val_acc = 0.35714285714285715\n",
      "Final val acc for lr=6.839252303713852e-06, wd=0.0004609389889268166: 0.3571\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.839252303713852e-06, wd=0.0004609389889268166, betas=(0.95, 0.999)\n",
      "iteration: 1/9   loss = 0.7254201173782349   train_acc = 0.5447761194029851   val_acc = 0.7142857142857143\n",
      "iteration: 2/9   loss = 0.6556439995765686   train_acc = 0.5597014925373134   val_acc = 0.5714285714285714\n",
      "iteration: 3/9   loss = 0.6223950386047363   train_acc = 0.5298507462686567   val_acc = 0.6428571428571429\n",
      "iteration: 4/9   loss = 0.6739076375961304   train_acc = 0.5671641791044776   val_acc = 0.6428571428571429\n",
      "iteration: 5/9   loss = 0.723400354385376   train_acc = 0.5373134328358209   val_acc = 0.8571428571428571\n",
      "iteration: 6/9   loss = 0.617006778717041   train_acc = 0.5298507462686567   val_acc = 0.7142857142857143\n",
      "iteration: 7/9   loss = 0.7194089889526367   train_acc = 0.5447761194029851   val_acc = 0.7142857142857143\n",
      "iteration: 8/9   loss = 0.7005341053009033   train_acc = 0.5447761194029851   val_acc = 0.7142857142857143\n",
      "iteration: 9/9   loss = 0.6386999487876892   train_acc = 0.5447761194029851   val_acc = 0.7142857142857143\n",
      "Final val acc for lr=6.839252303713852e-06, wd=0.0004609389889268166: 0.7143\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.839252303713852e-06, wd=0.0002300292096821906, betas=(0.85, 0.99)\n",
      "iteration: 1/9   loss = 0.7291852831840515   train_acc = 0.47761194029850745   val_acc = 0.5714285714285714\n",
      "iteration: 2/9   loss = 0.7822821736335754   train_acc = 0.4626865671641791   val_acc = 0.5714285714285714\n",
      "iteration: 3/9   loss = 0.91612309217453   train_acc = 0.4626865671641791   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.6897683143615723   train_acc = 0.5298507462686567   val_acc = 0.6428571428571429\n",
      "iteration: 5/9   loss = 0.6914011240005493   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.5919697284698486   train_acc = 0.5223880597014925   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.7535954713821411   train_acc = 0.5074626865671642   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.6692705154418945   train_acc = 0.5223880597014925   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.8685646057128906   train_acc = 0.5149253731343284   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=6.839252303713852e-06, wd=0.0002300292096821906: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.839252303713852e-06, wd=0.0002300292096821906, betas=(0.85, 0.995)\n",
      "iteration: 1/9   loss = 0.7172520160675049   train_acc = 0.5   val_acc = 0.6428571428571429\n",
      "iteration: 2/9   loss = 0.7728312015533447   train_acc = 0.5074626865671642   val_acc = 0.5714285714285714\n",
      "iteration: 3/9   loss = 0.6675103902816772   train_acc = 0.5373134328358209   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.7169103622436523   train_acc = 0.5149253731343284   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.6734527349472046   train_acc = 0.5597014925373134   val_acc = 0.5714285714285714\n",
      "iteration: 6/9   loss = 0.7936677932739258   train_acc = 0.5522388059701493   val_acc = 0.6428571428571429\n",
      "iteration: 7/9   loss = 0.7248567342758179   train_acc = 0.5447761194029851   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.6731778979301453   train_acc = 0.4701492537313433   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.5768693685531616   train_acc = 0.5522388059701493   val_acc = 0.5\n",
      "Final val acc for lr=6.839252303713852e-06, wd=0.0002300292096821906: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.839252303713852e-06, wd=0.0002300292096821906, betas=(0.9, 0.98)\n",
      "iteration: 1/9   loss = 0.7436608672142029   train_acc = 0.4925373134328358   val_acc = 0.35714285714285715\n",
      "iteration: 2/9   loss = 0.7353879809379578   train_acc = 0.4552238805970149   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.7382307052612305   train_acc = 0.4552238805970149   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.7266389727592468   train_acc = 0.44029850746268656   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.7126541137695312   train_acc = 0.48507462686567165   val_acc = 0.21428571428571427\n",
      "iteration: 6/9   loss = 0.7846411466598511   train_acc = 0.5074626865671642   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.7405036687850952   train_acc = 0.5223880597014925   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.738213062286377   train_acc = 0.6044776119402985   val_acc = 0.35714285714285715\n",
      "iteration: 9/9   loss = 0.7554737329483032   train_acc = 0.5373134328358209   val_acc = 0.35714285714285715\n",
      "Final val acc for lr=6.839252303713852e-06, wd=0.0002300292096821906: 0.3571\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.839252303713852e-06, wd=0.0002300292096821906, betas=(0.9, 0.99)\n",
      "iteration: 1/9   loss = 0.7237769365310669   train_acc = 0.5895522388059702   val_acc = 0.8571428571428571\n",
      "iteration: 2/9   loss = 0.7230040431022644   train_acc = 0.5671641791044776   val_acc = 0.5714285714285714\n",
      "iteration: 3/9   loss = 0.6625847220420837   train_acc = 0.5597014925373134   val_acc = 0.7857142857142857\n",
      "iteration: 4/9   loss = 0.6841187477111816   train_acc = 0.5522388059701493   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.6788442134857178   train_acc = 0.5149253731343284   val_acc = 0.5714285714285714\n",
      "iteration: 6/9   loss = 0.688745379447937   train_acc = 0.5298507462686567   val_acc = 0.8571428571428571\n",
      "iteration: 7/9   loss = 0.6387397646903992   train_acc = 0.582089552238806   val_acc = 0.6428571428571429\n",
      "iteration: 8/9   loss = 0.6761687994003296   train_acc = 0.6044776119402985   val_acc = 0.7142857142857143\n",
      "iteration: 9/9   loss = 0.8282605409622192   train_acc = 0.582089552238806   val_acc = 0.5714285714285714\n",
      "Final val acc for lr=6.839252303713852e-06, wd=0.0002300292096821906: 0.5714\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.839252303713852e-06, wd=0.0002300292096821906, betas=(0.9, 0.995)\n",
      "iteration: 1/9   loss = 0.7890033721923828   train_acc = 0.5298507462686567   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7220513820648193   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.7174032926559448   train_acc = 0.5298507462686567   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7638635635375977   train_acc = 0.5373134328358209   val_acc = 0.5714285714285714\n",
      "iteration: 5/9   loss = 0.718897819519043   train_acc = 0.5597014925373134   val_acc = 0.5714285714285714\n",
      "iteration: 6/9   loss = 0.6808497905731201   train_acc = 0.5447761194029851   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.6994974613189697   train_acc = 0.5746268656716418   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.7482736110687256   train_acc = 0.582089552238806   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.727944552898407   train_acc = 0.5671641791044776   val_acc = 0.5\n",
      "Final val acc for lr=6.839252303713852e-06, wd=0.0002300292096821906: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.839252303713852e-06, wd=0.0002300292096821906, betas=(0.9, 0.999)\n",
      "iteration: 1/9   loss = 0.6999080181121826   train_acc = 0.582089552238806   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7558237314224243   train_acc = 0.5970149253731343   val_acc = 0.6428571428571429\n",
      "iteration: 3/9   loss = 0.6954072713851929   train_acc = 0.5597014925373134   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7383474111557007   train_acc = 0.5447761194029851   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7366389036178589   train_acc = 0.582089552238806   val_acc = 0.5714285714285714\n",
      "iteration: 6/9   loss = 0.5650664567947388   train_acc = 0.6044776119402985   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.7342501282691956   train_acc = 0.5970149253731343   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.7158834338188171   train_acc = 0.5970149253731343   val_acc = 0.7142857142857143\n",
      "iteration: 9/9   loss = 0.7995723485946655   train_acc = 0.5373134328358209   val_acc = 0.7142857142857143\n",
      "Final val acc for lr=6.839252303713852e-06, wd=0.0002300292096821906: 0.7143\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.839252303713852e-06, wd=0.0002300292096821906, betas=(0.95, 0.98)\n",
      "iteration: 1/9   loss = 0.7469165325164795   train_acc = 0.4925373134328358   val_acc = 0.5714285714285714\n",
      "iteration: 2/9   loss = 0.7145602107048035   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.8094877004623413   train_acc = 0.4701492537313433   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.7525182366371155   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7553694844245911   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.7366669178009033   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.6878973245620728   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7319638729095459   train_acc = 0.5373134328358209   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.7620595693588257   train_acc = 0.4701492537313433   val_acc = 0.5\n",
      "Final val acc for lr=6.839252303713852e-06, wd=0.0002300292096821906: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.839252303713852e-06, wd=0.0002300292096821906, betas=(0.95, 0.99)\n",
      "iteration: 1/9   loss = 0.7535320520401001   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7601739764213562   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.7079262733459473   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7925612926483154   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7624374628067017   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.6618958711624146   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.725051760673523   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7377146482467651   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.7975131273269653   train_acc = 0.5074626865671642   val_acc = 0.5714285714285714\n",
      "Final val acc for lr=6.839252303713852e-06, wd=0.0002300292096821906: 0.5714\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.839252303713852e-06, wd=0.0002300292096821906, betas=(0.95, 0.995)\n",
      "iteration: 1/9   loss = 0.7661665678024292   train_acc = 0.5149253731343284   val_acc = 0.5714285714285714\n",
      "iteration: 2/9   loss = 0.7079781889915466   train_acc = 0.5   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.6252228617668152   train_acc = 0.48507462686567165   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.6850075721740723   train_acc = 0.47761194029850745   val_acc = 0.5714285714285714\n",
      "iteration: 5/9   loss = 0.7130904197692871   train_acc = 0.47761194029850745   val_acc = 0.6428571428571429\n",
      "iteration: 6/9   loss = 0.5422458052635193   train_acc = 0.5447761194029851   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.7257658243179321   train_acc = 0.5074626865671642   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.6957417726516724   train_acc = 0.5149253731343284   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.7740920782089233   train_acc = 0.5298507462686567   val_acc = 0.5\n",
      "Final val acc for lr=6.839252303713852e-06, wd=0.0002300292096821906: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.839252303713852e-06, wd=0.0002300292096821906, betas=(0.95, 0.999)\n",
      "iteration: 1/9   loss = 0.7624474763870239   train_acc = 0.47761194029850745   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7705518007278442   train_acc = 0.4626865671641791   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.7265402674674988   train_acc = 0.4626865671641791   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.769385576248169   train_acc = 0.4626865671641791   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.7079603672027588   train_acc = 0.4552238805970149   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.8332555294036865   train_acc = 0.4552238805970149   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7480278015136719   train_acc = 0.4552238805970149   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.7084072828292847   train_acc = 0.4626865671641791   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.8214725255966187   train_acc = 0.4552238805970149   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=6.839252303713852e-06, wd=0.0002300292096821906: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.839252303713852e-06, wd=0.00022555743977178082, betas=(0.85, 0.99)\n",
      "iteration: 1/9   loss = 0.7109397053718567   train_acc = 0.5223880597014925   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.7817237377166748   train_acc = 0.48507462686567165   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.7632166743278503   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.694880485534668   train_acc = 0.4253731343283582   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.7121590375900269   train_acc = 0.5522388059701493   val_acc = 0.2857142857142857\n",
      "iteration: 6/9   loss = 0.5842928290367126   train_acc = 0.5149253731343284   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.7049257159233093   train_acc = 0.5   val_acc = 0.2857142857142857\n",
      "iteration: 8/9   loss = 0.8072259426116943   train_acc = 0.5373134328358209   val_acc = 0.35714285714285715\n",
      "iteration: 9/9   loss = 0.7308579683303833   train_acc = 0.5149253731343284   val_acc = 0.35714285714285715\n",
      "Final val acc for lr=6.839252303713852e-06, wd=0.00022555743977178082: 0.3571\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.839252303713852e-06, wd=0.00022555743977178082, betas=(0.85, 0.995)\n",
      "iteration: 1/9   loss = 0.7739615440368652   train_acc = 0.43283582089552236   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7261258363723755   train_acc = 0.4626865671641791   val_acc = 0.5714285714285714\n",
      "iteration: 3/9   loss = 0.7249685525894165   train_acc = 0.41044776119402987   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.6780033111572266   train_acc = 0.44029850746268656   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7225457429885864   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.6474990844726562   train_acc = 0.4626865671641791   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.7229351997375488   train_acc = 0.4626865671641791   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.7132508158683777   train_acc = 0.48507462686567165   val_acc = 0.42857142857142855\n",
      "iteration: 9/9   loss = 0.8386243581771851   train_acc = 0.47761194029850745   val_acc = 0.5\n",
      "Final val acc for lr=6.839252303713852e-06, wd=0.00022555743977178082: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.839252303713852e-06, wd=0.00022555743977178082, betas=(0.9, 0.98)\n",
      "iteration: 1/9   loss = 0.7475916147232056   train_acc = 0.5149253731343284   val_acc = 0.6428571428571429\n",
      "iteration: 2/9   loss = 0.7295944690704346   train_acc = 0.47761194029850745   val_acc = 0.6428571428571429\n",
      "iteration: 3/9   loss = 0.6503270864486694   train_acc = 0.5   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.7026619911193848   train_acc = 0.5074626865671642   val_acc = 0.6428571428571429\n",
      "iteration: 5/9   loss = 0.73660808801651   train_acc = 0.4701492537313433   val_acc = 0.6428571428571429\n",
      "iteration: 6/9   loss = 0.6173056364059448   train_acc = 0.48507462686567165   val_acc = 0.6428571428571429\n",
      "iteration: 7/9   loss = 0.6939175128936768   train_acc = 0.48507462686567165   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.700576663017273   train_acc = 0.44776119402985076   val_acc = 0.7142857142857143\n",
      "iteration: 9/9   loss = 0.668041467666626   train_acc = 0.5074626865671642   val_acc = 0.6428571428571429\n",
      "Final val acc for lr=6.839252303713852e-06, wd=0.00022555743977178082: 0.6429\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.839252303713852e-06, wd=0.00022555743977178082, betas=(0.9, 0.99)\n",
      "iteration: 1/9   loss = 0.705529510974884   train_acc = 0.5447761194029851   val_acc = 0.5714285714285714\n",
      "iteration: 2/9   loss = 0.7295373678207397   train_acc = 0.5597014925373134   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.8358501195907593   train_acc = 0.582089552238806   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.7272236347198486   train_acc = 0.5597014925373134   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.6843758225440979   train_acc = 0.5895522388059702   val_acc = 0.5714285714285714\n",
      "iteration: 6/9   loss = 0.6170104146003723   train_acc = 0.6119402985074627   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.7171792984008789   train_acc = 0.5746268656716418   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.724053680896759   train_acc = 0.5970149253731343   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.8764156103134155   train_acc = 0.6194029850746269   val_acc = 0.5\n",
      "Final val acc for lr=6.839252303713852e-06, wd=0.00022555743977178082: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.839252303713852e-06, wd=0.00022555743977178082, betas=(0.9, 0.995)\n",
      "iteration: 1/9   loss = 0.701794445514679   train_acc = 0.5671641791044776   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.769560694694519   train_acc = 0.5522388059701493   val_acc = 0.5\n",
      "iteration: 3/9   loss = 1.1029107570648193   train_acc = 0.5447761194029851   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.673508882522583   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.6569837331771851   train_acc = 0.5522388059701493   val_acc = 0.6428571428571429\n",
      "iteration: 6/9   loss = 0.6038005948066711   train_acc = 0.5223880597014925   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.701085090637207   train_acc = 0.5895522388059702   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.7209657430648804   train_acc = 0.5447761194029851   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.8169506788253784   train_acc = 0.5746268656716418   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=6.839252303713852e-06, wd=0.00022555743977178082: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.839252303713852e-06, wd=0.00022555743977178082, betas=(0.9, 0.999)\n",
      "iteration: 1/9   loss = 0.7087396383285522   train_acc = 0.4552238805970149   val_acc = 0.5714285714285714\n",
      "iteration: 2/9   loss = 0.6873523592948914   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.6768938899040222   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.6939358711242676   train_acc = 0.4925373134328358   val_acc = 0.5714285714285714\n",
      "iteration: 5/9   loss = 0.7071094512939453   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.6631402373313904   train_acc = 0.4552238805970149   val_acc = 0.35714285714285715\n",
      "iteration: 7/9   loss = 0.6546168327331543   train_acc = 0.5298507462686567   val_acc = 0.6428571428571429\n",
      "iteration: 8/9   loss = 0.7217196226119995   train_acc = 0.47761194029850745   val_acc = 0.6428571428571429\n",
      "iteration: 9/9   loss = 0.6615473628044128   train_acc = 0.5   val_acc = 0.2857142857142857\n",
      "Final val acc for lr=6.839252303713852e-06, wd=0.00022555743977178082: 0.2857\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.839252303713852e-06, wd=0.00022555743977178082, betas=(0.95, 0.98)\n",
      "iteration: 1/9   loss = 0.7695980072021484   train_acc = 0.5373134328358209   val_acc = 0.5714285714285714\n",
      "iteration: 2/9   loss = 0.7444164752960205   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.5779889822006226   train_acc = 0.5298507462686567   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.7604649662971497   train_acc = 0.5447761194029851   val_acc = 0.5714285714285714\n",
      "iteration: 5/9   loss = 0.71744704246521   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.7104052305221558   train_acc = 0.5149253731343284   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.7460761070251465   train_acc = 0.5074626865671642   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.717729926109314   train_acc = 0.48507462686567165   val_acc = 0.6428571428571429\n",
      "iteration: 9/9   loss = 0.7037962675094604   train_acc = 0.5298507462686567   val_acc = 0.5\n",
      "Final val acc for lr=6.839252303713852e-06, wd=0.00022555743977178082: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.839252303713852e-06, wd=0.00022555743977178082, betas=(0.95, 0.99)\n",
      "iteration: 1/9   loss = 0.7015104293823242   train_acc = 0.5298507462686567   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7447617053985596   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.8861937522888184   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.6665007472038269   train_acc = 0.5223880597014925   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7508080005645752   train_acc = 0.4925373134328358   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.7051999568939209   train_acc = 0.5149253731343284   val_acc = 0.35714285714285715\n",
      "iteration: 7/9   loss = 0.7390487194061279   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.746146559715271   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.6824025511741638   train_acc = 0.5298507462686567   val_acc = 0.5\n",
      "Final val acc for lr=6.839252303713852e-06, wd=0.00022555743977178082: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.839252303713852e-06, wd=0.00022555743977178082, betas=(0.95, 0.995)\n",
      "iteration: 1/9   loss = 0.7276666760444641   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7903848886489868   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.8113330602645874   train_acc = 0.47761194029850745   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7457129955291748   train_acc = 0.47761194029850745   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.651326060295105   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 6/9   loss = 1.0242218971252441   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7064507007598877   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7423398494720459   train_acc = 0.4701492537313433   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.5671923160552979   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "Final val acc for lr=6.839252303713852e-06, wd=0.00022555743977178082: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.839252303713852e-06, wd=0.00022555743977178082, betas=(0.95, 0.999)\n",
      "iteration: 1/9   loss = 0.7110726237297058   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.8406039476394653   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.6853445768356323   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7809320092201233   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7499280571937561   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.8217065334320068   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.8108415007591248   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7655625343322754   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.8457380533218384   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "Final val acc for lr=6.839252303713852e-06, wd=0.00022555743977178082: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.839252303713852e-06, wd=0.00011822143451838108, betas=(0.85, 0.99)\n",
      "iteration: 1/9   loss = 0.7213729619979858   train_acc = 0.5298507462686567   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.7348276972770691   train_acc = 0.5149253731343284   val_acc = 0.5714285714285714\n",
      "iteration: 3/9   loss = 0.7090399265289307   train_acc = 0.48507462686567165   val_acc = 0.35714285714285715\n",
      "iteration: 4/9   loss = 0.7385468482971191   train_acc = 0.5223880597014925   val_acc = 0.5714285714285714\n",
      "iteration: 5/9   loss = 0.7438636422157288   train_acc = 0.48507462686567165   val_acc = 0.5714285714285714\n",
      "iteration: 6/9   loss = 0.592668890953064   train_acc = 0.5   val_acc = 0.35714285714285715\n",
      "iteration: 7/9   loss = 0.7028119564056396   train_acc = 0.5149253731343284   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.6992884278297424   train_acc = 0.5298507462686567   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.8505412936210632   train_acc = 0.48507462686567165   val_acc = 0.35714285714285715\n",
      "Final val acc for lr=6.839252303713852e-06, wd=0.00011822143451838108: 0.3571\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.839252303713852e-06, wd=0.00011822143451838108, betas=(0.85, 0.995)\n",
      "iteration: 1/9   loss = 0.7422394156455994   train_acc = 0.4626865671641791   val_acc = 0.7857142857142857\n",
      "iteration: 2/9   loss = 0.7262307405471802   train_acc = 0.48507462686567165   val_acc = 0.8571428571428571\n",
      "iteration: 3/9   loss = 0.6831508874893188   train_acc = 0.4626865671641791   val_acc = 0.7857142857142857\n",
      "iteration: 4/9   loss = 0.6929405331611633   train_acc = 0.4626865671641791   val_acc = 0.8571428571428571\n",
      "iteration: 5/9   loss = 0.7316831350326538   train_acc = 0.44029850746268656   val_acc = 0.8571428571428571\n",
      "iteration: 6/9   loss = 0.8722162246704102   train_acc = 0.43283582089552236   val_acc = 0.9285714285714286\n",
      "iteration: 7/9   loss = 0.7478474378585815   train_acc = 0.47761194029850745   val_acc = 0.8571428571428571\n",
      "iteration: 8/9   loss = 0.6648803353309631   train_acc = 0.4925373134328358   val_acc = 0.9285714285714286\n",
      "iteration: 9/9   loss = 0.6903390288352966   train_acc = 0.5149253731343284   val_acc = 0.8571428571428571\n",
      "Final val acc for lr=6.839252303713852e-06, wd=0.00011822143451838108: 0.8571\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.839252303713852e-06, wd=0.00011822143451838108, betas=(0.9, 0.98)\n",
      "iteration: 1/9   loss = 0.7322646975517273   train_acc = 0.5   val_acc = 0.21428571428571427\n",
      "iteration: 2/9   loss = 0.7294015288352966   train_acc = 0.5447761194029851   val_acc = 0.21428571428571427\n",
      "iteration: 3/9   loss = 0.6636190414428711   train_acc = 0.5074626865671642   val_acc = 0.35714285714285715\n",
      "iteration: 4/9   loss = 0.754420280456543   train_acc = 0.47761194029850745   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.6698364019393921   train_acc = 0.5746268656716418   val_acc = 0.35714285714285715\n",
      "iteration: 6/9   loss = 0.7627719640731812   train_acc = 0.5447761194029851   val_acc = 0.21428571428571427\n",
      "iteration: 7/9   loss = 0.6787776947021484   train_acc = 0.582089552238806   val_acc = 0.21428571428571427\n",
      "iteration: 8/9   loss = 0.6922646164894104   train_acc = 0.6044776119402985   val_acc = 0.2857142857142857\n",
      "iteration: 9/9   loss = 0.622456967830658   train_acc = 0.5522388059701493   val_acc = 0.21428571428571427\n",
      "Final val acc for lr=6.839252303713852e-06, wd=0.00011822143451838108: 0.2143\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.839252303713852e-06, wd=0.00011822143451838108, betas=(0.9, 0.99)\n",
      "iteration: 1/9   loss = 0.6675401926040649   train_acc = 0.5447761194029851   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.712122917175293   train_acc = 0.5298507462686567   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.6953573226928711   train_acc = 0.5447761194029851   val_acc = 0.2857142857142857\n",
      "iteration: 4/9   loss = 0.749410092830658   train_acc = 0.5223880597014925   val_acc = 0.35714285714285715\n",
      "iteration: 5/9   loss = 0.6957830190658569   train_acc = 0.5671641791044776   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.8584648966789246   train_acc = 0.5223880597014925   val_acc = 0.21428571428571427\n",
      "iteration: 7/9   loss = 0.7045305967330933   train_acc = 0.5223880597014925   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.7475553750991821   train_acc = 0.5149253731343284   val_acc = 0.35714285714285715\n",
      "iteration: 9/9   loss = 0.6563175320625305   train_acc = 0.5522388059701493   val_acc = 0.35714285714285715\n",
      "Final val acc for lr=6.839252303713852e-06, wd=0.00011822143451838108: 0.3571\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.839252303713852e-06, wd=0.00011822143451838108, betas=(0.9, 0.995)\n",
      "iteration: 1/9   loss = 0.7747673988342285   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7726583480834961   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.8391863703727722   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7318329811096191   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.839000940322876   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 6/9   loss = 1.2835273742675781   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7677359580993652   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7596147060394287   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.7133398056030273   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "Final val acc for lr=6.839252303713852e-06, wd=0.00011822143451838108: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.839252303713852e-06, wd=0.00011822143451838108, betas=(0.9, 0.999)\n",
      "iteration: 1/9   loss = 0.7460798025131226   train_acc = 0.5597014925373134   val_acc = 0.5714285714285714\n",
      "iteration: 2/9   loss = 0.768961489200592   train_acc = 0.5895522388059702   val_acc = 0.6428571428571429\n",
      "iteration: 3/9   loss = 0.7365519404411316   train_acc = 0.5373134328358209   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.6694163680076599   train_acc = 0.5895522388059702   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7226693630218506   train_acc = 0.5522388059701493   val_acc = 0.5714285714285714\n",
      "iteration: 6/9   loss = 0.7803775668144226   train_acc = 0.6194029850746269   val_acc = 0.6428571428571429\n",
      "iteration: 7/9   loss = 0.7318609952926636   train_acc = 0.5298507462686567   val_acc = 0.6428571428571429\n",
      "iteration: 8/9   loss = 0.7103992104530334   train_acc = 0.5597014925373134   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.7656569480895996   train_acc = 0.5671641791044776   val_acc = 0.6428571428571429\n",
      "Final val acc for lr=6.839252303713852e-06, wd=0.00011822143451838108: 0.6429\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.839252303713852e-06, wd=0.00011822143451838108, betas=(0.95, 0.98)\n",
      "iteration: 1/9   loss = 0.8113913536071777   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.6420953273773193   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.8661060333251953   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.6756290793418884   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.739212691783905   train_acc = 0.47761194029850745   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.8265390396118164   train_acc = 0.5   val_acc = 0.35714285714285715\n",
      "iteration: 7/9   loss = 0.73921799659729   train_acc = 0.5447761194029851   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.7200684547424316   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.682343602180481   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "Final val acc for lr=6.839252303713852e-06, wd=0.00011822143451838108: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.839252303713852e-06, wd=0.00011822143451838108, betas=(0.95, 0.99)\n",
      "iteration: 1/9   loss = 0.6931283473968506   train_acc = 0.43283582089552236   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.7088543176651001   train_acc = 0.4701492537313433   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.7695061564445496   train_acc = 0.47761194029850745   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.7366280555725098   train_acc = 0.4925373134328358   val_acc = 0.35714285714285715\n",
      "iteration: 5/9   loss = 0.7065001726150513   train_acc = 0.4701492537313433   val_acc = 0.35714285714285715\n",
      "iteration: 6/9   loss = 0.5736199617385864   train_acc = 0.4701492537313433   val_acc = 0.35714285714285715\n",
      "iteration: 7/9   loss = 0.7686534523963928   train_acc = 0.44776119402985076   val_acc = 0.35714285714285715\n",
      "iteration: 8/9   loss = 0.6917074918746948   train_acc = 0.4552238805970149   val_acc = 0.42857142857142855\n",
      "iteration: 9/9   loss = 0.7759069204330444   train_acc = 0.4626865671641791   val_acc = 0.35714285714285715\n",
      "Final val acc for lr=6.839252303713852e-06, wd=0.00011822143451838108: 0.3571\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.839252303713852e-06, wd=0.00011822143451838108, betas=(0.95, 0.995)\n",
      "iteration: 1/9   loss = 0.762345552444458   train_acc = 0.5373134328358209   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.7768615484237671   train_acc = 0.5373134328358209   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.7861752510070801   train_acc = 0.5298507462686567   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.736054539680481   train_acc = 0.5074626865671642   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.7238742113113403   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.6159361600875854   train_acc = 0.5298507462686567   val_acc = 0.6428571428571429\n",
      "iteration: 7/9   loss = 0.7368475198745728   train_acc = 0.5298507462686567   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7252111434936523   train_acc = 0.5223880597014925   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.7107560038566589   train_acc = 0.5671641791044776   val_acc = 0.5\n",
      "Final val acc for lr=6.839252303713852e-06, wd=0.00011822143451838108: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.839252303713852e-06, wd=0.00011822143451838108, betas=(0.95, 0.999)\n",
      "iteration: 1/9   loss = 0.7190673351287842   train_acc = 0.5597014925373134   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.7338112592697144   train_acc = 0.5373134328358209   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.6760610342025757   train_acc = 0.5447761194029851   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.6985501050949097   train_acc = 0.5298507462686567   val_acc = 0.35714285714285715\n",
      "iteration: 5/9   loss = 0.7280955910682678   train_acc = 0.5298507462686567   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.7229702472686768   train_acc = 0.5373134328358209   val_acc = 0.35714285714285715\n",
      "iteration: 7/9   loss = 0.7552458047866821   train_acc = 0.5298507462686567   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.7471312880516052   train_acc = 0.5522388059701493   val_acc = 0.2857142857142857\n",
      "iteration: 9/9   loss = 0.773097038269043   train_acc = 0.5671641791044776   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=6.839252303713852e-06, wd=0.00011822143451838108: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.839252303713852e-06, wd=0.00032736198184168545, betas=(0.85, 0.99)\n",
      "iteration: 1/9   loss = 0.6827725768089294   train_acc = 0.5149253731343284   val_acc = 0.2857142857142857\n",
      "iteration: 2/9   loss = 0.7540473341941833   train_acc = 0.5447761194029851   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.8106526136398315   train_acc = 0.5149253731343284   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.7224503755569458   train_acc = 0.4925373134328358   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.7397193908691406   train_acc = 0.5447761194029851   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.7924795150756836   train_acc = 0.582089552238806   val_acc = 0.35714285714285715\n",
      "iteration: 7/9   loss = 0.7234216332435608   train_acc = 0.5373134328358209   val_acc = 0.35714285714285715\n",
      "iteration: 8/9   loss = 0.6516064405441284   train_acc = 0.582089552238806   val_acc = 0.35714285714285715\n",
      "iteration: 9/9   loss = 0.7933872938156128   train_acc = 0.5074626865671642   val_acc = 0.35714285714285715\n",
      "Final val acc for lr=6.839252303713852e-06, wd=0.00032736198184168545: 0.3571\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.839252303713852e-06, wd=0.00032736198184168545, betas=(0.85, 0.995)\n",
      "iteration: 1/9   loss = 0.7315488457679749   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7450882196426392   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.8505010008811951   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7417293787002563   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7537153959274292   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.5413594841957092   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7485067844390869   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7517334818840027   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.9315437078475952   train_acc = 0.5074626865671642   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=6.839252303713852e-06, wd=0.00032736198184168545: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.839252303713852e-06, wd=0.00032736198184168545, betas=(0.9, 0.98)\n",
      "iteration: 1/9   loss = 0.7081042528152466   train_acc = 0.5149253731343284   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.7440630793571472   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.627705991268158   train_acc = 0.5447761194029851   val_acc = 0.35714285714285715\n",
      "iteration: 4/9   loss = 0.70039963722229   train_acc = 0.5298507462686567   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.7276204228401184   train_acc = 0.4925373134328358   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.7242815494537354   train_acc = 0.5298507462686567   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.7131335735321045   train_acc = 0.5522388059701493   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.7142535448074341   train_acc = 0.5298507462686567   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.7557566165924072   train_acc = 0.5597014925373134   val_acc = 0.5\n",
      "Final val acc for lr=6.839252303713852e-06, wd=0.00032736198184168545: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.839252303713852e-06, wd=0.00032736198184168545, betas=(0.9, 0.99)\n",
      "iteration: 1/9   loss = 0.7830358147621155   train_acc = 0.48507462686567165   val_acc = 0.5714285714285714\n",
      "iteration: 2/9   loss = 0.7182037830352783   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.7402474284172058   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7150411605834961   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7416850328445435   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.9435811042785645   train_acc = 0.5074626865671642   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.7384523749351501   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7174655199050903   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.6926082968711853   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "Final val acc for lr=6.839252303713852e-06, wd=0.00032736198184168545: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.839252303713852e-06, wd=0.00032736198184168545, betas=(0.9, 0.995)\n",
      "iteration: 1/9   loss = 0.6556859016418457   train_acc = 0.5373134328358209   val_acc = 0.5714285714285714\n",
      "iteration: 2/9   loss = 0.7063800096511841   train_acc = 0.582089552238806   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.6265041828155518   train_acc = 0.5671641791044776   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.7562037110328674   train_acc = 0.582089552238806   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.6979548335075378   train_acc = 0.582089552238806   val_acc = 0.35714285714285715\n",
      "iteration: 6/9   loss = 0.5893534421920776   train_acc = 0.5597014925373134   val_acc = 0.21428571428571427\n",
      "iteration: 7/9   loss = 0.6677331924438477   train_acc = 0.5895522388059702   val_acc = 0.35714285714285715\n",
      "iteration: 8/9   loss = 0.6990892887115479   train_acc = 0.6194029850746269   val_acc = 0.2857142857142857\n",
      "iteration: 9/9   loss = 0.6742607951164246   train_acc = 0.582089552238806   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=6.839252303713852e-06, wd=0.00032736198184168545: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.839252303713852e-06, wd=0.00032736198184168545, betas=(0.9, 0.999)\n",
      "iteration: 1/9   loss = 0.816954493522644   train_acc = 0.5074626865671642   val_acc = 0.5714285714285714\n",
      "iteration: 2/9   loss = 0.7185479998588562   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.6993511915206909   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7103308439254761   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.8073218464851379   train_acc = 0.4925373134328358   val_acc = 0.5714285714285714\n",
      "iteration: 6/9   loss = 0.6546381711959839   train_acc = 0.5149253731343284   val_acc = 0.6428571428571429\n",
      "iteration: 7/9   loss = 0.727532684803009   train_acc = 0.4925373134328358   val_acc = 0.6428571428571429\n",
      "iteration: 8/9   loss = 0.7114160060882568   train_acc = 0.4701492537313433   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.8542476892471313   train_acc = 0.5223880597014925   val_acc = 0.5\n",
      "Final val acc for lr=6.839252303713852e-06, wd=0.00032736198184168545: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.839252303713852e-06, wd=0.00032736198184168545, betas=(0.95, 0.98)\n",
      "iteration: 1/9   loss = 0.6856836080551147   train_acc = 0.48507462686567165   val_acc = 0.5714285714285714\n",
      "iteration: 2/9   loss = 0.7454855442047119   train_acc = 0.4925373134328358   val_acc = 0.5714285714285714\n",
      "iteration: 3/9   loss = 0.7648359537124634   train_acc = 0.5223880597014925   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.6860287189483643   train_acc = 0.5223880597014925   val_acc = 0.6428571428571429\n",
      "iteration: 5/9   loss = 0.7115057706832886   train_acc = 0.5522388059701493   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.7924320101737976   train_acc = 0.5074626865671642   val_acc = 0.6428571428571429\n",
      "iteration: 7/9   loss = 0.6558656096458435   train_acc = 0.5223880597014925   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.7828965783119202   train_acc = 0.5597014925373134   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.7025231122970581   train_acc = 0.5746268656716418   val_acc = 0.5\n",
      "Final val acc for lr=6.839252303713852e-06, wd=0.00032736198184168545: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.839252303713852e-06, wd=0.00032736198184168545, betas=(0.95, 0.99)\n",
      "iteration: 1/9   loss = 0.6888183355331421   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7494832277297974   train_acc = 0.5   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.9675248861312866   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.6978806257247925   train_acc = 0.5   val_acc = 0.35714285714285715\n",
      "iteration: 5/9   loss = 0.7303390502929688   train_acc = 0.5223880597014925   val_acc = 0.35714285714285715\n",
      "iteration: 6/9   loss = 0.6752960085868835   train_acc = 0.5223880597014925   val_acc = 0.35714285714285715\n",
      "iteration: 7/9   loss = 0.7293538451194763   train_acc = 0.5298507462686567   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.6994078159332275   train_acc = 0.5447761194029851   val_acc = 0.35714285714285715\n",
      "iteration: 9/9   loss = 0.6096581816673279   train_acc = 0.5298507462686567   val_acc = 0.35714285714285715\n",
      "Final val acc for lr=6.839252303713852e-06, wd=0.00032736198184168545: 0.3571\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.839252303713852e-06, wd=0.00032736198184168545, betas=(0.95, 0.995)\n",
      "iteration: 1/9   loss = 0.6801339983940125   train_acc = 0.4701492537313433   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.7050448060035706   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.7415046691894531   train_acc = 0.4701492537313433   val_acc = 0.2857142857142857\n",
      "iteration: 4/9   loss = 0.74284827709198   train_acc = 0.48507462686567165   val_acc = 0.2857142857142857\n",
      "iteration: 5/9   loss = 0.6997435092926025   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.6458645462989807   train_acc = 0.4925373134328358   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.7150456309318542   train_acc = 0.48507462686567165   val_acc = 0.35714285714285715\n",
      "iteration: 8/9   loss = 0.7290911078453064   train_acc = 0.5   val_acc = 0.42857142857142855\n",
      "iteration: 9/9   loss = 0.5775754451751709   train_acc = 0.5074626865671642   val_acc = 0.2857142857142857\n",
      "Final val acc for lr=6.839252303713852e-06, wd=0.00032736198184168545: 0.2857\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.839252303713852e-06, wd=0.00032736198184168545, betas=(0.95, 0.999)\n",
      "iteration: 1/9   loss = 0.6944949626922607   train_acc = 0.4626865671641791   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.6778708696365356   train_acc = 0.43283582089552236   val_acc = 0.5714285714285714\n",
      "iteration: 3/9   loss = 0.6234039664268494   train_acc = 0.417910447761194   val_acc = 0.35714285714285715\n",
      "iteration: 4/9   loss = 0.7370848059654236   train_acc = 0.3880597014925373   val_acc = 0.35714285714285715\n",
      "iteration: 5/9   loss = 0.7248401045799255   train_acc = 0.4701492537313433   val_acc = 0.21428571428571427\n",
      "iteration: 6/9   loss = 0.6573908925056458   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7356739044189453   train_acc = 0.4552238805970149   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7373892068862915   train_acc = 0.4552238805970149   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.6769735217094421   train_acc = 0.44776119402985076   val_acc = 0.35714285714285715\n",
      "Final val acc for lr=6.839252303713852e-06, wd=0.00032736198184168545: 0.3571\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.007520305204984156, wd=0.0004609389889268166, betas=(0.85, 0.99)\n",
      "iteration: 1/9   loss = 0.7132095098495483   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.9689014554023743   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 3/9   loss = 2.0376102924346924   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 4/9   loss = 1.2771947383880615   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 5/9   loss = 1.0336302518844604   train_acc = 0.5970149253731343   val_acc = 0.5714285714285714\n",
      "iteration: 6/9   loss = 0.7146203517913818   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7032126188278198   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7295724749565125   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.6482986807823181   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "Final val acc for lr=0.007520305204984156, wd=0.0004609389889268166: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.007520305204984156, wd=0.0004609389889268166, betas=(0.85, 0.995)\n",
      "iteration: 1/9   loss = 0.7609047293663025   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 2/9   loss = 1.473282814025879   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 3/9   loss = 1.0244159698486328   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.8211227655410767   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7057455778121948   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.7024070024490356   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.6887097954750061   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.73661208152771   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.8112348914146423   train_acc = 0.6194029850746269   val_acc = 0.7857142857142857\n",
      "Final val acc for lr=0.007520305204984156, wd=0.0004609389889268166: 0.7857\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.007520305204984156, wd=0.0004609389889268166, betas=(0.9, 0.98)\n",
      "iteration: 1/9   loss = 0.6600662469863892   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 2/9   loss = 1.7813102006912231   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.9196268320083618   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.8586957454681396   train_acc = 0.4925373134328358   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.7003238201141357   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.7303048968315125   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.6995301246643066   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.8425483703613281   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.7149300575256348   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "Final val acc for lr=0.007520305204984156, wd=0.0004609389889268166: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.007520305204984156, wd=0.0004609389889268166, betas=(0.9, 0.99)\n",
      "iteration: 1/9   loss = 0.7298049926757812   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 2/9   loss = 1.6990571022033691   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.7403103709220886   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.9304153323173523   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7538564205169678   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.6909035444259644   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.680801510810852   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7895625233650208   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.7998878359794617   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "Final val acc for lr=0.007520305204984156, wd=0.0004609389889268166: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.007520305204984156, wd=0.0004609389889268166, betas=(0.9, 0.995)\n",
      "iteration: 1/9   loss = 0.7238181233406067   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.8969517946243286   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 3/9   loss = 1.3285903930664062   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 4/9   loss = 1.1096532344818115   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.6514784097671509   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.6774637699127197   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7339621186256409   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.9406158924102783   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 9/9   loss = 1.2100138664245605   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "Final val acc for lr=0.007520305204984156, wd=0.0004609389889268166: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.007520305204984156, wd=0.0004609389889268166, betas=(0.9, 0.999)\n",
      "iteration: 1/9   loss = 0.7080497741699219   train_acc = 0.5746268656716418   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7640219926834106   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 3/9   loss = 2.683399200439453   train_acc = 0.6194029850746269   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.6988496780395508   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.8253117203712463   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.3289547860622406   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.9212117791175842   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.8569109439849854   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.46968191862106323   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "Final val acc for lr=0.007520305204984156, wd=0.0004609389889268166: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.007520305204984156, wd=0.0004609389889268166, betas=(0.95, 0.98)\n",
      "iteration: 1/9   loss = 0.7763240337371826   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 2/9   loss = 1.0148169994354248   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 3/9   loss = 1.299666404724121   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.8979768753051758   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7732124924659729   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.654404878616333   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7661164999008179   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.816523015499115   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.7966293096542358   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "Final val acc for lr=0.007520305204984156, wd=0.0004609389889268166: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.007520305204984156, wd=0.0004609389889268166, betas=(0.95, 0.99)\n",
      "iteration: 1/9   loss = 0.6651298403739929   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 2/9   loss = 1.0757676362991333   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 3/9   loss = 1.2959222793579102   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.9737015962600708   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7976819276809692   train_acc = 0.5746268656716418   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.7289695143699646   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7413099408149719   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.9471544623374939   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.6805450320243835   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "Final val acc for lr=0.007520305204984156, wd=0.0004609389889268166: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.007520305204984156, wd=0.0004609389889268166, betas=(0.95, 0.995)\n",
      "iteration: 1/9   loss = 0.7364621162414551   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 2/9   loss = 1.7552907466888428   train_acc = 0.582089552238806   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.6577358245849609   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.9005658030509949   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7787482142448425   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.8922140002250671   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.6831575632095337   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7677424550056458   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.8098679780960083   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "Final val acc for lr=0.007520305204984156, wd=0.0004609389889268166: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.007520305204984156, wd=0.0004609389889268166, betas=(0.95, 0.999)\n",
      "iteration: 1/9   loss = 0.7399408221244812   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7219950556755066   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 3/9   loss = 1.6734269857406616   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.77649986743927   train_acc = 0.5671641791044776   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7013208270072937   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.8815163373947144   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.8209230899810791   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.8338948488235474   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.6804237365722656   train_acc = 0.5970149253731343   val_acc = 0.2857142857142857\n",
      "Final val acc for lr=0.007520305204984156, wd=0.0004609389889268166: 0.2857\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.007520305204984156, wd=0.0002300292096821906, betas=(0.85, 0.99)\n",
      "iteration: 1/9   loss = 0.725006103515625   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 2/9   loss = 1.364255428314209   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 3/9   loss = 1.2031911611557007   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7449028491973877   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.6934220790863037   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.6557707190513611   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7654988765716553   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.8360395431518555   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.9028312563896179   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "Final val acc for lr=0.007520305204984156, wd=0.0002300292096821906: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.007520305204984156, wd=0.0002300292096821906, betas=(0.85, 0.995)\n",
      "iteration: 1/9   loss = 0.7711153030395508   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 2/9   loss = 1.2948336601257324   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.9989420771598816   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.8935513496398926   train_acc = 0.5671641791044776   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.6785292625427246   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.7600458860397339   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.706462025642395   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7713419198989868   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.6725178956985474   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "Final val acc for lr=0.007520305204984156, wd=0.0002300292096821906: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.007520305204984156, wd=0.0002300292096821906, betas=(0.9, 0.98)\n",
      "iteration: 1/9   loss = 0.7779476046562195   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 2/9   loss = 2.1960225105285645   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.9042851328849792   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7998968362808228   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.8196336030960083   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.5666664838790894   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7584449052810669   train_acc = 0.5522388059701493   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7151352167129517   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.7225360870361328   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "Final val acc for lr=0.007520305204984156, wd=0.0002300292096821906: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.007520305204984156, wd=0.0002300292096821906, betas=(0.9, 0.99)\n",
      "iteration: 1/9   loss = 0.7179107069969177   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 2/9   loss = 1.4225311279296875   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 3/9   loss = 1.3456929922103882   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7722561359405518   train_acc = 0.5671641791044776   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.6615965366363525   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.6365231275558472   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7444825768470764   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.8329415321350098   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.7875944972038269   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "Final val acc for lr=0.007520305204984156, wd=0.0002300292096821906: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.007520305204984156, wd=0.0002300292096821906, betas=(0.9, 0.995)\n",
      "iteration: 1/9   loss = 0.725345253944397   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 2/9   loss = 1.7221623659133911   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.7111908793449402   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.9126496911048889   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7102205753326416   train_acc = 0.5671641791044776   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.7191791534423828   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.713280975818634   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.8451676964759827   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 9/9   loss = 1.0347371101379395   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "Final val acc for lr=0.007520305204984156, wd=0.0002300292096821906: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.007520305204984156, wd=0.0002300292096821906, betas=(0.9, 0.999)\n",
      "iteration: 1/9   loss = 0.790063738822937   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 2/9   loss = 1.1921427249908447   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 3/9   loss = 1.5268323421478271   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7343131899833679   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.719099760055542   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.7337368726730347   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7647746801376343   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.6972933411598206   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.6819562315940857   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "Final val acc for lr=0.007520305204984156, wd=0.0002300292096821906: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.007520305204984156, wd=0.0002300292096821906, betas=(0.95, 0.98)\n",
      "iteration: 1/9   loss = 0.7432053685188293   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 2/9   loss = 1.3781273365020752   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.6949396133422852   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7544291019439697   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7456047534942627   train_acc = 0.5447761194029851   val_acc = 0.7142857142857143\n",
      "iteration: 6/9   loss = 0.8009678721427917   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7748230695724487   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.6576211452484131   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.6604270339012146   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "Final val acc for lr=0.007520305204984156, wd=0.0002300292096821906: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.007520305204984156, wd=0.0002300292096821906, betas=(0.95, 0.99)\n",
      "iteration: 1/9   loss = 0.7212940454483032   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 2/9   loss = 2.0016636848449707   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.8662527799606323   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.691852331161499   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7008867263793945   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.6216829419136047   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7092168927192688   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7536582350730896   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.7565087080001831   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "Final val acc for lr=0.007520305204984156, wd=0.0002300292096821906: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.007520305204984156, wd=0.0002300292096821906, betas=(0.95, 0.995)\n",
      "iteration: 1/9   loss = 0.7184408903121948   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.998873233795166   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 3/9   loss = 1.5980008840560913   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.990416407585144   train_acc = 0.6492537313432836   val_acc = 0.6428571428571429\n",
      "iteration: 5/9   loss = 0.6742348670959473   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.7214633822441101   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.8735082149505615   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7610872983932495   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.527982771396637   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "Final val acc for lr=0.007520305204984156, wd=0.0002300292096821906: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.007520305204984156, wd=0.0002300292096821906, betas=(0.95, 0.999)\n",
      "iteration: 1/9   loss = 0.7173259854316711   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 2/9   loss = 1.2626302242279053   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.8402807116508484   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 4/9   loss = 1.066056489944458   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.8729126453399658   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.6950780749320984   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.6885725259780884   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.818436861038208   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 9/9   loss = 1.1936190128326416   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "Final val acc for lr=0.007520305204984156, wd=0.0002300292096821906: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.007520305204984156, wd=0.00022555743977178082, betas=(0.85, 0.99)\n",
      "iteration: 1/9   loss = 0.7454988956451416   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 2/9   loss = 1.0300880670547485   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 3/9   loss = 1.516826868057251   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 4/9   loss = 1.1233515739440918   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.9690107703208923   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.6386892199516296   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7001490592956543   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.740951657295227   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.8857583403587341   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "Final val acc for lr=0.007520305204984156, wd=0.00022555743977178082: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.007520305204984156, wd=0.00022555743977178082, betas=(0.85, 0.995)\n",
      "iteration: 1/9   loss = 0.7338507175445557   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 2/9   loss = 1.2702181339263916   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 3/9   loss = 1.458905577659607   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7892584800720215   train_acc = 0.6044776119402985   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.7023812532424927   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.6919376850128174   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7368924021720886   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7974140644073486   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.6318652629852295   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "Final val acc for lr=0.007520305204984156, wd=0.00022555743977178082: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.007520305204984156, wd=0.00022555743977178082, betas=(0.9, 0.98)\n",
      "iteration: 1/9   loss = 0.6791172623634338   train_acc = 0.47761194029850745   val_acc = 0.6428571428571429\n",
      "iteration: 2/9   loss = 0.7748905420303345   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.6168347001075745   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 4/9   loss = 1.3748990297317505   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.8178804516792297   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.7800905108451843   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7488523721694946   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 8/9   loss = 1.2632534503936768   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.7518424391746521   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "Final val acc for lr=0.007520305204984156, wd=0.00022555743977178082: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.007520305204984156, wd=0.00022555743977178082, betas=(0.9, 0.99)\n",
      "iteration: 1/9   loss = 0.8006539344787598   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 2/9   loss = 1.5585405826568604   train_acc = 0.6268656716417911   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.7186352610588074   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 4/9   loss = 1.1577913761138916   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.8761804699897766   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 6/9   loss = 1.344994306564331   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7438321113586426   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.698089063167572   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.5837398767471313   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "Final val acc for lr=0.007520305204984156, wd=0.00022555743977178082: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.007520305204984156, wd=0.00022555743977178082, betas=(0.9, 0.995)\n",
      "iteration: 1/9   loss = 0.6758577823638916   train_acc = 0.582089552238806   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.8012232184410095   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 3/9   loss = 2.272047758102417   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 4/9   loss = 1.035093069076538   train_acc = 0.5223880597014925   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7047220468521118   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.5313236117362976   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.8937984108924866   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 8/9   loss = 1.0543622970581055   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.7275162935256958   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "Final val acc for lr=0.007520305204984156, wd=0.00022555743977178082: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.007520305204984156, wd=0.00022555743977178082, betas=(0.9, 0.999)\n",
      "iteration: 1/9   loss = 0.7223588824272156   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 2/9   loss = 1.1019022464752197   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.832288920879364   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 4/9   loss = 1.1017403602600098   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.9213093519210815   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.7963215112686157   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7860186100006104   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7997450828552246   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.6336227059364319   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "Final val acc for lr=0.007520305204984156, wd=0.00022555743977178082: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.007520305204984156, wd=0.00022555743977178082, betas=(0.95, 0.98)\n",
      "iteration: 1/9   loss = 0.7069950103759766   train_acc = 0.582089552238806   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.6903839707374573   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.9484824538230896   train_acc = 0.5970149253731343   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.683594286441803   train_acc = 0.6268656716417911   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.7124305963516235   train_acc = 0.6119402985074627   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.6022884845733643   train_acc = 0.5373134328358209   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.8405534625053406   train_acc = 0.5522388059701493   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7134244441986084   train_acc = 0.5746268656716418   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.7923490405082703   train_acc = 0.6044776119402985   val_acc = 0.5\n",
      "Final val acc for lr=0.007520305204984156, wd=0.00022555743977178082: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.007520305204984156, wd=0.00022555743977178082, betas=(0.95, 0.99)\n",
      "iteration: 1/9   loss = 0.7952591180801392   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 2/9   loss = 1.9751843214035034   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.6839906573295593   train_acc = 0.5671641791044776   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.6912780404090881   train_acc = 0.6567164179104478   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.704255223274231   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.7719671726226807   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.6730808019638062   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7406402826309204   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.7325307726860046   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "Final val acc for lr=0.007520305204984156, wd=0.00022555743977178082: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.007520305204984156, wd=0.00022555743977178082, betas=(0.95, 0.995)\n",
      "iteration: 1/9   loss = 0.7213034629821777   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 2/9   loss = 2.186617851257324   train_acc = 0.5746268656716418   val_acc = 0.2857142857142857\n",
      "iteration: 3/9   loss = 0.5748351216316223   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.8897691965103149   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7559280395507812   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.9749755859375   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.6897854804992676   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.730320930480957   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.7145565748214722   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "Final val acc for lr=0.007520305204984156, wd=0.00022555743977178082: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.007520305204984156, wd=0.00022555743977178082, betas=(0.95, 0.999)\n",
      "iteration: 1/9   loss = 0.7203066349029541   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 2/9   loss = 1.5222055912017822   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.6317498683929443   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 4/9   loss = 1.2543392181396484   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7074592113494873   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.5955724716186523   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.6982347965240479   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7011266946792603   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.9457113146781921   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "Final val acc for lr=0.007520305204984156, wd=0.00022555743977178082: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.007520305204984156, wd=0.00011822143451838108, betas=(0.85, 0.99)\n",
      "iteration: 1/9   loss = 0.641983687877655   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 2/9   loss = 1.3873262405395508   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.5158532857894897   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 4/9   loss = 1.4325860738754272   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.8492180705070496   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.828667163848877   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.804000973701477   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.8357163667678833   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.6332054734230042   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "Final val acc for lr=0.007520305204984156, wd=0.00011822143451838108: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.007520305204984156, wd=0.00011822143451838108, betas=(0.85, 0.995)\n",
      "iteration: 1/9   loss = 0.7129784822463989   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.9308024644851685   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 3/9   loss = 2.4683172702789307   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 4/9   loss = 1.0185991525650024   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.6900558471679688   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.8052825927734375   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.6944093704223633   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.8059480786323547   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.9813780784606934   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "Final val acc for lr=0.007520305204984156, wd=0.00011822143451838108: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.007520305204984156, wd=0.00011822143451838108, betas=(0.9, 0.98)\n",
      "iteration: 1/9   loss = 0.7285784482955933   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.9087305068969727   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 3/9   loss = 2.1144254207611084   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 4/9   loss = 1.1478687524795532   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7555711269378662   train_acc = 0.5597014925373134   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.7284955382347107   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7333453893661499   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7587318420410156   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 9/9   loss = 1.1269357204437256   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "Final val acc for lr=0.007520305204984156, wd=0.00011822143451838108: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.007520305204984156, wd=0.00011822143451838108, betas=(0.9, 0.99)\n",
      "iteration: 1/9   loss = 0.6529847383499146   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 2/9   loss = 1.0896878242492676   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.5740211606025696   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 4/9   loss = 1.6960241794586182   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 5/9   loss = 1.476224422454834   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.9298478364944458   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.6946578025817871   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.690653920173645   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.8323769569396973   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "Final val acc for lr=0.007520305204984156, wd=0.00011822143451838108: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.007520305204984156, wd=0.00011822143451838108, betas=(0.9, 0.995)\n",
      "iteration: 1/9   loss = 0.7612847089767456   train_acc = 0.6044776119402985   val_acc = 0.2857142857142857\n",
      "iteration: 2/9   loss = 0.7057836055755615   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 3/9   loss = 1.9798221588134766   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.737779438495636   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.706847071647644   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.8546513319015503   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.8355361223220825   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7180861234664917   train_acc = 0.6865671641791045   val_acc = 0.35714285714285715\n",
      "iteration: 9/9   loss = 0.7067825794219971   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "Final val acc for lr=0.007520305204984156, wd=0.00011822143451838108: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.007520305204984156, wd=0.00011822143451838108, betas=(0.9, 0.999)\n",
      "iteration: 1/9   loss = 0.6925384402275085   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 2/9   loss = 1.5051614046096802   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 3/9   loss = 1.1021857261657715   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 4/9   loss = 1.4326753616333008   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.8205429315567017   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.8422205448150635   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7143077850341797   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7853466868400574   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.942539393901825   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "Final val acc for lr=0.007520305204984156, wd=0.00011822143451838108: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.007520305204984156, wd=0.00011822143451838108, betas=(0.95, 0.98)\n",
      "iteration: 1/9   loss = 0.6835392117500305   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 2/9   loss = 1.2237664461135864   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 3/9   loss = 1.4012162685394287   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7932007312774658   train_acc = 0.5746268656716418   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.6715356111526489   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.7220256328582764   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.8690228462219238   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.8494012355804443   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.38157862424850464   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "Final val acc for lr=0.007520305204984156, wd=0.00011822143451838108: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.007520305204984156, wd=0.00011822143451838108, betas=(0.95, 0.99)\n",
      "iteration: 1/9   loss = 0.7410215139389038   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.816562831401825   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 3/9   loss = 2.6104414463043213   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 4/9   loss = 1.2057011127471924   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.8768534660339355   train_acc = 0.5746268656716418   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.7711212038993835   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7744911313056946   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.8764737844467163   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.44871753454208374   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "Final val acc for lr=0.007520305204984156, wd=0.00011822143451838108: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.007520305204984156, wd=0.00011822143451838108, betas=(0.95, 0.995)\n",
      "iteration: 1/9   loss = 0.640143871307373   train_acc = 0.6194029850746269   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.6359573006629944   train_acc = 0.5298507462686567   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.5953997373580933   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.818430483341217   train_acc = 0.6268656716417911   val_acc = 0.5714285714285714\n",
      "iteration: 5/9   loss = 0.7400200366973877   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.7799383997917175   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.8681976199150085   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7939364910125732   train_acc = 0.7164179104477612   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.5824210047721863   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "Final val acc for lr=0.007520305204984156, wd=0.00011822143451838108: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.007520305204984156, wd=0.00011822143451838108, betas=(0.95, 0.999)\n",
      "iteration: 1/9   loss = 0.7255607843399048   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7473971843719482   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 3/9   loss = 1.7810018062591553   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7259225845336914   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.8144509196281433   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.5061579942703247   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.8917226195335388   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.8962351083755493   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.5313225984573364   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "Final val acc for lr=0.007520305204984156, wd=0.00011822143451838108: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.007520305204984156, wd=0.00032736198184168545, betas=(0.85, 0.99)\n",
      "iteration: 1/9   loss = 0.7274067997932434   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 2/9   loss = 1.4252105951309204   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.36073535680770874   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 4/9   loss = 1.0685889720916748   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 5/9   loss = 1.2245241403579712   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.8703445196151733   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7256112098693848   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7349767684936523   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.8686506152153015   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "Final val acc for lr=0.007520305204984156, wd=0.00032736198184168545: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.007520305204984156, wd=0.00032736198184168545, betas=(0.85, 0.995)\n",
      "iteration: 1/9   loss = 0.6946493983268738   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 2/9   loss = 1.2368015050888062   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 3/9   loss = 1.3028419017791748   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 4/9   loss = 1.5312378406524658   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.8022197484970093   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.6778007745742798   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7231403589248657   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.8009491562843323   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.8240827918052673   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "Final val acc for lr=0.007520305204984156, wd=0.00032736198184168545: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.007520305204984156, wd=0.00032736198184168545, betas=(0.9, 0.98)\n",
      "iteration: 1/9   loss = 0.7418972849845886   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.752547025680542   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 3/9   loss = 2.035672903060913   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7740702629089355   train_acc = 0.5746268656716418   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.6565117835998535   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.9893369674682617   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7906875610351562   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.764354944229126   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.8208715915679932   train_acc = 0.582089552238806   val_acc = 0.5\n",
      "Final val acc for lr=0.007520305204984156, wd=0.00032736198184168545: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.007520305204984156, wd=0.00032736198184168545, betas=(0.9, 0.99)\n",
      "iteration: 1/9   loss = 0.7327219247817993   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.69631427526474   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.7585406303405762   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.8292109966278076   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.8952696323394775   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.8645676374435425   train_acc = 0.5522388059701493   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.6711981892585754   train_acc = 0.6119402985074627   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.6694165468215942   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.6884950399398804   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "Final val acc for lr=0.007520305204984156, wd=0.00032736198184168545: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.007520305204984156, wd=0.00032736198184168545, betas=(0.9, 0.995)\n",
      "iteration: 1/9   loss = 0.7279772758483887   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 2/9   loss = 1.485158085823059   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.7747600078582764   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.8954278230667114   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7353827953338623   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.7684166431427002   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7516176700592041   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7749429941177368   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.590640664100647   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "Final val acc for lr=0.007520305204984156, wd=0.00032736198184168545: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.007520305204984156, wd=0.00032736198184168545, betas=(0.9, 0.999)\n",
      "iteration: 1/9   loss = 0.6805820465087891   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7933509349822998   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 3/9   loss = 2.3895936012268066   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 4/9   loss = 1.2522521018981934   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.8273456692695618   train_acc = 0.5298507462686567   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.6744369864463806   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7730149030685425   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.9022457599639893   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.8211457133293152   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "Final val acc for lr=0.007520305204984156, wd=0.00032736198184168545: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.007520305204984156, wd=0.00032736198184168545, betas=(0.95, 0.98)\n",
      "iteration: 1/9   loss = 0.6940162181854248   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.8266963958740234   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 3/9   loss = 1.6704163551330566   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 4/9   loss = 1.2558188438415527   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.8125720024108887   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.8040038347244263   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7564144730567932   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7428110837936401   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.48411640524864197   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "Final val acc for lr=0.007520305204984156, wd=0.00032736198184168545: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.007520305204984156, wd=0.00032736198184168545, betas=(0.95, 0.99)\n",
      "iteration: 1/9   loss = 0.7138873338699341   train_acc = 0.582089552238806   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7310036420822144   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 3/9   loss = 2.4085655212402344   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.6698938608169556   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.727193295955658   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.8096720576286316   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.8449364900588989   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7012468576431274   train_acc = 0.582089552238806   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.6947618722915649   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "Final val acc for lr=0.007520305204984156, wd=0.00032736198184168545: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.007520305204984156, wd=0.00032736198184168545, betas=(0.95, 0.995)\n",
      "iteration: 1/9   loss = 0.7178186178207397   train_acc = 0.5522388059701493   val_acc = 0.35714285714285715\n",
      "iteration: 2/9   loss = 0.7848234176635742   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 3/9   loss = 1.1987305879592896   train_acc = 0.5895522388059702   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.6617140769958496   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 5/9   loss = 1.0125706195831299   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 6/9   loss = 1.208702802658081   train_acc = 0.5746268656716418   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.6998950839042664   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.6899076700210571   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.9597222208976746   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "Final val acc for lr=0.007520305204984156, wd=0.00032736198184168545: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.007520305204984156, wd=0.00032736198184168545, betas=(0.95, 0.999)\n",
      "iteration: 1/9   loss = 0.7583913207054138   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 2/9   loss = 1.9068927764892578   train_acc = 0.5298507462686567   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.819999098777771   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.9367643594741821   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.8537757396697998   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 6/9   loss = 1.0690667629241943   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7064906358718872   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.6905480623245239   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.9304327964782715   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "Final val acc for lr=0.007520305204984156, wd=0.00032736198184168545: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.0002368042204919325, wd=0.0004609389889268166, betas=(0.85, 0.99)\n",
      "iteration: 1/9   loss = 0.726911723613739   train_acc = 0.5746268656716418   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.7067453861236572   train_acc = 0.5895522388059702   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.7593748569488525   train_acc = 0.6567164179104478   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.6615967750549316   train_acc = 0.6492537313432836   val_acc = 0.35714285714285715\n",
      "iteration: 5/9   loss = 0.615861177444458   train_acc = 0.6492537313432836   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.8657528758049011   train_acc = 0.6417910447761194   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.6578379273414612   train_acc = 0.7089552238805971   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.6371674537658691   train_acc = 0.7014925373134329   val_acc = 0.35714285714285715\n",
      "iteration: 9/9   loss = 0.44726377725601196   train_acc = 0.6567164179104478   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=0.0002368042204919325, wd=0.0004609389889268166: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.0002368042204919325, wd=0.0004609389889268166, betas=(0.85, 0.995)\n",
      "iteration: 1/9   loss = 0.6633268594741821   train_acc = 0.5970149253731343   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7278994917869568   train_acc = 0.582089552238806   val_acc = 0.21428571428571427\n",
      "iteration: 3/9   loss = 0.6696470975875854   train_acc = 0.5970149253731343   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7274314165115356   train_acc = 0.6194029850746269   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.6231383085250854   train_acc = 0.6417910447761194   val_acc = 0.35714285714285715\n",
      "iteration: 6/9   loss = 0.67963045835495   train_acc = 0.6044776119402985   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7275524139404297   train_acc = 0.6268656716417911   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.5797353982925415   train_acc = 0.6119402985074627   val_acc = 0.35714285714285715\n",
      "iteration: 9/9   loss = 0.8228540420532227   train_acc = 0.7014925373134329   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=0.0002368042204919325, wd=0.0004609389889268166: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.0002368042204919325, wd=0.0004609389889268166, betas=(0.9, 0.98)\n",
      "iteration: 1/9   loss = 0.6472401022911072   train_acc = 0.5373134328358209   val_acc = 0.35714285714285715\n",
      "iteration: 2/9   loss = 0.7544875144958496   train_acc = 0.6268656716417911   val_acc = 0.35714285714285715\n",
      "iteration: 3/9   loss = 0.5166828632354736   train_acc = 0.5970149253731343   val_acc = 0.2857142857142857\n",
      "iteration: 4/9   loss = 0.6456769704818726   train_acc = 0.6268656716417911   val_acc = 0.2857142857142857\n",
      "iteration: 5/9   loss = 0.6388165950775146   train_acc = 0.6194029850746269   val_acc = 0.35714285714285715\n",
      "iteration: 6/9   loss = 0.6277289390563965   train_acc = 0.6492537313432836   val_acc = 0.35714285714285715\n",
      "iteration: 7/9   loss = 0.6232743263244629   train_acc = 0.6417910447761194   val_acc = 0.35714285714285715\n",
      "iteration: 8/9   loss = 0.615013837814331   train_acc = 0.6268656716417911   val_acc = 0.21428571428571427\n",
      "iteration: 9/9   loss = 0.4416024684906006   train_acc = 0.6492537313432836   val_acc = 0.35714285714285715\n",
      "Final val acc for lr=0.0002368042204919325, wd=0.0004609389889268166: 0.3571\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.0002368042204919325, wd=0.0004609389889268166, betas=(0.9, 0.99)\n",
      "iteration: 1/9   loss = 0.7025545835494995   train_acc = 0.664179104477612   val_acc = 0.35714285714285715\n",
      "iteration: 2/9   loss = 0.7154721617698669   train_acc = 0.6716417910447762   val_acc = 0.21428571428571427\n",
      "iteration: 3/9   loss = 0.7061702609062195   train_acc = 0.6492537313432836   val_acc = 0.35714285714285715\n",
      "iteration: 4/9   loss = 0.6146953701972961   train_acc = 0.6791044776119403   val_acc = 0.35714285714285715\n",
      "iteration: 5/9   loss = 0.7072532773017883   train_acc = 0.6865671641791045   val_acc = 0.35714285714285715\n",
      "iteration: 6/9   loss = 0.7201398611068726   train_acc = 0.6194029850746269   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.5656402707099915   train_acc = 0.6268656716417911   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.6927843689918518   train_acc = 0.6044776119402985   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.7437732219696045   train_acc = 0.6194029850746269   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=0.0002368042204919325, wd=0.0004609389889268166: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.0002368042204919325, wd=0.0004609389889268166, betas=(0.9, 0.995)\n",
      "iteration: 1/9   loss = 0.7523835897445679   train_acc = 0.5373134328358209   val_acc = 0.6428571428571429\n",
      "iteration: 2/9   loss = 0.7028928399085999   train_acc = 0.6343283582089553   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.7328181266784668   train_acc = 0.6940298507462687   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.6485641002655029   train_acc = 0.6044776119402985   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.648785412311554   train_acc = 0.6044776119402985   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.7363728284835815   train_acc = 0.582089552238806   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.6542178392410278   train_acc = 0.5895522388059702   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.6361804604530334   train_acc = 0.5970149253731343   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.5997138619422913   train_acc = 0.6417910447761194   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=0.0002368042204919325, wd=0.0004609389889268166: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.0002368042204919325, wd=0.0004609389889268166, betas=(0.9, 0.999)\n",
      "iteration: 1/9   loss = 0.6817441582679749   train_acc = 0.5895522388059702   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7195810675621033   train_acc = 0.5671641791044776   val_acc = 0.5714285714285714\n",
      "iteration: 3/9   loss = 0.6650146245956421   train_acc = 0.582089552238806   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.6667753458023071   train_acc = 0.6268656716417911   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.6269487142562866   train_acc = 0.6417910447761194   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.8123180270195007   train_acc = 0.6940298507462687   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.6338860392570496   train_acc = 0.6567164179104478   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.7044059038162231   train_acc = 0.6492537313432836   val_acc = 0.42857142857142855\n",
      "iteration: 9/9   loss = 0.616497278213501   train_acc = 0.6940298507462687   val_acc = 0.5\n",
      "Final val acc for lr=0.0002368042204919325, wd=0.0004609389889268166: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.0002368042204919325, wd=0.0004609389889268166, betas=(0.95, 0.98)\n",
      "iteration: 1/9   loss = 0.7339287996292114   train_acc = 0.5671641791044776   val_acc = 0.2857142857142857\n",
      "iteration: 2/9   loss = 0.7710487842559814   train_acc = 0.6044776119402985   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.7030420303344727   train_acc = 0.7014925373134329   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.6226204633712769   train_acc = 0.6865671641791045   val_acc = 0.5714285714285714\n",
      "iteration: 5/9   loss = 0.6507710218429565   train_acc = 0.6716417910447762   val_acc = 0.35714285714285715\n",
      "iteration: 6/9   loss = 0.6591441631317139   train_acc = 0.7089552238805971   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.679620623588562   train_acc = 0.7014925373134329   val_acc = 0.6428571428571429\n",
      "iteration: 8/9   loss = 0.6183424592018127   train_acc = 0.6716417910447762   val_acc = 0.6428571428571429\n",
      "iteration: 9/9   loss = 0.6899503469467163   train_acc = 0.7388059701492538   val_acc = 0.6428571428571429\n",
      "Final val acc for lr=0.0002368042204919325, wd=0.0004609389889268166: 0.6429\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.0002368042204919325, wd=0.0004609389889268166, betas=(0.95, 0.99)\n",
      "iteration: 1/9   loss = 0.6914047598838806   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7158986330032349   train_acc = 0.5970149253731343   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.794344425201416   train_acc = 0.664179104477612   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.6780368089675903   train_acc = 0.6268656716417911   val_acc = 0.6428571428571429\n",
      "iteration: 5/9   loss = 0.6935924887657166   train_acc = 0.664179104477612   val_acc = 0.7142857142857143\n",
      "iteration: 6/9   loss = 0.7632428407669067   train_acc = 0.6268656716417911   val_acc = 0.7142857142857143\n",
      "iteration: 7/9   loss = 0.6815146207809448   train_acc = 0.6940298507462687   val_acc = 0.6428571428571429\n",
      "iteration: 8/9   loss = 0.5949245691299438   train_acc = 0.6194029850746269   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.7228164672851562   train_acc = 0.6268656716417911   val_acc = 0.5\n",
      "Final val acc for lr=0.0002368042204919325, wd=0.0004609389889268166: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.0002368042204919325, wd=0.0004609389889268166, betas=(0.95, 0.995)\n",
      "iteration: 1/9   loss = 0.7254887819290161   train_acc = 0.6194029850746269   val_acc = 0.35714285714285715\n",
      "iteration: 2/9   loss = 0.7101905345916748   train_acc = 0.664179104477612   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.7751225829124451   train_acc = 0.6268656716417911   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.6778836846351624   train_acc = 0.6417910447761194   val_acc = 0.5714285714285714\n",
      "iteration: 5/9   loss = 0.6879081130027771   train_acc = 0.7238805970149254   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.690498948097229   train_acc = 0.6865671641791045   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.6579623222351074   train_acc = 0.7014925373134329   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.6553874015808105   train_acc = 0.664179104477612   val_acc = 0.35714285714285715\n",
      "iteration: 9/9   loss = 0.5542693138122559   train_acc = 0.6343283582089553   val_acc = 0.6428571428571429\n",
      "Final val acc for lr=0.0002368042204919325, wd=0.0004609389889268166: 0.6429\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.0002368042204919325, wd=0.0004609389889268166, betas=(0.95, 0.999)\n",
      "iteration: 1/9   loss = 0.7034034729003906   train_acc = 0.5149253731343284   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.7638710737228394   train_acc = 0.5895522388059702   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.5055029392242432   train_acc = 0.6119402985074627   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.6914172172546387   train_acc = 0.6567164179104478   val_acc = 0.35714285714285715\n",
      "iteration: 5/9   loss = 0.6432117223739624   train_acc = 0.7164179104477612   val_acc = 0.2857142857142857\n",
      "iteration: 6/9   loss = 0.658034086227417   train_acc = 0.6567164179104478   val_acc = 0.21428571428571427\n",
      "iteration: 7/9   loss = 0.6619338989257812   train_acc = 0.6865671641791045   val_acc = 0.35714285714285715\n",
      "iteration: 8/9   loss = 0.5970906615257263   train_acc = 0.664179104477612   val_acc = 0.2857142857142857\n",
      "iteration: 9/9   loss = 0.5109962224960327   train_acc = 0.6865671641791045   val_acc = 0.21428571428571427\n",
      "Final val acc for lr=0.0002368042204919325, wd=0.0004609389889268166: 0.2143\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.0002368042204919325, wd=0.0002300292096821906, betas=(0.85, 0.99)\n",
      "iteration: 1/9   loss = 0.7672162652015686   train_acc = 0.6044776119402985   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.7509347200393677   train_acc = 0.5970149253731343   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.5972188711166382   train_acc = 0.5746268656716418   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.6651300191879272   train_acc = 0.5970149253731343   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7043963074684143   train_acc = 0.6567164179104478   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.6416716575622559   train_acc = 0.6716417910447762   val_acc = 0.35714285714285715\n",
      "iteration: 7/9   loss = 0.6135562062263489   train_acc = 0.664179104477612   val_acc = 0.2857142857142857\n",
      "iteration: 8/9   loss = 0.6002368927001953   train_acc = 0.6194029850746269   val_acc = 0.2857142857142857\n",
      "iteration: 9/9   loss = 0.7542187571525574   train_acc = 0.6492537313432836   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=0.0002368042204919325, wd=0.0002300292096821906: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.0002368042204919325, wd=0.0002300292096821906, betas=(0.85, 0.995)\n",
      "iteration: 1/9   loss = 0.7457002401351929   train_acc = 0.5671641791044776   val_acc = 0.7142857142857143\n",
      "iteration: 2/9   loss = 0.662031888961792   train_acc = 0.5970149253731343   val_acc = 0.5714285714285714\n",
      "iteration: 3/9   loss = 0.6510391235351562   train_acc = 0.6268656716417911   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.6790886521339417   train_acc = 0.6716417910447762   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.6658964157104492   train_acc = 0.6194029850746269   val_acc = 0.35714285714285715\n",
      "iteration: 6/9   loss = 0.6901809573173523   train_acc = 0.6417910447761194   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.6975263953208923   train_acc = 0.6417910447761194   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.6082902550697327   train_acc = 0.6417910447761194   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.6242382526397705   train_acc = 0.6119402985074627   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=0.0002368042204919325, wd=0.0002300292096821906: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.0002368042204919325, wd=0.0002300292096821906, betas=(0.9, 0.98)\n",
      "iteration: 1/9   loss = 0.7148357629776001   train_acc = 0.5373134328358209   val_acc = 0.6428571428571429\n",
      "iteration: 2/9   loss = 0.7306519150733948   train_acc = 0.5895522388059702   val_acc = 0.6428571428571429\n",
      "iteration: 3/9   loss = 0.7259604334831238   train_acc = 0.664179104477612   val_acc = 0.35714285714285715\n",
      "iteration: 4/9   loss = 0.6905007362365723   train_acc = 0.6716417910447762   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.634539008140564   train_acc = 0.6567164179104478   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.8900236487388611   train_acc = 0.6940298507462687   val_acc = 0.6428571428571429\n",
      "iteration: 7/9   loss = 0.6583150625228882   train_acc = 0.6119402985074627   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.6443506479263306   train_acc = 0.6716417910447762   val_acc = 0.7142857142857143\n",
      "iteration: 9/9   loss = 0.5729168653488159   train_acc = 0.7164179104477612   val_acc = 0.5\n",
      "Final val acc for lr=0.0002368042204919325, wd=0.0002300292096821906: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.0002368042204919325, wd=0.0002300292096821906, betas=(0.9, 0.99)\n",
      "iteration: 1/9   loss = 0.6872700452804565   train_acc = 0.5970149253731343   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.747377872467041   train_acc = 0.5597014925373134   val_acc = 0.6428571428571429\n",
      "iteration: 3/9   loss = 0.7480363249778748   train_acc = 0.6716417910447762   val_acc = 0.35714285714285715\n",
      "iteration: 4/9   loss = 0.6337419748306274   train_acc = 0.5970149253731343   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.6481635570526123   train_acc = 0.5970149253731343   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.4907835125923157   train_acc = 0.6044776119402985   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.6761595010757446   train_acc = 0.6044776119402985   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.6344964504241943   train_acc = 0.6492537313432836   val_acc = 0.42857142857142855\n",
      "iteration: 9/9   loss = 0.8280444145202637   train_acc = 0.6417910447761194   val_acc = 0.5\n",
      "Final val acc for lr=0.0002368042204919325, wd=0.0002300292096821906: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.0002368042204919325, wd=0.0002300292096821906, betas=(0.9, 0.995)\n",
      "iteration: 1/9   loss = 0.7309862375259399   train_acc = 0.5522388059701493   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7011143565177917   train_acc = 0.6194029850746269   val_acc = 0.2857142857142857\n",
      "iteration: 3/9   loss = 0.7475270628929138   train_acc = 0.6119402985074627   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.6952210664749146   train_acc = 0.582089552238806   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.6592698097229004   train_acc = 0.582089552238806   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.662051796913147   train_acc = 0.6268656716417911   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.6349979043006897   train_acc = 0.6716417910447762   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.6324096322059631   train_acc = 0.6791044776119403   val_acc = 0.21428571428571427\n",
      "iteration: 9/9   loss = 0.7397341132164001   train_acc = 0.6492537313432836   val_acc = 0.35714285714285715\n",
      "Final val acc for lr=0.0002368042204919325, wd=0.0002300292096821906: 0.3571\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.0002368042204919325, wd=0.0002300292096821906, betas=(0.9, 0.999)\n",
      "iteration: 1/9   loss = 0.715232253074646   train_acc = 0.5671641791044776   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7196524143218994   train_acc = 0.6865671641791045   val_acc = 0.6428571428571429\n",
      "iteration: 3/9   loss = 0.557064950466156   train_acc = 0.6492537313432836   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.6182944774627686   train_acc = 0.6268656716417911   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.652758002281189   train_acc = 0.6417910447761194   val_acc = 0.5714285714285714\n",
      "iteration: 6/9   loss = 0.5284156799316406   train_acc = 0.6940298507462687   val_acc = 0.7142857142857143\n",
      "iteration: 7/9   loss = 0.6084491014480591   train_acc = 0.6716417910447762   val_acc = 0.6428571428571429\n",
      "iteration: 8/9   loss = 0.648923397064209   train_acc = 0.6417910447761194   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.45211338996887207   train_acc = 0.6716417910447762   val_acc = 0.5714285714285714\n",
      "Final val acc for lr=0.0002368042204919325, wd=0.0002300292096821906: 0.5714\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.0002368042204919325, wd=0.0002300292096821906, betas=(0.95, 0.98)\n",
      "iteration: 1/9   loss = 0.7054173946380615   train_acc = 0.5373134328358209   val_acc = 0.5714285714285714\n",
      "iteration: 2/9   loss = 0.7489407658576965   train_acc = 0.6417910447761194   val_acc = 0.5714285714285714\n",
      "iteration: 3/9   loss = 0.6558687090873718   train_acc = 0.6865671641791045   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.6741806268692017   train_acc = 0.664179104477612   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.6456129550933838   train_acc = 0.6343283582089553   val_acc = 0.2857142857142857\n",
      "iteration: 6/9   loss = 0.6250255107879639   train_acc = 0.6567164179104478   val_acc = 0.35714285714285715\n",
      "iteration: 7/9   loss = 0.6552088260650635   train_acc = 0.6119402985074627   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.6528719663619995   train_acc = 0.6044776119402985   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.5085819959640503   train_acc = 0.5746268656716418   val_acc = 0.5\n",
      "Final val acc for lr=0.0002368042204919325, wd=0.0002300292096821906: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.0002368042204919325, wd=0.0002300292096821906, betas=(0.95, 0.99)\n",
      "iteration: 1/9   loss = 0.6937940120697021   train_acc = 0.6268656716417911   val_acc = 0.35714285714285715\n",
      "iteration: 2/9   loss = 0.7089899182319641   train_acc = 0.6194029850746269   val_acc = 0.2857142857142857\n",
      "iteration: 3/9   loss = 0.8117269277572632   train_acc = 0.6492537313432836   val_acc = 0.2857142857142857\n",
      "iteration: 4/9   loss = 0.6673194766044617   train_acc = 0.6940298507462687   val_acc = 0.35714285714285715\n",
      "iteration: 5/9   loss = 0.6337864995002747   train_acc = 0.6865671641791045   val_acc = 0.5714285714285714\n",
      "iteration: 6/9   loss = 0.6118171215057373   train_acc = 0.6194029850746269   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.6721050143241882   train_acc = 0.5970149253731343   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.6494741439819336   train_acc = 0.6044776119402985   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.5483910441398621   train_acc = 0.6119402985074627   val_acc = 0.5\n",
      "Final val acc for lr=0.0002368042204919325, wd=0.0002300292096821906: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.0002368042204919325, wd=0.0002300292096821906, betas=(0.95, 0.995)\n",
      "iteration: 1/9   loss = 0.8040021657943726   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7870699763298035   train_acc = 0.5597014925373134   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.6892380118370056   train_acc = 0.5746268656716418   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.6761945486068726   train_acc = 0.5597014925373134   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.6956139802932739   train_acc = 0.6194029850746269   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.7761155962944031   train_acc = 0.6492537313432836   val_acc = 0.14285714285714285\n",
      "iteration: 7/9   loss = 0.636059582233429   train_acc = 0.6492537313432836   val_acc = 0.21428571428571427\n",
      "iteration: 8/9   loss = 0.620673656463623   train_acc = 0.6791044776119403   val_acc = 0.14285714285714285\n",
      "iteration: 9/9   loss = 0.7393977642059326   train_acc = 0.664179104477612   val_acc = 0.35714285714285715\n",
      "Final val acc for lr=0.0002368042204919325, wd=0.0002300292096821906: 0.3571\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.0002368042204919325, wd=0.0002300292096821906, betas=(0.95, 0.999)\n",
      "iteration: 1/9   loss = 0.7878528237342834   train_acc = 0.5970149253731343   val_acc = 0.5714285714285714\n",
      "iteration: 2/9   loss = 0.7189433574676514   train_acc = 0.664179104477612   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.6843470335006714   train_acc = 0.6716417910447762   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.5748700499534607   train_acc = 0.6343283582089553   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.668813943862915   train_acc = 0.6791044776119403   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.677788257598877   train_acc = 0.6716417910447762   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.6542930006980896   train_acc = 0.6865671641791045   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.6001052260398865   train_acc = 0.6567164179104478   val_acc = 0.42857142857142855\n",
      "iteration: 9/9   loss = 0.6261236667633057   train_acc = 0.6268656716417911   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=0.0002368042204919325, wd=0.0002300292096821906: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.0002368042204919325, wd=0.00022555743977178082, betas=(0.85, 0.99)\n",
      "iteration: 1/9   loss = 0.6734642386436462   train_acc = 0.6865671641791045   val_acc = 0.6428571428571429\n",
      "iteration: 2/9   loss = 0.7833503484725952   train_acc = 0.6119402985074627   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.6664330959320068   train_acc = 0.6492537313432836   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.6800490617752075   train_acc = 0.6417910447761194   val_acc = 0.7857142857142857\n",
      "iteration: 5/9   loss = 0.6342447996139526   train_acc = 0.7014925373134329   val_acc = 0.6428571428571429\n",
      "iteration: 6/9   loss = 0.5726059675216675   train_acc = 0.6940298507462687   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.5979709625244141   train_acc = 0.664179104477612   val_acc = 0.6428571428571429\n",
      "iteration: 8/9   loss = 0.5664609670639038   train_acc = 0.6865671641791045   val_acc = 0.7142857142857143\n",
      "iteration: 9/9   loss = 0.7026462554931641   train_acc = 0.6492537313432836   val_acc = 0.5\n",
      "Final val acc for lr=0.0002368042204919325, wd=0.00022555743977178082: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.0002368042204919325, wd=0.00022555743977178082, betas=(0.85, 0.995)\n",
      "iteration: 1/9   loss = 0.7908468246459961   train_acc = 0.582089552238806   val_acc = 0.35714285714285715\n",
      "iteration: 2/9   loss = 0.6563007831573486   train_acc = 0.5522388059701493   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.7163614630699158   train_acc = 0.5671641791044776   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.6854654550552368   train_acc = 0.5671641791044776   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.6952894330024719   train_acc = 0.6194029850746269   val_acc = 0.5714285714285714\n",
      "iteration: 6/9   loss = 0.593026876449585   train_acc = 0.664179104477612   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.7024917602539062   train_acc = 0.6567164179104478   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.6485722064971924   train_acc = 0.6119402985074627   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.4932878911495209   train_acc = 0.5895522388059702   val_acc = 0.5\n",
      "Final val acc for lr=0.0002368042204919325, wd=0.00022555743977178082: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.0002368042204919325, wd=0.00022555743977178082, betas=(0.9, 0.98)\n",
      "iteration: 1/9   loss = 0.6828963756561279   train_acc = 0.6044776119402985   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.6664075255393982   train_acc = 0.5970149253731343   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.7246649265289307   train_acc = 0.5671641791044776   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7242869138717651   train_acc = 0.5671641791044776   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.6371117830276489   train_acc = 0.5671641791044776   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.7497767210006714   train_acc = 0.5671641791044776   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.6751967668533325   train_acc = 0.5671641791044776   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.6456894874572754   train_acc = 0.5671641791044776   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.8780717849731445   train_acc = 0.5671641791044776   val_acc = 0.5\n",
      "Final val acc for lr=0.0002368042204919325, wd=0.00022555743977178082: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.0002368042204919325, wd=0.00022555743977178082, betas=(0.9, 0.99)\n",
      "iteration: 1/9   loss = 0.7656154036521912   train_acc = 0.6194029850746269   val_acc = 0.6428571428571429\n",
      "iteration: 2/9   loss = 0.718494176864624   train_acc = 0.582089552238806   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.612869143486023   train_acc = 0.582089552238806   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.7025206685066223   train_acc = 0.6194029850746269   val_acc = 0.5714285714285714\n",
      "iteration: 5/9   loss = 0.6185356974601746   train_acc = 0.6865671641791045   val_acc = 0.35714285714285715\n",
      "iteration: 6/9   loss = 0.5817142724990845   train_acc = 0.6865671641791045   val_acc = 0.2857142857142857\n",
      "iteration: 7/9   loss = 0.6343883872032166   train_acc = 0.6567164179104478   val_acc = 0.35714285714285715\n",
      "iteration: 8/9   loss = 0.6745741367340088   train_acc = 0.7014925373134329   val_acc = 0.2857142857142857\n",
      "iteration: 9/9   loss = 0.5627250671386719   train_acc = 0.7388059701492538   val_acc = 0.35714285714285715\n",
      "Final val acc for lr=0.0002368042204919325, wd=0.00022555743977178082: 0.3571\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.0002368042204919325, wd=0.00022555743977178082, betas=(0.9, 0.995)\n",
      "iteration: 1/9   loss = 0.7010462880134583   train_acc = 0.5298507462686567   val_acc = 0.6428571428571429\n",
      "iteration: 2/9   loss = 0.7743220925331116   train_acc = 0.6119402985074627   val_acc = 0.5714285714285714\n",
      "iteration: 3/9   loss = 0.8039131164550781   train_acc = 0.6343283582089553   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7074042558670044   train_acc = 0.6119402985074627   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.6314864158630371   train_acc = 0.6417910447761194   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.5678699016571045   train_acc = 0.5970149253731343   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.6582955121994019   train_acc = 0.5895522388059702   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.6177176833152771   train_acc = 0.582089552238806   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.7060062289237976   train_acc = 0.5970149253731343   val_acc = 0.5\n",
      "Final val acc for lr=0.0002368042204919325, wd=0.00022555743977178082: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.0002368042204919325, wd=0.00022555743977178082, betas=(0.9, 0.999)\n",
      "iteration: 1/9   loss = 0.7074103355407715   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7111826539039612   train_acc = 0.6343283582089553   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.5288591384887695   train_acc = 0.5671641791044776   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.6920040845870972   train_acc = 0.6119402985074627   val_acc = 0.5714285714285714\n",
      "iteration: 5/9   loss = 0.638779878616333   train_acc = 0.5895522388059702   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.8172800540924072   train_acc = 0.6119402985074627   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.6595213413238525   train_acc = 0.6940298507462687   val_acc = 0.6428571428571429\n",
      "iteration: 8/9   loss = 0.5577040910720825   train_acc = 0.6567164179104478   val_acc = 0.6428571428571429\n",
      "iteration: 9/9   loss = 0.8841266632080078   train_acc = 0.6492537313432836   val_acc = 0.5\n",
      "Final val acc for lr=0.0002368042204919325, wd=0.00022555743977178082: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.0002368042204919325, wd=0.00022555743977178082, betas=(0.95, 0.98)\n",
      "iteration: 1/9   loss = 0.7615577578544617   train_acc = 0.5671641791044776   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.7337325215339661   train_acc = 0.5298507462686567   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.6847630739212036   train_acc = 0.5522388059701493   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.6911635398864746   train_acc = 0.5746268656716418   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.6808262467384338   train_acc = 0.6268656716417911   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.7482528686523438   train_acc = 0.6194029850746269   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.6690549850463867   train_acc = 0.6119402985074627   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.6605948209762573   train_acc = 0.6492537313432836   val_acc = 0.42857142857142855\n",
      "iteration: 9/9   loss = 0.47553977370262146   train_acc = 0.6119402985074627   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=0.0002368042204919325, wd=0.00022555743977178082: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.0002368042204919325, wd=0.00022555743977178082, betas=(0.95, 0.99)\n",
      "iteration: 1/9   loss = 0.7145075798034668   train_acc = 0.6194029850746269   val_acc = 0.6428571428571429\n",
      "iteration: 2/9   loss = 0.7177797555923462   train_acc = 0.6268656716417911   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.6272026300430298   train_acc = 0.7089552238805971   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.6316747665405273   train_acc = 0.6940298507462687   val_acc = 0.7857142857142857\n",
      "iteration: 5/9   loss = 0.6405540704727173   train_acc = 0.6492537313432836   val_acc = 0.7142857142857143\n",
      "iteration: 6/9   loss = 0.6491520404815674   train_acc = 0.6567164179104478   val_acc = 0.6428571428571429\n",
      "iteration: 7/9   loss = 0.6458002924919128   train_acc = 0.6417910447761194   val_acc = 0.7142857142857143\n",
      "iteration: 8/9   loss = 0.6891312599182129   train_acc = 0.6865671641791045   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.5734637975692749   train_acc = 0.6791044776119403   val_acc = 0.5714285714285714\n",
      "Final val acc for lr=0.0002368042204919325, wd=0.00022555743977178082: 0.5714\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.0002368042204919325, wd=0.00022555743977178082, betas=(0.95, 0.995)\n",
      "iteration: 1/9   loss = 0.7721560597419739   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7171919941902161   train_acc = 0.5298507462686567   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.8464300036430359   train_acc = 0.6268656716417911   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.7032331228256226   train_acc = 0.6492537313432836   val_acc = 0.35714285714285715\n",
      "iteration: 5/9   loss = 0.6664549112319946   train_acc = 0.6119402985074627   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.4792230725288391   train_acc = 0.6417910447761194   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.664696455001831   train_acc = 0.6343283582089553   val_acc = 0.2857142857142857\n",
      "iteration: 8/9   loss = 0.660427987575531   train_acc = 0.7164179104477612   val_acc = 0.35714285714285715\n",
      "iteration: 9/9   loss = 0.6024867296218872   train_acc = 0.6417910447761194   val_acc = 0.6428571428571429\n",
      "Final val acc for lr=0.0002368042204919325, wd=0.00022555743977178082: 0.6429\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.0002368042204919325, wd=0.00022555743977178082, betas=(0.95, 0.999)\n",
      "iteration: 1/9   loss = 0.6623802781105042   train_acc = 0.582089552238806   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.705859899520874   train_acc = 0.6343283582089553   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.6439533829689026   train_acc = 0.6194029850746269   val_acc = 0.35714285714285715\n",
      "iteration: 4/9   loss = 0.6947101950645447   train_acc = 0.5970149253731343   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.646270751953125   train_acc = 0.5970149253731343   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.6721069812774658   train_acc = 0.582089552238806   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7093890905380249   train_acc = 0.5895522388059702   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.6059168577194214   train_acc = 0.5895522388059702   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.8820885419845581   train_acc = 0.6194029850746269   val_acc = 0.5\n",
      "Final val acc for lr=0.0002368042204919325, wd=0.00022555743977178082: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.0002368042204919325, wd=0.00011822143451838108, betas=(0.85, 0.99)\n",
      "iteration: 1/9   loss = 0.7663896083831787   train_acc = 0.5298507462686567   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.6865300536155701   train_acc = 0.5447761194029851   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.5926913022994995   train_acc = 0.5373134328358209   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7281492948532104   train_acc = 0.5895522388059702   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7258228063583374   train_acc = 0.6492537313432836   val_acc = 0.5714285714285714\n",
      "iteration: 6/9   loss = 0.5676000118255615   train_acc = 0.7089552238805971   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.6161022782325745   train_acc = 0.6567164179104478   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.628983736038208   train_acc = 0.6492537313432836   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.5724902153015137   train_acc = 0.6716417910447762   val_acc = 0.6428571428571429\n",
      "Final val acc for lr=0.0002368042204919325, wd=0.00011822143451838108: 0.6429\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.0002368042204919325, wd=0.00011822143451838108, betas=(0.85, 0.995)\n",
      "iteration: 1/9   loss = 0.7396426200866699   train_acc = 0.5223880597014925   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.7651202082633972   train_acc = 0.6194029850746269   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.6482911705970764   train_acc = 0.6194029850746269   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.6701791882514954   train_acc = 0.6791044776119403   val_acc = 0.5714285714285714\n",
      "iteration: 5/9   loss = 0.6591256856918335   train_acc = 0.6716417910447762   val_acc = 0.5714285714285714\n",
      "iteration: 6/9   loss = 0.6475125551223755   train_acc = 0.6567164179104478   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.6725552082061768   train_acc = 0.664179104477612   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.6112494468688965   train_acc = 0.6791044776119403   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.6623082160949707   train_acc = 0.6343283582089553   val_acc = 0.7142857142857143\n",
      "Final val acc for lr=0.0002368042204919325, wd=0.00011822143451838108: 0.7143\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.0002368042204919325, wd=0.00011822143451838108, betas=(0.9, 0.98)\n",
      "iteration: 1/9   loss = 0.7150645852088928   train_acc = 0.5746268656716418   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.6422007083892822   train_acc = 0.6044776119402985   val_acc = 0.5714285714285714\n",
      "iteration: 3/9   loss = 0.7593719363212585   train_acc = 0.6791044776119403   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.6644809246063232   train_acc = 0.6119402985074627   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.6416323184967041   train_acc = 0.6119402985074627   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.7494966983795166   train_acc = 0.6492537313432836   val_acc = 0.2857142857142857\n",
      "iteration: 7/9   loss = 0.6467635035514832   train_acc = 0.7089552238805971   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.5975606441497803   train_acc = 0.7089552238805971   val_acc = 0.42857142857142855\n",
      "iteration: 9/9   loss = 0.7921410799026489   train_acc = 0.7238805970149254   val_acc = 0.5\n",
      "Final val acc for lr=0.0002368042204919325, wd=0.00011822143451838108: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.0002368042204919325, wd=0.00011822143451838108, betas=(0.9, 0.99)\n",
      "iteration: 1/9   loss = 0.7533613443374634   train_acc = 0.5597014925373134   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.6592813730239868   train_acc = 0.5895522388059702   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.5883297920227051   train_acc = 0.664179104477612   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.6827816963195801   train_acc = 0.6417910447761194   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.6388872861862183   train_acc = 0.7313432835820896   val_acc = 0.21428571428571427\n",
      "iteration: 6/9   loss = 0.5422893762588501   train_acc = 0.6343283582089553   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.6220054030418396   train_acc = 0.6119402985074627   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.6367616653442383   train_acc = 0.6194029850746269   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.8218657374382019   train_acc = 0.6119402985074627   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=0.0002368042204919325, wd=0.00011822143451838108: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.0002368042204919325, wd=0.00011822143451838108, betas=(0.9, 0.995)\n",
      "iteration: 1/9   loss = 0.793187141418457   train_acc = 0.5597014925373134   val_acc = 0.21428571428571427\n",
      "iteration: 2/9   loss = 0.635863721370697   train_acc = 0.5597014925373134   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.6196326017379761   train_acc = 0.5597014925373134   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.739541232585907   train_acc = 0.6119402985074627   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.6810182929039001   train_acc = 0.6417910447761194   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.5605930089950562   train_acc = 0.6940298507462687   val_acc = 0.7142857142857143\n",
      "iteration: 7/9   loss = 0.6530066132545471   train_acc = 0.6791044776119403   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.5985037088394165   train_acc = 0.6791044776119403   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.5899430513381958   train_acc = 0.6343283582089553   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=0.0002368042204919325, wd=0.00011822143451838108: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.0002368042204919325, wd=0.00011822143451838108, betas=(0.9, 0.999)\n",
      "iteration: 1/9   loss = 0.7126815319061279   train_acc = 0.582089552238806   val_acc = 0.5714285714285714\n",
      "iteration: 2/9   loss = 0.7011639475822449   train_acc = 0.582089552238806   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.893757700920105   train_acc = 0.6417910447761194   val_acc = 0.7142857142857143\n",
      "iteration: 4/9   loss = 0.6461480855941772   train_acc = 0.6716417910447762   val_acc = 0.7857142857142857\n",
      "iteration: 5/9   loss = 0.679911732673645   train_acc = 0.6567164179104478   val_acc = 0.6428571428571429\n",
      "iteration: 6/9   loss = 0.6061192750930786   train_acc = 0.6044776119402985   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.6471832990646362   train_acc = 0.6044776119402985   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.6036005020141602   train_acc = 0.582089552238806   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.7123734951019287   train_acc = 0.6194029850746269   val_acc = 0.5\n",
      "Final val acc for lr=0.0002368042204919325, wd=0.00011822143451838108: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.0002368042204919325, wd=0.00011822143451838108, betas=(0.95, 0.98)\n",
      "iteration: 1/9   loss = 0.8037614822387695   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7993513345718384   train_acc = 0.5223880597014925   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.7587930560112   train_acc = 0.5895522388059702   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7022582292556763   train_acc = 0.6343283582089553   val_acc = 0.6428571428571429\n",
      "iteration: 5/9   loss = 0.6465831995010376   train_acc = 0.664179104477612   val_acc = 0.2857142857142857\n",
      "iteration: 6/9   loss = 0.5680472254753113   train_acc = 0.6417910447761194   val_acc = 0.35714285714285715\n",
      "iteration: 7/9   loss = 0.6393516063690186   train_acc = 0.6940298507462687   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.5873051881790161   train_acc = 0.6567164179104478   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.5622109174728394   train_acc = 0.6716417910447762   val_acc = 0.5\n",
      "Final val acc for lr=0.0002368042204919325, wd=0.00011822143451838108: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.0002368042204919325, wd=0.00011822143451838108, betas=(0.95, 0.99)\n",
      "iteration: 1/9   loss = 0.7045943737030029   train_acc = 0.5447761194029851   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7400690317153931   train_acc = 0.7164179104477612   val_acc = 0.7142857142857143\n",
      "iteration: 3/9   loss = 0.7286232709884644   train_acc = 0.5671641791044776   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.6706116199493408   train_acc = 0.5597014925373134   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7069792747497559   train_acc = 0.5970149253731343   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.753514289855957   train_acc = 0.5895522388059702   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.6726751327514648   train_acc = 0.6044776119402985   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.6009587645530701   train_acc = 0.6716417910447762   val_acc = 0.42857142857142855\n",
      "iteration: 9/9   loss = 0.6501992344856262   train_acc = 0.6865671641791045   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=0.0002368042204919325, wd=0.00011822143451838108: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.0002368042204919325, wd=0.00011822143451838108, betas=(0.95, 0.995)\n",
      "iteration: 1/9   loss = 0.7598378658294678   train_acc = 0.582089552238806   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.6570724248886108   train_acc = 0.6119402985074627   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.6095384955406189   train_acc = 0.6716417910447762   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.6596889495849609   train_acc = 0.6343283582089553   val_acc = 0.35714285714285715\n",
      "iteration: 5/9   loss = 0.6623681783676147   train_acc = 0.6417910447761194   val_acc = 0.21428571428571427\n",
      "iteration: 6/9   loss = 0.6375399827957153   train_acc = 0.6194029850746269   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.7087825536727905   train_acc = 0.6044776119402985   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.6646602749824524   train_acc = 0.6044776119402985   val_acc = 0.42857142857142855\n",
      "iteration: 9/9   loss = 0.6629785895347595   train_acc = 0.6194029850746269   val_acc = 0.5\n",
      "Final val acc for lr=0.0002368042204919325, wd=0.00011822143451838108: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.0002368042204919325, wd=0.00011822143451838108, betas=(0.95, 0.999)\n",
      "iteration: 1/9   loss = 0.7251094579696655   train_acc = 0.5074626865671642   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.7689400911331177   train_acc = 0.6417910447761194   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.7833291888237   train_acc = 0.582089552238806   val_acc = 0.35714285714285715\n",
      "iteration: 4/9   loss = 0.654376208782196   train_acc = 0.582089552238806   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.6954676508903503   train_acc = 0.6716417910447762   val_acc = 0.21428571428571427\n",
      "iteration: 6/9   loss = 0.80284583568573   train_acc = 0.6417910447761194   val_acc = 0.35714285714285715\n",
      "iteration: 7/9   loss = 0.6175155639648438   train_acc = 0.6343283582089553   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.6547586917877197   train_acc = 0.6044776119402985   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.7292580604553223   train_acc = 0.5970149253731343   val_acc = 0.5\n",
      "Final val acc for lr=0.0002368042204919325, wd=0.00011822143451838108: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.0002368042204919325, wd=0.00032736198184168545, betas=(0.85, 0.99)\n",
      "iteration: 1/9   loss = 0.723800539970398   train_acc = 0.582089552238806   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.6593069434165955   train_acc = 0.7089552238805971   val_acc = 0.35714285714285715\n",
      "iteration: 3/9   loss = 0.5542773008346558   train_acc = 0.6417910447761194   val_acc = 0.35714285714285715\n",
      "iteration: 4/9   loss = 0.6295419931411743   train_acc = 0.6417910447761194   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.653447151184082   train_acc = 0.6865671641791045   val_acc = 0.35714285714285715\n",
      "iteration: 6/9   loss = 0.7045933604240417   train_acc = 0.6940298507462687   val_acc = 0.21428571428571427\n",
      "iteration: 7/9   loss = 0.6577349305152893   train_acc = 0.6268656716417911   val_acc = 0.35714285714285715\n",
      "iteration: 8/9   loss = 0.6719585657119751   train_acc = 0.6268656716417911   val_acc = 0.42857142857142855\n",
      "iteration: 9/9   loss = 0.5667700171470642   train_acc = 0.6268656716417911   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=0.0002368042204919325, wd=0.00032736198184168545: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.0002368042204919325, wd=0.00032736198184168545, betas=(0.85, 0.995)\n",
      "iteration: 1/9   loss = 0.7190927267074585   train_acc = 0.5597014925373134   val_acc = 0.5714285714285714\n",
      "iteration: 2/9   loss = 0.7703040838241577   train_acc = 0.6119402985074627   val_acc = 0.6428571428571429\n",
      "iteration: 3/9   loss = 0.7803494334220886   train_acc = 0.6044776119402985   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.6259522438049316   train_acc = 0.582089552238806   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.6781184077262878   train_acc = 0.5895522388059702   val_acc = 0.5714285714285714\n",
      "iteration: 6/9   loss = 0.7377098798751831   train_acc = 0.6865671641791045   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.6480726003646851   train_acc = 0.6044776119402985   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.6198739409446716   train_acc = 0.582089552238806   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.47079119086265564   train_acc = 0.5671641791044776   val_acc = 0.5\n",
      "Final val acc for lr=0.0002368042204919325, wd=0.00032736198184168545: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.0002368042204919325, wd=0.00032736198184168545, betas=(0.9, 0.98)\n",
      "iteration: 1/9   loss = 0.7160665988922119   train_acc = 0.5671641791044776   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7681436538696289   train_acc = 0.6343283582089553   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.7661828994750977   train_acc = 0.5746268656716418   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7211495637893677   train_acc = 0.5895522388059702   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.6679046154022217   train_acc = 0.6119402985074627   val_acc = 0.5714285714285714\n",
      "iteration: 6/9   loss = 0.5144774913787842   train_acc = 0.6343283582089553   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.6086393594741821   train_acc = 0.6716417910447762   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.6408980488777161   train_acc = 0.6940298507462687   val_acc = 0.6428571428571429\n",
      "iteration: 9/9   loss = 0.7298849821090698   train_acc = 0.7388059701492538   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=0.0002368042204919325, wd=0.00032736198184168545: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.0002368042204919325, wd=0.00032736198184168545, betas=(0.9, 0.99)\n",
      "iteration: 1/9   loss = 0.7119907140731812   train_acc = 0.5447761194029851   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7544364929199219   train_acc = 0.6417910447761194   val_acc = 0.5714285714285714\n",
      "iteration: 3/9   loss = 0.8476071357727051   train_acc = 0.664179104477612   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.6472941637039185   train_acc = 0.6417910447761194   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.6920642852783203   train_acc = 0.6492537313432836   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.6601359248161316   train_acc = 0.6865671641791045   val_acc = 0.6428571428571429\n",
      "iteration: 7/9   loss = 0.6324740648269653   train_acc = 0.6268656716417911   val_acc = 0.7142857142857143\n",
      "iteration: 8/9   loss = 0.6433738470077515   train_acc = 0.6268656716417911   val_acc = 0.6428571428571429\n",
      "iteration: 9/9   loss = 0.7060298919677734   train_acc = 0.6492537313432836   val_acc = 0.7142857142857143\n",
      "Final val acc for lr=0.0002368042204919325, wd=0.00032736198184168545: 0.7143\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.0002368042204919325, wd=0.00032736198184168545, betas=(0.9, 0.995)\n",
      "iteration: 1/9   loss = 0.7328944802284241   train_acc = 0.6343283582089553   val_acc = 0.35714285714285715\n",
      "iteration: 2/9   loss = 0.7250543832778931   train_acc = 0.6343283582089553   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.6011571884155273   train_acc = 0.6716417910447762   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.6731959581375122   train_acc = 0.6791044776119403   val_acc = 0.35714285714285715\n",
      "iteration: 5/9   loss = 0.6047240495681763   train_acc = 0.7014925373134329   val_acc = 0.5714285714285714\n",
      "iteration: 6/9   loss = 0.5236387252807617   train_acc = 0.6791044776119403   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.6497719883918762   train_acc = 0.7611940298507462   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.6482909917831421   train_acc = 0.6716417910447762   val_acc = 0.42857142857142855\n",
      "iteration: 9/9   loss = 0.48674649000167847   train_acc = 0.6791044776119403   val_acc = 0.35714285714285715\n",
      "Final val acc for lr=0.0002368042204919325, wd=0.00032736198184168545: 0.3571\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.0002368042204919325, wd=0.00032736198184168545, betas=(0.9, 0.999)\n",
      "iteration: 1/9   loss = 0.7271256446838379   train_acc = 0.5895522388059702   val_acc = 0.35714285714285715\n",
      "iteration: 2/9   loss = 0.6786103248596191   train_acc = 0.5970149253731343   val_acc = 0.21428571428571427\n",
      "iteration: 3/9   loss = 0.7182895541191101   train_acc = 0.664179104477612   val_acc = 0.35714285714285715\n",
      "iteration: 4/9   loss = 0.658936619758606   train_acc = 0.6716417910447762   val_acc = 0.2857142857142857\n",
      "iteration: 5/9   loss = 0.6389162540435791   train_acc = 0.664179104477612   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.7128468751907349   train_acc = 0.6268656716417911   val_acc = 0.35714285714285715\n",
      "iteration: 7/9   loss = 0.70778489112854   train_acc = 0.7089552238805971   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.6298239827156067   train_acc = 0.6940298507462687   val_acc = 0.42857142857142855\n",
      "iteration: 9/9   loss = 0.6362133026123047   train_acc = 0.6791044776119403   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=0.0002368042204919325, wd=0.00032736198184168545: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.0002368042204919325, wd=0.00032736198184168545, betas=(0.95, 0.98)\n",
      "iteration: 1/9   loss = 0.8389171957969666   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.779994547367096   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.5746197700500488   train_acc = 0.5447761194029851   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7379974722862244   train_acc = 0.5746268656716418   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.689926266670227   train_acc = 0.6343283582089553   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.5830926299095154   train_acc = 0.7089552238805971   val_acc = 0.6428571428571429\n",
      "iteration: 7/9   loss = 0.6283488273620605   train_acc = 0.6716417910447762   val_acc = 0.6428571428571429\n",
      "iteration: 8/9   loss = 0.6565560698509216   train_acc = 0.6940298507462687   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.5021116137504578   train_acc = 0.6716417910447762   val_acc = 0.5714285714285714\n",
      "Final val acc for lr=0.0002368042204919325, wd=0.00032736198184168545: 0.5714\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.0002368042204919325, wd=0.00032736198184168545, betas=(0.95, 0.99)\n",
      "iteration: 1/9   loss = 0.7258802056312561   train_acc = 0.48507462686567165   val_acc = 0.5714285714285714\n",
      "iteration: 2/9   loss = 0.776558518409729   train_acc = 0.5970149253731343   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.7645842432975769   train_acc = 0.5895522388059702   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.6724755167961121   train_acc = 0.6268656716417911   val_acc = 0.6428571428571429\n",
      "iteration: 5/9   loss = 0.6502593159675598   train_acc = 0.6940298507462687   val_acc = 0.35714285714285715\n",
      "iteration: 6/9   loss = 0.6236730217933655   train_acc = 0.6343283582089553   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.6376673579216003   train_acc = 0.6194029850746269   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.6969209909439087   train_acc = 0.6119402985074627   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.5869001150131226   train_acc = 0.6417910447761194   val_acc = 0.5714285714285714\n",
      "Final val acc for lr=0.0002368042204919325, wd=0.00032736198184168545: 0.5714\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.0002368042204919325, wd=0.00032736198184168545, betas=(0.95, 0.995)\n",
      "iteration: 1/9   loss = 0.7070907354354858   train_acc = 0.6865671641791045   val_acc = 0.5714285714285714\n",
      "iteration: 2/9   loss = 0.6971156597137451   train_acc = 0.6492537313432836   val_acc = 0.6428571428571429\n",
      "iteration: 3/9   loss = 0.6769236922264099   train_acc = 0.6044776119402985   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.6730448007583618   train_acc = 0.6567164179104478   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.6042081117630005   train_acc = 0.6567164179104478   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.7443650960922241   train_acc = 0.6865671641791045   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.5621458292007446   train_acc = 0.6865671641791045   val_acc = 0.6428571428571429\n",
      "iteration: 8/9   loss = 0.6179957389831543   train_acc = 0.664179104477612   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.5868128538131714   train_acc = 0.6343283582089553   val_acc = 0.35714285714285715\n",
      "Final val acc for lr=0.0002368042204919325, wd=0.00032736198184168545: 0.3571\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.0002368042204919325, wd=0.00032736198184168545, betas=(0.95, 0.999)\n",
      "iteration: 1/9   loss = 0.7371527552604675   train_acc = 0.6268656716417911   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.6668174266815186   train_acc = 0.664179104477612   val_acc = 0.35714285714285715\n",
      "iteration: 3/9   loss = 0.7916061282157898   train_acc = 0.6492537313432836   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.6106457710266113   train_acc = 0.6940298507462687   val_acc = 0.2857142857142857\n",
      "iteration: 5/9   loss = 0.7177983522415161   train_acc = 0.6865671641791045   val_acc = 0.2857142857142857\n",
      "iteration: 6/9   loss = 0.5096456408500671   train_acc = 0.7089552238805971   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.649458110332489   train_acc = 0.6865671641791045   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.6067412495613098   train_acc = 0.7238805970149254   val_acc = 0.35714285714285715\n",
      "iteration: 9/9   loss = 0.7921345233917236   train_acc = 0.6492537313432836   val_acc = 0.6428571428571429\n",
      "Final val acc for lr=0.0002368042204919325, wd=0.00032736198184168545: 0.6429\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=3.382370800515571e-06, wd=0.0004609389889268166, betas=(0.85, 0.99)\n",
      "iteration: 1/9   loss = 0.7095522880554199   train_acc = 0.47761194029850745   val_acc = 0.5714285714285714\n",
      "iteration: 2/9   loss = 0.7095859050750732   train_acc = 0.48507462686567165   val_acc = 0.5714285714285714\n",
      "iteration: 3/9   loss = 0.806564211845398   train_acc = 0.47761194029850745   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.6647951602935791   train_acc = 0.47761194029850745   val_acc = 0.5714285714285714\n",
      "iteration: 5/9   loss = 0.6873281598091125   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.8145265579223633   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7167856693267822   train_acc = 0.48507462686567165   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.7248021364212036   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.6845992803573608   train_acc = 0.4701492537313433   val_acc = 0.5714285714285714\n",
      "Final val acc for lr=3.382370800515571e-06, wd=0.0004609389889268166: 0.5714\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=3.382370800515571e-06, wd=0.0004609389889268166, betas=(0.85, 0.995)\n",
      "iteration: 1/9   loss = 0.652124285697937   train_acc = 0.5746268656716418   val_acc = 0.35714285714285715\n",
      "iteration: 2/9   loss = 0.7186622023582458   train_acc = 0.5895522388059702   val_acc = 0.5714285714285714\n",
      "iteration: 3/9   loss = 0.6121729016304016   train_acc = 0.6119402985074627   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.704616904258728   train_acc = 0.5671641791044776   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.6913458108901978   train_acc = 0.5970149253731343   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.6639493107795715   train_acc = 0.5671641791044776   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7102863192558289   train_acc = 0.582089552238806   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7136991024017334   train_acc = 0.5746268656716418   val_acc = 0.42857142857142855\n",
      "iteration: 9/9   loss = 0.6617429256439209   train_acc = 0.5522388059701493   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=3.382370800515571e-06, wd=0.0004609389889268166: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=3.382370800515571e-06, wd=0.0004609389889268166, betas=(0.9, 0.98)\n",
      "iteration: 1/9   loss = 0.8550294637680054   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7694258093833923   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.8095847964286804   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7836405038833618   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.8367795944213867   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.511520504951477   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7672881484031677   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7979167699813843   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.8229368925094604   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "Final val acc for lr=3.382370800515571e-06, wd=0.0004609389889268166: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=3.382370800515571e-06, wd=0.0004609389889268166, betas=(0.9, 0.99)\n",
      "iteration: 1/9   loss = 0.7689284682273865   train_acc = 0.4925373134328358   val_acc = 0.35714285714285715\n",
      "iteration: 2/9   loss = 0.7352966070175171   train_acc = 0.5223880597014925   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.6567900776863098   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7345665693283081   train_acc = 0.5   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.705123782157898   train_acc = 0.5373134328358209   val_acc = 0.2857142857142857\n",
      "iteration: 6/9   loss = 0.943024754524231   train_acc = 0.5373134328358209   val_acc = 0.2857142857142857\n",
      "iteration: 7/9   loss = 0.732903003692627   train_acc = 0.5522388059701493   val_acc = 0.2857142857142857\n",
      "iteration: 8/9   loss = 0.754885733127594   train_acc = 0.5522388059701493   val_acc = 0.35714285714285715\n",
      "iteration: 9/9   loss = 0.7682889699935913   train_acc = 0.5447761194029851   val_acc = 0.5\n",
      "Final val acc for lr=3.382370800515571e-06, wd=0.0004609389889268166: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=3.382370800515571e-06, wd=0.0004609389889268166, betas=(0.9, 0.995)\n",
      "iteration: 1/9   loss = 0.7960478067398071   train_acc = 0.40298507462686567   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7120497226715088   train_acc = 0.4552238805970149   val_acc = 0.5714285714285714\n",
      "iteration: 3/9   loss = 0.6643259525299072   train_acc = 0.44776119402985076   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7565975189208984   train_acc = 0.44029850746268656   val_acc = 0.5714285714285714\n",
      "iteration: 5/9   loss = 0.7598668336868286   train_acc = 0.44776119402985076   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.683164119720459   train_acc = 0.47761194029850745   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7199508547782898   train_acc = 0.417910447761194   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.7182073593139648   train_acc = 0.4253731343283582   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.8558981418609619   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "Final val acc for lr=3.382370800515571e-06, wd=0.0004609389889268166: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=3.382370800515571e-06, wd=0.0004609389889268166, betas=(0.9, 0.999)\n",
      "iteration: 1/9   loss = 0.7560224533081055   train_acc = 0.47761194029850745   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7938801050186157   train_acc = 0.47761194029850745   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.9415873885154724   train_acc = 0.4626865671641791   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.8569626808166504   train_acc = 0.47761194029850745   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7052015662193298   train_acc = 0.44776119402985076   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.8418204188346863   train_acc = 0.47761194029850745   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7179458141326904   train_acc = 0.4552238805970149   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.729410707950592   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.6801222562789917   train_acc = 0.4626865671641791   val_acc = 0.5\n",
      "Final val acc for lr=3.382370800515571e-06, wd=0.0004609389889268166: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=3.382370800515571e-06, wd=0.0004609389889268166, betas=(0.95, 0.98)\n",
      "iteration: 1/9   loss = 0.7109005451202393   train_acc = 0.48507462686567165   val_acc = 0.5714285714285714\n",
      "iteration: 2/9   loss = 0.7405725717544556   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.8759907484054565   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.6808430552482605   train_acc = 0.4626865671641791   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7291812896728516   train_acc = 0.4925373134328358   val_acc = 0.6428571428571429\n",
      "iteration: 6/9   loss = 0.7299898862838745   train_acc = 0.4701492537313433   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.715599000453949   train_acc = 0.4701492537313433   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.7319736480712891   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.5562531352043152   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "Final val acc for lr=3.382370800515571e-06, wd=0.0004609389889268166: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=3.382370800515571e-06, wd=0.0004609389889268166, betas=(0.95, 0.99)\n",
      "iteration: 1/9   loss = 0.708667516708374   train_acc = 0.5522388059701493   val_acc = 0.6428571428571429\n",
      "iteration: 2/9   loss = 0.6604657769203186   train_acc = 0.47761194029850745   val_acc = 0.5714285714285714\n",
      "iteration: 3/9   loss = 0.7762885093688965   train_acc = 0.4701492537313433   val_acc = 0.7142857142857143\n",
      "iteration: 4/9   loss = 0.6770633459091187   train_acc = 0.5522388059701493   val_acc = 0.5714285714285714\n",
      "iteration: 5/9   loss = 0.7389805316925049   train_acc = 0.5671641791044776   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.7086169123649597   train_acc = 0.5671641791044776   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.74007648229599   train_acc = 0.5223880597014925   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.7326245307922363   train_acc = 0.5298507462686567   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.6532473564147949   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "Final val acc for lr=3.382370800515571e-06, wd=0.0004609389889268166: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=3.382370800515571e-06, wd=0.0004609389889268166, betas=(0.95, 0.995)\n",
      "iteration: 1/9   loss = 0.7550052404403687   train_acc = 0.44776119402985076   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.6694474220275879   train_acc = 0.47761194029850745   val_acc = 0.35714285714285715\n",
      "iteration: 3/9   loss = 0.8169970512390137   train_acc = 0.4552238805970149   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.6901620030403137   train_acc = 0.39552238805970147   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.7403805255889893   train_acc = 0.4925373134328358   val_acc = 0.35714285714285715\n",
      "iteration: 6/9   loss = 0.8150842189788818   train_acc = 0.48507462686567165   val_acc = 0.21428571428571427\n",
      "iteration: 7/9   loss = 0.6742936372756958   train_acc = 0.5074626865671642   val_acc = 0.2857142857142857\n",
      "iteration: 8/9   loss = 0.7225501537322998   train_acc = 0.48507462686567165   val_acc = 0.35714285714285715\n",
      "iteration: 9/9   loss = 0.72982257604599   train_acc = 0.44029850746268656   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=3.382370800515571e-06, wd=0.0004609389889268166: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=3.382370800515571e-06, wd=0.0004609389889268166, betas=(0.95, 0.999)\n",
      "iteration: 1/9   loss = 0.7864868640899658   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.8080767393112183   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.9734742641448975   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7932679653167725   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7806336879730225   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 6/9   loss = 1.0346901416778564   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7989284992218018   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7455747127532959   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 9/9   loss = 1.1252951622009277   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "Final val acc for lr=3.382370800515571e-06, wd=0.0004609389889268166: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=3.382370800515571e-06, wd=0.0002300292096821906, betas=(0.85, 0.99)\n",
      "iteration: 1/9   loss = 0.7032777070999146   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.6931973695755005   train_acc = 0.4552238805970149   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.798504114151001   train_acc = 0.5   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.7421639561653137   train_acc = 0.48507462686567165   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.7243887186050415   train_acc = 0.43283582089552236   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.8725795149803162   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7808928489685059   train_acc = 0.44776119402985076   val_acc = 0.35714285714285715\n",
      "iteration: 8/9   loss = 0.7296496629714966   train_acc = 0.4701492537313433   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.6744848489761353   train_acc = 0.4925373134328358   val_acc = 0.5714285714285714\n",
      "Final val acc for lr=3.382370800515571e-06, wd=0.0002300292096821906: 0.5714\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=3.382370800515571e-06, wd=0.0002300292096821906, betas=(0.85, 0.995)\n",
      "iteration: 1/9   loss = 0.7277923822402954   train_acc = 0.5149253731343284   val_acc = 0.5714285714285714\n",
      "iteration: 2/9   loss = 0.771762490272522   train_acc = 0.4925373134328358   val_acc = 0.5714285714285714\n",
      "iteration: 3/9   loss = 0.6978628039360046   train_acc = 0.5223880597014925   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.6978155970573425   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7434259653091431   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.6628279685974121   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7226593494415283   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7000186443328857   train_acc = 0.5373134328358209   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.6973640322685242   train_acc = 0.5074626865671642   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=3.382370800515571e-06, wd=0.0002300292096821906: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=3.382370800515571e-06, wd=0.0002300292096821906, betas=(0.9, 0.98)\n",
      "iteration: 1/9   loss = 0.728547215461731   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7336904406547546   train_acc = 0.5298507462686567   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.6737474799156189   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7396438121795654   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.6771314144134521   train_acc = 0.5149253731343284   val_acc = 0.5714285714285714\n",
      "iteration: 6/9   loss = 0.6145493388175964   train_acc = 0.5373134328358209   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7109432816505432   train_acc = 0.5298507462686567   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.6626948118209839   train_acc = 0.5522388059701493   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.7072805166244507   train_acc = 0.5522388059701493   val_acc = 0.5\n",
      "Final val acc for lr=3.382370800515571e-06, wd=0.0002300292096821906: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=3.382370800515571e-06, wd=0.0002300292096821906, betas=(0.9, 0.99)\n",
      "iteration: 1/9   loss = 0.731431245803833   train_acc = 0.4925373134328358   val_acc = 0.5714285714285714\n",
      "iteration: 2/9   loss = 0.7423156499862671   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.8704110980033875   train_acc = 0.5   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.758992075920105   train_acc = 0.5   val_acc = 0.5714285714285714\n",
      "iteration: 5/9   loss = 0.6944646835327148   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.7161716222763062   train_acc = 0.4925373134328358   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.7679916024208069   train_acc = 0.47761194029850745   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7138321995735168   train_acc = 0.47761194029850745   val_acc = 0.42857142857142855\n",
      "iteration: 9/9   loss = 0.7960357069969177   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "Final val acc for lr=3.382370800515571e-06, wd=0.0002300292096821906: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=3.382370800515571e-06, wd=0.0002300292096821906, betas=(0.9, 0.995)\n",
      "iteration: 1/9   loss = 0.7070894241333008   train_acc = 0.41044776119402987   val_acc = 0.35714285714285715\n",
      "iteration: 2/9   loss = 0.7529810667037964   train_acc = 0.44776119402985076   val_acc = 0.6428571428571429\n",
      "iteration: 3/9   loss = 0.8254685401916504   train_acc = 0.40298507462686567   val_acc = 0.6428571428571429\n",
      "iteration: 4/9   loss = 0.7806861400604248   train_acc = 0.417910447761194   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7754349708557129   train_acc = 0.4552238805970149   val_acc = 0.6428571428571429\n",
      "iteration: 6/9   loss = 0.7439000010490417   train_acc = 0.41044776119402987   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.797258734703064   train_acc = 0.44029850746268656   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7674674391746521   train_acc = 0.4253731343283582   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.9741406440734863   train_acc = 0.47761194029850745   val_acc = 0.5714285714285714\n",
      "Final val acc for lr=3.382370800515571e-06, wd=0.0002300292096821906: 0.5714\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=3.382370800515571e-06, wd=0.0002300292096821906, betas=(0.9, 0.999)\n",
      "iteration: 1/9   loss = 0.7690162658691406   train_acc = 0.5   val_acc = 0.35714285714285715\n",
      "iteration: 2/9   loss = 0.6940141916275024   train_acc = 0.4925373134328358   val_acc = 0.35714285714285715\n",
      "iteration: 3/9   loss = 0.6286577582359314   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7219913005828857   train_acc = 0.4925373134328358   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.7351194024085999   train_acc = 0.47761194029850745   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.7600523829460144   train_acc = 0.47761194029850745   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.7558162808418274   train_acc = 0.4925373134328358   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.7001336216926575   train_acc = 0.4701492537313433   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.5977362394332886   train_acc = 0.47761194029850745   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=3.382370800515571e-06, wd=0.0002300292096821906: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=3.382370800515571e-06, wd=0.0002300292096821906, betas=(0.95, 0.98)\n",
      "iteration: 1/9   loss = 0.7890126705169678   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7132061123847961   train_acc = 0.5223880597014925   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.6447192430496216   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7528793215751648   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7324483394622803   train_acc = 0.5223880597014925   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.9105198383331299   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.75041663646698   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.785876989364624   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.5793046951293945   train_acc = 0.5   val_acc = 0.5\n",
      "Final val acc for lr=3.382370800515571e-06, wd=0.0002300292096821906: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=3.382370800515571e-06, wd=0.0002300292096821906, betas=(0.95, 0.99)\n",
      "iteration: 1/9   loss = 0.7307863235473633   train_acc = 0.5223880597014925   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.6804766058921814   train_acc = 0.5298507462686567   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.7607958316802979   train_acc = 0.582089552238806   val_acc = 0.35714285714285715\n",
      "iteration: 4/9   loss = 0.6929073333740234   train_acc = 0.5895522388059702   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7586725950241089   train_acc = 0.5447761194029851   val_acc = 0.2857142857142857\n",
      "iteration: 6/9   loss = 0.7151986360549927   train_acc = 0.5597014925373134   val_acc = 0.35714285714285715\n",
      "iteration: 7/9   loss = 0.6969524025917053   train_acc = 0.582089552238806   val_acc = 0.35714285714285715\n",
      "iteration: 8/9   loss = 0.7268821001052856   train_acc = 0.5447761194029851   val_acc = 0.42857142857142855\n",
      "iteration: 9/9   loss = 0.681627631187439   train_acc = 0.6044776119402985   val_acc = 0.5\n",
      "Final val acc for lr=3.382370800515571e-06, wd=0.0002300292096821906: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=3.382370800515571e-06, wd=0.0002300292096821906, betas=(0.95, 0.995)\n",
      "iteration: 1/9   loss = 0.7049591541290283   train_acc = 0.47761194029850745   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.7293380498886108   train_acc = 0.48507462686567165   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.5661019086837769   train_acc = 0.44029850746268656   val_acc = 0.2857142857142857\n",
      "iteration: 4/9   loss = 0.7228516340255737   train_acc = 0.43283582089552236   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.7400981187820435   train_acc = 0.43283582089552236   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.6196269989013672   train_acc = 0.4552238805970149   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7548633813858032   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7097533345222473   train_acc = 0.44029850746268656   val_acc = 0.35714285714285715\n",
      "iteration: 9/9   loss = 0.7946550250053406   train_acc = 0.4701492537313433   val_acc = 0.5\n",
      "Final val acc for lr=3.382370800515571e-06, wd=0.0002300292096821906: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=3.382370800515571e-06, wd=0.0002300292096821906, betas=(0.95, 0.999)\n",
      "iteration: 1/9   loss = 0.7966806888580322   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7343687415122986   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.653084397315979   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7381128072738647   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7205642461776733   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.7972844839096069   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.6790995597839355   train_acc = 0.5223880597014925   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7634509801864624   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.7675763368606567   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "Final val acc for lr=3.382370800515571e-06, wd=0.0002300292096821906: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=3.382370800515571e-06, wd=0.00022555743977178082, betas=(0.85, 0.99)\n",
      "iteration: 1/9   loss = 0.6999726295471191   train_acc = 0.4925373134328358   val_acc = 0.6428571428571429\n",
      "iteration: 2/9   loss = 0.7267928123474121   train_acc = 0.4925373134328358   val_acc = 0.5714285714285714\n",
      "iteration: 3/9   loss = 0.640017032623291   train_acc = 0.5298507462686567   val_acc = 0.7142857142857143\n",
      "iteration: 4/9   loss = 0.7029277086257935   train_acc = 0.5074626865671642   val_acc = 0.6428571428571429\n",
      "iteration: 5/9   loss = 0.7462167739868164   train_acc = 0.44029850746268656   val_acc = 0.7142857142857143\n",
      "iteration: 6/9   loss = 0.6996766924858093   train_acc = 0.5149253731343284   val_acc = 0.6428571428571429\n",
      "iteration: 7/9   loss = 0.6963455677032471   train_acc = 0.47761194029850745   val_acc = 0.6428571428571429\n",
      "iteration: 8/9   loss = 0.6681331396102905   train_acc = 0.48507462686567165   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.6400126814842224   train_acc = 0.5   val_acc = 0.6428571428571429\n",
      "Final val acc for lr=3.382370800515571e-06, wd=0.00022555743977178082: 0.6429\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=3.382370800515571e-06, wd=0.00022555743977178082, betas=(0.85, 0.995)\n",
      "iteration: 1/9   loss = 0.7076947689056396   train_acc = 0.5522388059701493   val_acc = 0.6428571428571429\n",
      "iteration: 2/9   loss = 0.7129532098770142   train_acc = 0.5074626865671642   val_acc = 0.6428571428571429\n",
      "iteration: 3/9   loss = 0.6765143275260925   train_acc = 0.5298507462686567   val_acc = 0.7142857142857143\n",
      "iteration: 4/9   loss = 0.7161568999290466   train_acc = 0.5373134328358209   val_acc = 0.5714285714285714\n",
      "iteration: 5/9   loss = 0.7512791156768799   train_acc = 0.5373134328358209   val_acc = 0.5714285714285714\n",
      "iteration: 6/9   loss = 0.7856911420822144   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7203080654144287   train_acc = 0.48507462686567165   val_acc = 0.6428571428571429\n",
      "iteration: 8/9   loss = 0.6836112141609192   train_acc = 0.5149253731343284   val_acc = 0.7142857142857143\n",
      "iteration: 9/9   loss = 0.6880269050598145   train_acc = 0.5298507462686567   val_acc = 0.6428571428571429\n",
      "Final val acc for lr=3.382370800515571e-06, wd=0.00022555743977178082: 0.6429\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=3.382370800515571e-06, wd=0.00022555743977178082, betas=(0.9, 0.98)\n",
      "iteration: 1/9   loss = 0.7352721691131592   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7175381183624268   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.6564850211143494   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7128204107284546   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7594797015190125   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.6256810426712036   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7535086274147034   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7280535697937012   train_acc = 0.5074626865671642   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.9390665888786316   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "Final val acc for lr=3.382370800515571e-06, wd=0.00022555743977178082: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=3.382370800515571e-06, wd=0.00022555743977178082, betas=(0.9, 0.99)\n",
      "iteration: 1/9   loss = 0.6883237957954407   train_acc = 0.5447761194029851   val_acc = 0.5714285714285714\n",
      "iteration: 2/9   loss = 0.727049708366394   train_acc = 0.5223880597014925   val_acc = 0.5714285714285714\n",
      "iteration: 3/9   loss = 0.7970613241195679   train_acc = 0.5447761194029851   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.73678058385849   train_acc = 0.5149253731343284   val_acc = 0.7857142857142857\n",
      "iteration: 5/9   loss = 0.7089176177978516   train_acc = 0.5223880597014925   val_acc = 0.5714285714285714\n",
      "iteration: 6/9   loss = 0.6073857545852661   train_acc = 0.5597014925373134   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.7124708294868469   train_acc = 0.5671641791044776   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.7633046507835388   train_acc = 0.582089552238806   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.6929054260253906   train_acc = 0.5597014925373134   val_acc = 0.5714285714285714\n",
      "Final val acc for lr=3.382370800515571e-06, wd=0.00022555743977178082: 0.5714\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=3.382370800515571e-06, wd=0.00022555743977178082, betas=(0.9, 0.995)\n",
      "iteration: 1/9   loss = 0.8014284372329712   train_acc = 0.5223880597014925   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.6927348375320435   train_acc = 0.5522388059701493   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.6255728602409363   train_acc = 0.5373134328358209   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7314608693122864   train_acc = 0.5522388059701493   val_acc = 0.5714285714285714\n",
      "iteration: 5/9   loss = 0.740864634513855   train_acc = 0.5597014925373134   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.7209014892578125   train_acc = 0.5597014925373134   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.7326829433441162   train_acc = 0.5298507462686567   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.7272230386734009   train_acc = 0.5447761194029851   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.6014906764030457   train_acc = 0.5447761194029851   val_acc = 0.5\n",
      "Final val acc for lr=3.382370800515571e-06, wd=0.00022555743977178082: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=3.382370800515571e-06, wd=0.00022555743977178082, betas=(0.9, 0.999)\n",
      "iteration: 1/9   loss = 0.7164306640625   train_acc = 0.4701492537313433   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.742801308631897   train_acc = 0.5074626865671642   val_acc = 0.6428571428571429\n",
      "iteration: 3/9   loss = 0.8206068873405457   train_acc = 0.44776119402985076   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.7162060737609863   train_acc = 0.47761194029850745   val_acc = 0.5714285714285714\n",
      "iteration: 5/9   loss = 0.7248916029930115   train_acc = 0.5   val_acc = 0.5714285714285714\n",
      "iteration: 6/9   loss = 0.671411395072937   train_acc = 0.44776119402985076   val_acc = 0.6428571428571429\n",
      "iteration: 7/9   loss = 0.7465291023254395   train_acc = 0.5298507462686567   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.6974155902862549   train_acc = 0.44776119402985076   val_acc = 0.6428571428571429\n",
      "iteration: 9/9   loss = 0.7163903117179871   train_acc = 0.5074626865671642   val_acc = 0.6428571428571429\n",
      "Final val acc for lr=3.382370800515571e-06, wd=0.00022555743977178082: 0.6429\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=3.382370800515571e-06, wd=0.00022555743977178082, betas=(0.95, 0.98)\n",
      "iteration: 1/9   loss = 0.7484134435653687   train_acc = 0.5223880597014925   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7283944487571716   train_acc = 0.5223880597014925   val_acc = 0.35714285714285715\n",
      "iteration: 3/9   loss = 0.7522974014282227   train_acc = 0.5223880597014925   val_acc = 0.35714285714285715\n",
      "iteration: 4/9   loss = 0.7525060772895813   train_acc = 0.5149253731343284   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.711618185043335   train_acc = 0.5149253731343284   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.7200809717178345   train_acc = 0.5074626865671642   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.7197826504707336   train_acc = 0.5373134328358209   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.7434316277503967   train_acc = 0.5447761194029851   val_acc = 0.42857142857142855\n",
      "iteration: 9/9   loss = 0.7440826892852783   train_acc = 0.5149253731343284   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=3.382370800515571e-06, wd=0.00022555743977178082: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=3.382370800515571e-06, wd=0.00022555743977178082, betas=(0.95, 0.99)\n",
      "iteration: 1/9   loss = 0.742006778717041   train_acc = 0.4701492537313433   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.735793948173523   train_acc = 0.4925373134328358   val_acc = 0.2857142857142857\n",
      "iteration: 3/9   loss = 0.7141761779785156   train_acc = 0.4925373134328358   val_acc = 0.35714285714285715\n",
      "iteration: 4/9   loss = 0.761661171913147   train_acc = 0.4626865671641791   val_acc = 0.35714285714285715\n",
      "iteration: 5/9   loss = 0.7218984365463257   train_acc = 0.47761194029850745   val_acc = 0.35714285714285715\n",
      "iteration: 6/9   loss = 0.6554799675941467   train_acc = 0.47761194029850745   val_acc = 0.35714285714285715\n",
      "iteration: 7/9   loss = 0.706320583820343   train_acc = 0.48507462686567165   val_acc = 0.35714285714285715\n",
      "iteration: 8/9   loss = 0.7523009181022644   train_acc = 0.4701492537313433   val_acc = 0.35714285714285715\n",
      "iteration: 9/9   loss = 0.7227179408073425   train_acc = 0.44776119402985076   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=3.382370800515571e-06, wd=0.00022555743977178082: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=3.382370800515571e-06, wd=0.00022555743977178082, betas=(0.95, 0.995)\n",
      "iteration: 1/9   loss = 0.8015624284744263   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7094022035598755   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.6337599754333496   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.6855578422546387   train_acc = 0.4925373134328358   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.7818878293037415   train_acc = 0.5   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.8060795664787292   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7041555643081665   train_acc = 0.5   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.7153611183166504   train_acc = 0.5074626865671642   val_acc = 0.42857142857142855\n",
      "iteration: 9/9   loss = 0.7821944952011108   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "Final val acc for lr=3.382370800515571e-06, wd=0.00022555743977178082: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=3.382370800515571e-06, wd=0.00022555743977178082, betas=(0.95, 0.999)\n",
      "iteration: 1/9   loss = 0.7577186822891235   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7407320737838745   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.9348520636558533   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7887179255485535   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.753017246723175   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.7185070514678955   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.715551495552063   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7403010129928589   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.6383631825447083   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "Final val acc for lr=3.382370800515571e-06, wd=0.00022555743977178082: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=3.382370800515571e-06, wd=0.00011822143451838108, betas=(0.85, 0.99)\n",
      "iteration: 1/9   loss = 0.8040262460708618   train_acc = 0.48507462686567165   val_acc = 0.5714285714285714\n",
      "iteration: 2/9   loss = 0.7083529233932495   train_acc = 0.4701492537313433   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.6617475748062134   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7251721620559692   train_acc = 0.4626865671641791   val_acc = 0.5714285714285714\n",
      "iteration: 5/9   loss = 0.7623098492622375   train_acc = 0.4626865671641791   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.7068488001823425   train_acc = 0.47761194029850745   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7536044120788574   train_acc = 0.47761194029850745   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7962120771408081   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.8221925497055054   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "Final val acc for lr=3.382370800515571e-06, wd=0.00011822143451838108: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=3.382370800515571e-06, wd=0.00011822143451838108, betas=(0.85, 0.995)\n",
      "iteration: 1/9   loss = 0.7064593434333801   train_acc = 0.5970149253731343   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7296319007873535   train_acc = 0.6119402985074627   val_acc = 0.5714285714285714\n",
      "iteration: 3/9   loss = 0.6456993818283081   train_acc = 0.5895522388059702   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.7239959836006165   train_acc = 0.5895522388059702   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7167145013809204   train_acc = 0.5970149253731343   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.7974061965942383   train_acc = 0.6119402985074627   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.6871915459632874   train_acc = 0.5895522388059702   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7466979026794434   train_acc = 0.5373134328358209   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.7628806829452515   train_acc = 0.6044776119402985   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=3.382370800515571e-06, wd=0.00011822143451838108: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=3.382370800515571e-06, wd=0.00011822143451838108, betas=(0.9, 0.98)\n",
      "iteration: 1/9   loss = 0.8376425504684448   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7686010599136353   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.7392241358757019   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7670879364013672   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7854801416397095   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.692967414855957   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7934236526489258   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7876354455947876   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.5254393815994263   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "Final val acc for lr=3.382370800515571e-06, wd=0.00011822143451838108: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=3.382370800515571e-06, wd=0.00011822143451838108, betas=(0.9, 0.99)\n",
      "iteration: 1/9   loss = 0.7336784601211548   train_acc = 0.5597014925373134   val_acc = 0.6428571428571429\n",
      "iteration: 2/9   loss = 0.7329232692718506   train_acc = 0.6119402985074627   val_acc = 0.5714285714285714\n",
      "iteration: 3/9   loss = 0.6586300134658813   train_acc = 0.5597014925373134   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.7357758283615112   train_acc = 0.5597014925373134   val_acc = 0.5714285714285714\n",
      "iteration: 5/9   loss = 0.7658610343933105   train_acc = 0.582089552238806   val_acc = 0.7142857142857143\n",
      "iteration: 6/9   loss = 0.6689054369926453   train_acc = 0.6268656716417911   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.6844128370285034   train_acc = 0.6044776119402985   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.693291187286377   train_acc = 0.5597014925373134   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.7768086194992065   train_acc = 0.582089552238806   val_acc = 0.7142857142857143\n",
      "Final val acc for lr=3.382370800515571e-06, wd=0.00011822143451838108: 0.7143\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=3.382370800515571e-06, wd=0.00011822143451838108, betas=(0.9, 0.995)\n",
      "iteration: 1/9   loss = 0.7270773649215698   train_acc = 0.4925373134328358   val_acc = 0.35714285714285715\n",
      "iteration: 2/9   loss = 0.7012670636177063   train_acc = 0.5149253731343284   val_acc = 0.35714285714285715\n",
      "iteration: 3/9   loss = 0.8081701993942261   train_acc = 0.5671641791044776   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.7443423271179199   train_acc = 0.5   val_acc = 0.21428571428571427\n",
      "iteration: 5/9   loss = 0.6896870136260986   train_acc = 0.5223880597014925   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.65474933385849   train_acc = 0.5149253731343284   val_acc = 0.2857142857142857\n",
      "iteration: 7/9   loss = 0.6996618509292603   train_acc = 0.5522388059701493   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.7381259799003601   train_acc = 0.5373134328358209   val_acc = 0.35714285714285715\n",
      "iteration: 9/9   loss = 0.6511551141738892   train_acc = 0.5746268656716418   val_acc = 0.2857142857142857\n",
      "Final val acc for lr=3.382370800515571e-06, wd=0.00011822143451838108: 0.2857\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=3.382370800515571e-06, wd=0.00011822143451838108, betas=(0.9, 0.999)\n",
      "iteration: 1/9   loss = 0.7763619422912598   train_acc = 0.5   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.6991744041442871   train_acc = 0.5074626865671642   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.7809756994247437   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7459636330604553   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7792853116989136   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.5846284627914429   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7999054193496704   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7119510769844055   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.6664581894874573   train_acc = 0.5   val_acc = 0.5\n",
      "Final val acc for lr=3.382370800515571e-06, wd=0.00011822143451838108: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=3.382370800515571e-06, wd=0.00011822143451838108, betas=(0.95, 0.98)\n",
      "iteration: 1/9   loss = 0.7771562337875366   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7086639404296875   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.8249112367630005   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7516146302223206   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7785211205482483   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.9660601615905762   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7458889484405518   train_acc = 0.5298507462686567   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.6529439687728882   train_acc = 0.5447761194029851   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.879436194896698   train_acc = 0.5   val_acc = 0.5714285714285714\n",
      "Final val acc for lr=3.382370800515571e-06, wd=0.00011822143451838108: 0.5714\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=3.382370800515571e-06, wd=0.00011822143451838108, betas=(0.95, 0.99)\n",
      "iteration: 1/9   loss = 0.7556198835372925   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.6989989280700684   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.6705201864242554   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.6959635019302368   train_acc = 0.5074626865671642   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.7630037665367126   train_acc = 0.5074626865671642   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.8773360252380371   train_acc = 0.5223880597014925   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.7391373515129089   train_acc = 0.5   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.7190949320793152   train_acc = 0.5074626865671642   val_acc = 0.42857142857142855\n",
      "iteration: 9/9   loss = 0.7156059145927429   train_acc = 0.5074626865671642   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=3.382370800515571e-06, wd=0.00011822143451838108: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=3.382370800515571e-06, wd=0.00011822143451838108, betas=(0.95, 0.995)\n",
      "iteration: 1/9   loss = 0.7326496243476868   train_acc = 0.5   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.7225766777992249   train_acc = 0.5074626865671642   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.6406124830245972   train_acc = 0.4925373134328358   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.6967393755912781   train_acc = 0.5   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.7371718883514404   train_acc = 0.4925373134328358   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.652786910533905   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7494547963142395   train_acc = 0.5074626865671642   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.7001456022262573   train_acc = 0.4925373134328358   val_acc = 0.35714285714285715\n",
      "iteration: 9/9   loss = 0.8090970516204834   train_acc = 0.5   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=3.382370800515571e-06, wd=0.00011822143451838108: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=3.382370800515571e-06, wd=0.00011822143451838108, betas=(0.95, 0.999)\n",
      "iteration: 1/9   loss = 0.6877752542495728   train_acc = 0.5   val_acc = 0.5714285714285714\n",
      "iteration: 2/9   loss = 0.742220401763916   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.8126583099365234   train_acc = 0.4701492537313433   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.646975576877594   train_acc = 0.4552238805970149   val_acc = 0.5714285714285714\n",
      "iteration: 5/9   loss = 0.7532434463500977   train_acc = 0.47761194029850745   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.7498957514762878   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7974410653114319   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7562521696090698   train_acc = 0.5298507462686567   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.6290268301963806   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "Final val acc for lr=3.382370800515571e-06, wd=0.00011822143451838108: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=3.382370800515571e-06, wd=0.00032736198184168545, betas=(0.85, 0.99)\n",
      "iteration: 1/9   loss = 0.6855912804603577   train_acc = 0.47761194029850745   val_acc = 0.7142857142857143\n",
      "iteration: 2/9   loss = 0.7133650779724121   train_acc = 0.5373134328358209   val_acc = 0.6428571428571429\n",
      "iteration: 3/9   loss = 0.6586114168167114   train_acc = 0.5149253731343284   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.7186721563339233   train_acc = 0.48507462686567165   val_acc = 0.6428571428571429\n",
      "iteration: 5/9   loss = 0.7131574153900146   train_acc = 0.5223880597014925   val_acc = 0.6428571428571429\n",
      "iteration: 6/9   loss = 0.6785537600517273   train_acc = 0.5373134328358209   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.7137527465820312   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7413933277130127   train_acc = 0.5149253731343284   val_acc = 0.6428571428571429\n",
      "iteration: 9/9   loss = 0.739855170249939   train_acc = 0.5   val_acc = 0.7142857142857143\n",
      "Final val acc for lr=3.382370800515571e-06, wd=0.00032736198184168545: 0.7143\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=3.382370800515571e-06, wd=0.00032736198184168545, betas=(0.85, 0.995)\n",
      "iteration: 1/9   loss = 0.7153654098510742   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7179534435272217   train_acc = 0.4552238805970149   val_acc = 0.5714285714285714\n",
      "iteration: 3/9   loss = 0.594645619392395   train_acc = 0.4626865671641791   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.6896291971206665   train_acc = 0.44776119402985076   val_acc = 0.5714285714285714\n",
      "iteration: 5/9   loss = 0.6858953833580017   train_acc = 0.48507462686567165   val_acc = 0.5714285714285714\n",
      "iteration: 6/9   loss = 0.7556750774383545   train_acc = 0.5   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.7082188129425049   train_acc = 0.5   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.6980849504470825   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.8342318534851074   train_acc = 0.4925373134328358   val_acc = 0.5714285714285714\n",
      "Final val acc for lr=3.382370800515571e-06, wd=0.00032736198184168545: 0.5714\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=3.382370800515571e-06, wd=0.00032736198184168545, betas=(0.9, 0.98)\n",
      "iteration: 1/9   loss = 0.7614963054656982   train_acc = 0.4552238805970149   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.7550143599510193   train_acc = 0.47761194029850745   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.6404885053634644   train_acc = 0.4626865671641791   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.760202169418335   train_acc = 0.44776119402985076   val_acc = 0.35714285714285715\n",
      "iteration: 5/9   loss = 0.7504549026489258   train_acc = 0.4552238805970149   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.6735579371452332   train_acc = 0.44776119402985076   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.7538849115371704   train_acc = 0.44776119402985076   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.715898334980011   train_acc = 0.48507462686567165   val_acc = 0.35714285714285715\n",
      "iteration: 9/9   loss = 0.6856679320335388   train_acc = 0.4552238805970149   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=3.382370800515571e-06, wd=0.00032736198184168545: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=3.382370800515571e-06, wd=0.00032736198184168545, betas=(0.9, 0.99)\n",
      "iteration: 1/9   loss = 0.6781420707702637   train_acc = 0.5074626865671642   val_acc = 0.35714285714285715\n",
      "iteration: 2/9   loss = 0.6948543787002563   train_acc = 0.5223880597014925   val_acc = 0.2857142857142857\n",
      "iteration: 3/9   loss = 0.6844031810760498   train_acc = 0.5   val_acc = 0.35714285714285715\n",
      "iteration: 4/9   loss = 0.7586826086044312   train_acc = 0.5447761194029851   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.7099825143814087   train_acc = 0.5223880597014925   val_acc = 0.2857142857142857\n",
      "iteration: 6/9   loss = 0.7113602161407471   train_acc = 0.5074626865671642   val_acc = 0.35714285714285715\n",
      "iteration: 7/9   loss = 0.6824900507926941   train_acc = 0.5223880597014925   val_acc = 0.2857142857142857\n",
      "iteration: 8/9   loss = 0.7103132605552673   train_acc = 0.5223880597014925   val_acc = 0.35714285714285715\n",
      "iteration: 9/9   loss = 0.7920635938644409   train_acc = 0.5298507462686567   val_acc = 0.2857142857142857\n",
      "Final val acc for lr=3.382370800515571e-06, wd=0.00032736198184168545: 0.2857\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=3.382370800515571e-06, wd=0.00032736198184168545, betas=(0.9, 0.995)\n",
      "iteration: 1/9   loss = 0.7627466917037964   train_acc = 0.47761194029850745   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7373756766319275   train_acc = 0.4701492537313433   val_acc = 0.5714285714285714\n",
      "iteration: 3/9   loss = 0.6605203747749329   train_acc = 0.4701492537313433   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.7306135892868042   train_acc = 0.3880597014925373   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.7188565731048584   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.8169468641281128   train_acc = 0.4701492537313433   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7391090393066406   train_acc = 0.4701492537313433   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.733503520488739   train_acc = 0.4925373134328358   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.490842342376709   train_acc = 0.4925373134328358   val_acc = 0.6428571428571429\n",
      "Final val acc for lr=3.382370800515571e-06, wd=0.00032736198184168545: 0.6429\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=3.382370800515571e-06, wd=0.00032736198184168545, betas=(0.9, 0.999)\n",
      "iteration: 1/9   loss = 0.7089637517929077   train_acc = 0.5223880597014925   val_acc = 0.2857142857142857\n",
      "iteration: 2/9   loss = 0.7006394863128662   train_acc = 0.5   val_acc = 0.35714285714285715\n",
      "iteration: 3/9   loss = 0.5579805374145508   train_acc = 0.5373134328358209   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.6788222193717957   train_acc = 0.48507462686567165   val_acc = 0.35714285714285715\n",
      "iteration: 5/9   loss = 0.7543780207633972   train_acc = 0.5223880597014925   val_acc = 0.35714285714285715\n",
      "iteration: 6/9   loss = 0.731616199016571   train_acc = 0.5149253731343284   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.7357546091079712   train_acc = 0.5522388059701493   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7561156153678894   train_acc = 0.5373134328358209   val_acc = 0.35714285714285715\n",
      "iteration: 9/9   loss = 0.6168564558029175   train_acc = 0.5298507462686567   val_acc = 0.2857142857142857\n",
      "Final val acc for lr=3.382370800515571e-06, wd=0.00032736198184168545: 0.2857\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=3.382370800515571e-06, wd=0.00032736198184168545, betas=(0.95, 0.98)\n",
      "iteration: 1/9   loss = 0.7474155426025391   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.716011106967926   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.5617539882659912   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7050957679748535   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7638684511184692   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.6628026962280273   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.8298046588897705   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7361066341400146   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.8629250526428223   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "Final val acc for lr=3.382370800515571e-06, wd=0.00032736198184168545: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=3.382370800515571e-06, wd=0.00032736198184168545, betas=(0.95, 0.99)\n",
      "iteration: 1/9   loss = 0.7282916307449341   train_acc = 0.5223880597014925   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7112631797790527   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.6472068428993225   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.6833432912826538   train_acc = 0.5223880597014925   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7809712886810303   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.6705000400543213   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7411700487136841   train_acc = 0.5223880597014925   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7060892581939697   train_acc = 0.5223880597014925   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.726719856262207   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "Final val acc for lr=3.382370800515571e-06, wd=0.00032736198184168545: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=3.382370800515571e-06, wd=0.00032736198184168545, betas=(0.95, 0.995)\n",
      "iteration: 1/9   loss = 0.7410180568695068   train_acc = 0.5074626865671642   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.7365434169769287   train_acc = 0.5522388059701493   val_acc = 0.5714285714285714\n",
      "iteration: 3/9   loss = 0.721491277217865   train_acc = 0.5373134328358209   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7008768320083618   train_acc = 0.5   val_acc = 0.5714285714285714\n",
      "iteration: 5/9   loss = 0.7066247463226318   train_acc = 0.5970149253731343   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.7965594530105591   train_acc = 0.6119402985074627   val_acc = 0.2857142857142857\n",
      "iteration: 7/9   loss = 0.6819047927856445   train_acc = 0.5447761194029851   val_acc = 0.35714285714285715\n",
      "iteration: 8/9   loss = 0.7406630516052246   train_acc = 0.5746268656716418   val_acc = 0.35714285714285715\n",
      "iteration: 9/9   loss = 0.6489734053611755   train_acc = 0.5671641791044776   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=3.382370800515571e-06, wd=0.00032736198184168545: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=3.382370800515571e-06, wd=0.00032736198184168545, betas=(0.95, 0.999)\n",
      "iteration: 1/9   loss = 0.7665745615959167   train_acc = 0.5   val_acc = 0.6428571428571429\n",
      "iteration: 2/9   loss = 0.7689444422721863   train_acc = 0.4626865671641791   val_acc = 0.5714285714285714\n",
      "iteration: 3/9   loss = 0.7668417692184448   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.6973515748977661   train_acc = 0.44776119402985076   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.6981321573257446   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.5400347709655762   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.6803324222564697   train_acc = 0.5   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.7110195755958557   train_acc = 0.5223880597014925   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.6345663070678711   train_acc = 0.5298507462686567   val_acc = 0.5714285714285714\n",
      "Final val acc for lr=3.382370800515571e-06, wd=0.00032736198184168545: 0.5714\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=7.481537196579658e-05, wd=0.0004609389889268166, betas=(0.85, 0.99)\n",
      "iteration: 1/9   loss = 0.6962858438491821   train_acc = 0.5149253731343284   val_acc = 0.5714285714285714\n",
      "iteration: 2/9   loss = 0.690861701965332   train_acc = 0.6044776119402985   val_acc = 0.35714285714285715\n",
      "iteration: 3/9   loss = 0.8645314574241638   train_acc = 0.6268656716417911   val_acc = 0.35714285714285715\n",
      "iteration: 4/9   loss = 0.6353534460067749   train_acc = 0.7014925373134329   val_acc = 0.35714285714285715\n",
      "iteration: 5/9   loss = 0.675818145275116   train_acc = 0.6044776119402985   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.6358952522277832   train_acc = 0.6716417910447762   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.6694268584251404   train_acc = 0.664179104477612   val_acc = 0.21428571428571427\n",
      "iteration: 8/9   loss = 0.6183844208717346   train_acc = 0.7014925373134329   val_acc = 0.35714285714285715\n",
      "iteration: 9/9   loss = 0.6025628447532654   train_acc = 0.6567164179104478   val_acc = 0.2857142857142857\n",
      "Final val acc for lr=7.481537196579658e-05, wd=0.0004609389889268166: 0.2857\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=7.481537196579658e-05, wd=0.0004609389889268166, betas=(0.85, 0.995)\n",
      "iteration: 1/9   loss = 0.7229465842247009   train_acc = 0.5522388059701493   val_acc = 0.2857142857142857\n",
      "iteration: 2/9   loss = 0.7489832639694214   train_acc = 0.5970149253731343   val_acc = 0.2857142857142857\n",
      "iteration: 3/9   loss = 0.8402471542358398   train_acc = 0.6268656716417911   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.7069376707077026   train_acc = 0.5895522388059702   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.681815505027771   train_acc = 0.5746268656716418   val_acc = 0.35714285714285715\n",
      "iteration: 6/9   loss = 0.8114562034606934   train_acc = 0.5895522388059702   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.7596186399459839   train_acc = 0.6194029850746269   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.6727546453475952   train_acc = 0.6343283582089553   val_acc = 0.42857142857142855\n",
      "iteration: 9/9   loss = 0.6042703986167908   train_acc = 0.6268656716417911   val_acc = 0.5\n",
      "Final val acc for lr=7.481537196579658e-05, wd=0.0004609389889268166: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=7.481537196579658e-05, wd=0.0004609389889268166, betas=(0.9, 0.98)\n",
      "iteration: 1/9   loss = 0.6585378646850586   train_acc = 0.5671641791044776   val_acc = 0.6428571428571429\n",
      "iteration: 2/9   loss = 0.7196873426437378   train_acc = 0.5522388059701493   val_acc = 0.6428571428571429\n",
      "iteration: 3/9   loss = 0.7358084917068481   train_acc = 0.5895522388059702   val_acc = 0.7857142857142857\n",
      "iteration: 4/9   loss = 0.6584792137145996   train_acc = 0.6268656716417911   val_acc = 0.7857142857142857\n",
      "iteration: 5/9   loss = 0.6819314360618591   train_acc = 0.6343283582089553   val_acc = 0.6428571428571429\n",
      "iteration: 6/9   loss = 0.6393575668334961   train_acc = 0.5746268656716418   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.6087323427200317   train_acc = 0.582089552238806   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.6727720499038696   train_acc = 0.5671641791044776   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.7401079535484314   train_acc = 0.582089552238806   val_acc = 0.5\n",
      "Final val acc for lr=7.481537196579658e-05, wd=0.0004609389889268166: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=7.481537196579658e-05, wd=0.0004609389889268166, betas=(0.9, 0.99)\n",
      "iteration: 1/9   loss = 0.8024157285690308   train_acc = 0.5074626865671642   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.7684913277626038   train_acc = 0.582089552238806   val_acc = 0.6428571428571429\n",
      "iteration: 3/9   loss = 0.6958842277526855   train_acc = 0.5970149253731343   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.7308107018470764   train_acc = 0.5671641791044776   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7025970816612244   train_acc = 0.5373134328358209   val_acc = 0.5714285714285714\n",
      "iteration: 6/9   loss = 0.5161872506141663   train_acc = 0.582089552238806   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.6823676824569702   train_acc = 0.5746268656716418   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.6813182830810547   train_acc = 0.582089552238806   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.632556676864624   train_acc = 0.582089552238806   val_acc = 0.5\n",
      "Final val acc for lr=7.481537196579658e-05, wd=0.0004609389889268166: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=7.481537196579658e-05, wd=0.0004609389889268166, betas=(0.9, 0.995)\n",
      "iteration: 1/9   loss = 0.7211490273475647   train_acc = 0.5   val_acc = 0.2857142857142857\n",
      "iteration: 2/9   loss = 0.7451164722442627   train_acc = 0.5746268656716418   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.73212069272995   train_acc = 0.6343283582089553   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7175459861755371   train_acc = 0.6044776119402985   val_acc = 0.35714285714285715\n",
      "iteration: 5/9   loss = 0.6614124178886414   train_acc = 0.5970149253731343   val_acc = 0.35714285714285715\n",
      "iteration: 6/9   loss = 0.6116930842399597   train_acc = 0.582089552238806   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.663179337978363   train_acc = 0.6194029850746269   val_acc = 0.35714285714285715\n",
      "iteration: 8/9   loss = 0.6565405130386353   train_acc = 0.6417910447761194   val_acc = 0.42857142857142855\n",
      "iteration: 9/9   loss = 0.6749967336654663   train_acc = 0.6044776119402985   val_acc = 0.35714285714285715\n",
      "Final val acc for lr=7.481537196579658e-05, wd=0.0004609389889268166: 0.3571\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=7.481537196579658e-05, wd=0.0004609389889268166, betas=(0.9, 0.999)\n",
      "iteration: 1/9   loss = 0.7065733671188354   train_acc = 0.5074626865671642   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.7751367092132568   train_acc = 0.5522388059701493   val_acc = 0.6428571428571429\n",
      "iteration: 3/9   loss = 0.6200923919677734   train_acc = 0.6417910447761194   val_acc = 0.6428571428571429\n",
      "iteration: 4/9   loss = 0.6977176070213318   train_acc = 0.6194029850746269   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.7479875683784485   train_acc = 0.6343283582089553   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.6251018047332764   train_acc = 0.6268656716417911   val_acc = 0.7142857142857143\n",
      "iteration: 7/9   loss = 0.6083606481552124   train_acc = 0.6417910447761194   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.6995700001716614   train_acc = 0.6194029850746269   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.5710353851318359   train_acc = 0.6417910447761194   val_acc = 0.5714285714285714\n",
      "Final val acc for lr=7.481537196579658e-05, wd=0.0004609389889268166: 0.5714\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=7.481537196579658e-05, wd=0.0004609389889268166, betas=(0.95, 0.98)\n",
      "iteration: 1/9   loss = 0.6885125637054443   train_acc = 0.5   val_acc = 0.5714285714285714\n",
      "iteration: 2/9   loss = 0.7095394134521484   train_acc = 0.6044776119402985   val_acc = 0.7857142857142857\n",
      "iteration: 3/9   loss = 0.7176894545555115   train_acc = 0.5522388059701493   val_acc = 0.6428571428571429\n",
      "iteration: 4/9   loss = 0.6886749863624573   train_acc = 0.5373134328358209   val_acc = 0.6428571428571429\n",
      "iteration: 5/9   loss = 0.7344353199005127   train_acc = 0.5223880597014925   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.6448628902435303   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.6844197511672974   train_acc = 0.5149253731343284   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.6653062105178833   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.8237282633781433   train_acc = 0.5   val_acc = 0.5\n",
      "Final val acc for lr=7.481537196579658e-05, wd=0.0004609389889268166: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=7.481537196579658e-05, wd=0.0004609389889268166, betas=(0.95, 0.99)\n",
      "iteration: 1/9   loss = 0.7437963485717773   train_acc = 0.5597014925373134   val_acc = 0.5714285714285714\n",
      "iteration: 2/9   loss = 0.7032957077026367   train_acc = 0.6268656716417911   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.8535593748092651   train_acc = 0.6567164179104478   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.7202330827713013   train_acc = 0.6119402985074627   val_acc = 0.6428571428571429\n",
      "iteration: 5/9   loss = 0.6833189129829407   train_acc = 0.6716417910447762   val_acc = 0.6428571428571429\n",
      "iteration: 6/9   loss = 0.5579198598861694   train_acc = 0.6492537313432836   val_acc = 0.6428571428571429\n",
      "iteration: 7/9   loss = 0.7084392309188843   train_acc = 0.664179104477612   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.6650041937828064   train_acc = 0.6567164179104478   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.6334937810897827   train_acc = 0.7089552238805971   val_acc = 0.7142857142857143\n",
      "Final val acc for lr=7.481537196579658e-05, wd=0.0004609389889268166: 0.7143\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=7.481537196579658e-05, wd=0.0004609389889268166, betas=(0.95, 0.995)\n",
      "iteration: 1/9   loss = 0.7480830550193787   train_acc = 0.5447761194029851   val_acc = 0.35714285714285715\n",
      "iteration: 2/9   loss = 0.6719944477081299   train_acc = 0.6044776119402985   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.6968368291854858   train_acc = 0.6417910447761194   val_acc = 0.35714285714285715\n",
      "iteration: 4/9   loss = 0.7182823419570923   train_acc = 0.5895522388059702   val_acc = 0.2857142857142857\n",
      "iteration: 5/9   loss = 0.6954777836799622   train_acc = 0.6417910447761194   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.588011622428894   train_acc = 0.6044776119402985   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.6706836223602295   train_acc = 0.6343283582089553   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.685066282749176   train_acc = 0.6194029850746269   val_acc = 0.42857142857142855\n",
      "iteration: 9/9   loss = 0.5099575519561768   train_acc = 0.6044776119402985   val_acc = 0.5\n",
      "Final val acc for lr=7.481537196579658e-05, wd=0.0004609389889268166: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=7.481537196579658e-05, wd=0.0004609389889268166, betas=(0.95, 0.999)\n",
      "iteration: 1/9   loss = 0.7031898498535156   train_acc = 0.5597014925373134   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7578431963920593   train_acc = 0.5671641791044776   val_acc = 0.5714285714285714\n",
      "iteration: 3/9   loss = 0.7096480131149292   train_acc = 0.5895522388059702   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.7071950435638428   train_acc = 0.6567164179104478   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.6623678803443909   train_acc = 0.6417910447761194   val_acc = 0.5714285714285714\n",
      "iteration: 6/9   loss = 0.6371311545372009   train_acc = 0.6119402985074627   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.6576617360115051   train_acc = 0.6417910447761194   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.6941816806793213   train_acc = 0.6343283582089553   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.6777859926223755   train_acc = 0.6268656716417911   val_acc = 0.5\n",
      "Final val acc for lr=7.481537196579658e-05, wd=0.0004609389889268166: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=7.481537196579658e-05, wd=0.0002300292096821906, betas=(0.85, 0.99)\n",
      "iteration: 1/9   loss = 0.6640423536300659   train_acc = 0.5373134328358209   val_acc = 0.5714285714285714\n",
      "iteration: 2/9   loss = 0.6590815782546997   train_acc = 0.6417910447761194   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.6998068690299988   train_acc = 0.6119402985074627   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.7098113894462585   train_acc = 0.6492537313432836   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.6846410036087036   train_acc = 0.582089552238806   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.6551351547241211   train_acc = 0.6492537313432836   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.679633378982544   train_acc = 0.6194029850746269   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.6491560935974121   train_acc = 0.6716417910447762   val_acc = 0.42857142857142855\n",
      "iteration: 9/9   loss = 0.8366678953170776   train_acc = 0.664179104477612   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=7.481537196579658e-05, wd=0.0002300292096821906: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=7.481537196579658e-05, wd=0.0002300292096821906, betas=(0.85, 0.995)\n",
      "iteration: 1/9   loss = 0.7330647706985474   train_acc = 0.582089552238806   val_acc = 0.5714285714285714\n",
      "iteration: 2/9   loss = 0.6935391426086426   train_acc = 0.582089552238806   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.6263898611068726   train_acc = 0.582089552238806   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.676123321056366   train_acc = 0.5447761194029851   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7160681486129761   train_acc = 0.5671641791044776   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.7743915319442749   train_acc = 0.5970149253731343   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.6982349157333374   train_acc = 0.5895522388059702   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.6583936214447021   train_acc = 0.5895522388059702   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.6162230968475342   train_acc = 0.5970149253731343   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=7.481537196579658e-05, wd=0.0002300292096821906: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=7.481537196579658e-05, wd=0.0002300292096821906, betas=(0.9, 0.98)\n",
      "iteration: 1/9   loss = 0.6818702220916748   train_acc = 0.5373134328358209   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.7060900926589966   train_acc = 0.5597014925373134   val_acc = 0.7142857142857143\n",
      "iteration: 3/9   loss = 0.7216043472290039   train_acc = 0.6194029850746269   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.7534118890762329   train_acc = 0.5671641791044776   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.7082353830337524   train_acc = 0.5447761194029851   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.5354940295219421   train_acc = 0.582089552238806   val_acc = 0.35714285714285715\n",
      "iteration: 7/9   loss = 0.7006550431251526   train_acc = 0.5746268656716418   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.6645114421844482   train_acc = 0.5671641791044776   val_acc = 0.42857142857142855\n",
      "iteration: 9/9   loss = 0.7145947813987732   train_acc = 0.6044776119402985   val_acc = 0.5\n",
      "Final val acc for lr=7.481537196579658e-05, wd=0.0002300292096821906: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=7.481537196579658e-05, wd=0.0002300292096821906, betas=(0.9, 0.99)\n",
      "iteration: 1/9   loss = 0.6674717664718628   train_acc = 0.6119402985074627   val_acc = 0.7142857142857143\n",
      "iteration: 2/9   loss = 0.7332693934440613   train_acc = 0.6343283582089553   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.7403649687767029   train_acc = 0.6044776119402985   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.6272169351577759   train_acc = 0.6791044776119403   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.6388030648231506   train_acc = 0.6268656716417911   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.653312623500824   train_acc = 0.6492537313432836   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.6526963710784912   train_acc = 0.6567164179104478   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.6589444875717163   train_acc = 0.6716417910447762   val_acc = 0.35714285714285715\n",
      "iteration: 9/9   loss = 0.6621570587158203   train_acc = 0.6567164179104478   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=7.481537196579658e-05, wd=0.0002300292096821906: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=7.481537196579658e-05, wd=0.0002300292096821906, betas=(0.9, 0.995)\n",
      "iteration: 1/9   loss = 0.798530101776123   train_acc = 0.5298507462686567   val_acc = 0.5714285714285714\n",
      "iteration: 2/9   loss = 0.7296972274780273   train_acc = 0.6791044776119403   val_acc = 0.7142857142857143\n",
      "iteration: 3/9   loss = 0.7157205939292908   train_acc = 0.6268656716417911   val_acc = 0.6428571428571429\n",
      "iteration: 4/9   loss = 0.6634606122970581   train_acc = 0.6343283582089553   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.6739510297775269   train_acc = 0.6791044776119403   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.527762770652771   train_acc = 0.6343283582089553   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.6432993412017822   train_acc = 0.6492537313432836   val_acc = 0.7142857142857143\n",
      "iteration: 8/9   loss = 0.6249615550041199   train_acc = 0.6791044776119403   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.7339701652526855   train_acc = 0.6716417910447762   val_acc = 0.5\n",
      "Final val acc for lr=7.481537196579658e-05, wd=0.0002300292096821906: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=7.481537196579658e-05, wd=0.0002300292096821906, betas=(0.9, 0.999)\n",
      "iteration: 1/9   loss = 0.7002468109130859   train_acc = 0.5223880597014925   val_acc = 0.5714285714285714\n",
      "iteration: 2/9   loss = 0.7245857119560242   train_acc = 0.5970149253731343   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.6859891414642334   train_acc = 0.6268656716417911   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.7104619741439819   train_acc = 0.6492537313432836   val_acc = 0.6428571428571429\n",
      "iteration: 5/9   loss = 0.6743126511573792   train_acc = 0.7089552238805971   val_acc = 0.5714285714285714\n",
      "iteration: 6/9   loss = 0.7642353773117065   train_acc = 0.6567164179104478   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.6450356245040894   train_acc = 0.6716417910447762   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.6903551816940308   train_acc = 0.664179104477612   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.7803225517272949   train_acc = 0.6492537313432836   val_acc = 0.5714285714285714\n",
      "Final val acc for lr=7.481537196579658e-05, wd=0.0002300292096821906: 0.5714\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=7.481537196579658e-05, wd=0.0002300292096821906, betas=(0.95, 0.98)\n",
      "iteration: 1/9   loss = 0.6814548373222351   train_acc = 0.6044776119402985   val_acc = 0.35714285714285715\n",
      "iteration: 2/9   loss = 0.7068865299224854   train_acc = 0.6417910447761194   val_acc = 0.2857142857142857\n",
      "iteration: 3/9   loss = 0.7529646754264832   train_acc = 0.6865671641791045   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.6869645118713379   train_acc = 0.6194029850746269   val_acc = 0.2857142857142857\n",
      "iteration: 5/9   loss = 0.7111979722976685   train_acc = 0.6417910447761194   val_acc = 0.35714285714285715\n",
      "iteration: 6/9   loss = 0.7307280898094177   train_acc = 0.6492537313432836   val_acc = 0.2857142857142857\n",
      "iteration: 7/9   loss = 0.693688690662384   train_acc = 0.6119402985074627   val_acc = 0.35714285714285715\n",
      "iteration: 8/9   loss = 0.6803364753723145   train_acc = 0.664179104477612   val_acc = 0.35714285714285715\n",
      "iteration: 9/9   loss = 0.7661261558532715   train_acc = 0.6716417910447762   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=7.481537196579658e-05, wd=0.0002300292096821906: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=7.481537196579658e-05, wd=0.0002300292096821906, betas=(0.95, 0.99)\n",
      "iteration: 1/9   loss = 0.7256536483764648   train_acc = 0.5522388059701493   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7311935424804688   train_acc = 0.5522388059701493   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.7063196897506714   train_acc = 0.582089552238806   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.6620303988456726   train_acc = 0.6194029850746269   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.7206932902336121   train_acc = 0.6417910447761194   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.5420705080032349   train_acc = 0.6194029850746269   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.6694787740707397   train_acc = 0.6716417910447762   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.6625618934631348   train_acc = 0.6417910447761194   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.5267840623855591   train_acc = 0.664179104477612   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=7.481537196579658e-05, wd=0.0002300292096821906: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=7.481537196579658e-05, wd=0.0002300292096821906, betas=(0.95, 0.995)\n",
      "iteration: 1/9   loss = 0.755628228187561   train_acc = 0.4701492537313433   val_acc = 0.2857142857142857\n",
      "iteration: 2/9   loss = 0.7244166135787964   train_acc = 0.47761194029850745   val_acc = 0.21428571428571427\n",
      "iteration: 3/9   loss = 0.867965817451477   train_acc = 0.5671641791044776   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.6811727285385132   train_acc = 0.5447761194029851   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7549272775650024   train_acc = 0.5223880597014925   val_acc = 0.7142857142857143\n",
      "iteration: 6/9   loss = 0.7681745886802673   train_acc = 0.5522388059701493   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.687588095664978   train_acc = 0.6044776119402985   val_acc = 0.6428571428571429\n",
      "iteration: 8/9   loss = 0.6638950109481812   train_acc = 0.6268656716417911   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.715235710144043   train_acc = 0.6268656716417911   val_acc = 0.5714285714285714\n",
      "Final val acc for lr=7.481537196579658e-05, wd=0.0002300292096821906: 0.5714\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=7.481537196579658e-05, wd=0.0002300292096821906, betas=(0.95, 0.999)\n",
      "iteration: 1/9   loss = 0.7052296996116638   train_acc = 0.582089552238806   val_acc = 0.5714285714285714\n",
      "iteration: 2/9   loss = 0.7398724555969238   train_acc = 0.5895522388059702   val_acc = 0.6428571428571429\n",
      "iteration: 3/9   loss = 0.8165779113769531   train_acc = 0.6417910447761194   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7102547287940979   train_acc = 0.5746268656716418   val_acc = 0.5714285714285714\n",
      "iteration: 5/9   loss = 0.7054581046104431   train_acc = 0.5671641791044776   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.41256123781204224   train_acc = 0.5447761194029851   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.63776695728302   train_acc = 0.5223880597014925   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7053278684616089   train_acc = 0.5597014925373134   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.8036355972290039   train_acc = 0.5373134328358209   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=7.481537196579658e-05, wd=0.0002300292096821906: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=7.481537196579658e-05, wd=0.00022555743977178082, betas=(0.85, 0.99)\n",
      "iteration: 1/9   loss = 0.6903731822967529   train_acc = 0.5373134328358209   val_acc = 0.5714285714285714\n",
      "iteration: 2/9   loss = 0.711685061454773   train_acc = 0.5970149253731343   val_acc = 0.7857142857142857\n",
      "iteration: 3/9   loss = 0.6558082103729248   train_acc = 0.6194029850746269   val_acc = 0.6428571428571429\n",
      "iteration: 4/9   loss = 0.7177042961120605   train_acc = 0.6119402985074627   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.6282944679260254   train_acc = 0.6194029850746269   val_acc = 0.5714285714285714\n",
      "iteration: 6/9   loss = 0.5984316468238831   train_acc = 0.6492537313432836   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.6717662811279297   train_acc = 0.7238805970149254   val_acc = 0.35714285714285715\n",
      "iteration: 8/9   loss = 0.7000827193260193   train_acc = 0.6791044776119403   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.8436762690544128   train_acc = 0.6716417910447762   val_acc = 0.5\n",
      "Final val acc for lr=7.481537196579658e-05, wd=0.00022555743977178082: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=7.481537196579658e-05, wd=0.00022555743977178082, betas=(0.85, 0.995)\n",
      "iteration: 1/9   loss = 0.7042796611785889   train_acc = 0.582089552238806   val_acc = 0.8571428571428571\n",
      "iteration: 2/9   loss = 0.7374333143234253   train_acc = 0.5671641791044776   val_acc = 0.9285714285714286\n",
      "iteration: 3/9   loss = 0.6070482730865479   train_acc = 0.582089552238806   val_acc = 0.7142857142857143\n",
      "iteration: 4/9   loss = 0.7246807813644409   train_acc = 0.5895522388059702   val_acc = 0.6428571428571429\n",
      "iteration: 5/9   loss = 0.6646349430084229   train_acc = 0.6044776119402985   val_acc = 0.7857142857142857\n",
      "iteration: 6/9   loss = 0.6382501125335693   train_acc = 0.6940298507462687   val_acc = 0.7142857142857143\n",
      "iteration: 7/9   loss = 0.6591845750808716   train_acc = 0.6716417910447762   val_acc = 0.7857142857142857\n",
      "iteration: 8/9   loss = 0.6997463703155518   train_acc = 0.6865671641791045   val_acc = 0.6428571428571429\n",
      "iteration: 9/9   loss = 0.4898775517940521   train_acc = 0.6343283582089553   val_acc = 0.7142857142857143\n",
      "Final val acc for lr=7.481537196579658e-05, wd=0.00022555743977178082: 0.7143\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=7.481537196579658e-05, wd=0.00022555743977178082, betas=(0.9, 0.98)\n",
      "iteration: 1/9   loss = 0.696725606918335   train_acc = 0.5447761194029851   val_acc = 0.35714285714285715\n",
      "iteration: 2/9   loss = 0.7324884533882141   train_acc = 0.5970149253731343   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.7577922344207764   train_acc = 0.6343283582089553   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.6938600540161133   train_acc = 0.582089552238806   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.7020008563995361   train_acc = 0.6417910447761194   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.7224790453910828   train_acc = 0.6044776119402985   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.6772063970565796   train_acc = 0.6194029850746269   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.6437215805053711   train_acc = 0.6567164179104478   val_acc = 0.42857142857142855\n",
      "iteration: 9/9   loss = 0.5987449884414673   train_acc = 0.664179104477612   val_acc = 0.2857142857142857\n",
      "Final val acc for lr=7.481537196579658e-05, wd=0.00022555743977178082: 0.2857\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=7.481537196579658e-05, wd=0.00022555743977178082, betas=(0.9, 0.99)\n",
      "iteration: 1/9   loss = 0.7133989334106445   train_acc = 0.5895522388059702   val_acc = 0.7142857142857143\n",
      "iteration: 2/9   loss = 0.7581918239593506   train_acc = 0.582089552238806   val_acc = 0.7142857142857143\n",
      "iteration: 3/9   loss = 0.621262788772583   train_acc = 0.6044776119402985   val_acc = 0.6428571428571429\n",
      "iteration: 4/9   loss = 0.638505220413208   train_acc = 0.5970149253731343   val_acc = 0.6428571428571429\n",
      "iteration: 5/9   loss = 0.6837983131408691   train_acc = 0.5895522388059702   val_acc = 0.7142857142857143\n",
      "iteration: 6/9   loss = 0.7285968065261841   train_acc = 0.6492537313432836   val_acc = 0.7857142857142857\n",
      "iteration: 7/9   loss = 0.7043151259422302   train_acc = 0.6268656716417911   val_acc = 0.8571428571428571\n",
      "iteration: 8/9   loss = 0.6368430852890015   train_acc = 0.6716417910447762   val_acc = 0.7142857142857143\n",
      "iteration: 9/9   loss = 0.6093997955322266   train_acc = 0.6268656716417911   val_acc = 0.6428571428571429\n",
      "Final val acc for lr=7.481537196579658e-05, wd=0.00022555743977178082: 0.6429\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=7.481537196579658e-05, wd=0.00022555743977178082, betas=(0.9, 0.995)\n",
      "iteration: 1/9   loss = 0.7046384215354919   train_acc = 0.5373134328358209   val_acc = 0.35714285714285715\n",
      "iteration: 2/9   loss = 0.7474478483200073   train_acc = 0.5522388059701493   val_acc = 0.2857142857142857\n",
      "iteration: 3/9   loss = 0.6423212289810181   train_acc = 0.5746268656716418   val_acc = 0.35714285714285715\n",
      "iteration: 4/9   loss = 0.6904687881469727   train_acc = 0.5447761194029851   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.6472017765045166   train_acc = 0.5597014925373134   val_acc = 0.35714285714285715\n",
      "iteration: 6/9   loss = 0.6941826343536377   train_acc = 0.5746268656716418   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7280774712562561   train_acc = 0.5597014925373134   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.6503342390060425   train_acc = 0.6044776119402985   val_acc = 0.35714285714285715\n",
      "iteration: 9/9   loss = 0.8774099349975586   train_acc = 0.6194029850746269   val_acc = 0.35714285714285715\n",
      "Final val acc for lr=7.481537196579658e-05, wd=0.00022555743977178082: 0.3571\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=7.481537196579658e-05, wd=0.00022555743977178082, betas=(0.9, 0.999)\n",
      "iteration: 1/9   loss = 0.7226434946060181   train_acc = 0.5970149253731343   val_acc = 0.5714285714285714\n",
      "iteration: 2/9   loss = 0.7125481367111206   train_acc = 0.5447761194029851   val_acc = 0.7142857142857143\n",
      "iteration: 3/9   loss = 0.5060859322547913   train_acc = 0.6119402985074627   val_acc = 0.6428571428571429\n",
      "iteration: 4/9   loss = 0.71575927734375   train_acc = 0.5970149253731343   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.6550769805908203   train_acc = 0.6343283582089553   val_acc = 0.6428571428571429\n",
      "iteration: 6/9   loss = 0.7043416500091553   train_acc = 0.6940298507462687   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.6720625162124634   train_acc = 0.6417910447761194   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.6425485610961914   train_acc = 0.6417910447761194   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.9447654485702515   train_acc = 0.6567164179104478   val_acc = 0.6428571428571429\n",
      "Final val acc for lr=7.481537196579658e-05, wd=0.00022555743977178082: 0.6429\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=7.481537196579658e-05, wd=0.00022555743977178082, betas=(0.95, 0.98)\n",
      "iteration: 1/9   loss = 0.7776228785514832   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.6714279651641846   train_acc = 0.5447761194029851   val_acc = 0.5714285714285714\n",
      "iteration: 3/9   loss = 0.6061464548110962   train_acc = 0.5746268656716418   val_acc = 0.35714285714285715\n",
      "iteration: 4/9   loss = 0.6300879120826721   train_acc = 0.5522388059701493   val_acc = 0.35714285714285715\n",
      "iteration: 5/9   loss = 0.6483011245727539   train_acc = 0.5671641791044776   val_acc = 0.2857142857142857\n",
      "iteration: 6/9   loss = 0.8896486163139343   train_acc = 0.582089552238806   val_acc = 0.35714285714285715\n",
      "iteration: 7/9   loss = 0.7085146307945251   train_acc = 0.7089552238805971   val_acc = 0.35714285714285715\n",
      "iteration: 8/9   loss = 0.7250528335571289   train_acc = 0.664179104477612   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.6761321425437927   train_acc = 0.6417910447761194   val_acc = 0.5\n",
      "Final val acc for lr=7.481537196579658e-05, wd=0.00022555743977178082: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=7.481537196579658e-05, wd=0.00022555743977178082, betas=(0.95, 0.99)\n",
      "iteration: 1/9   loss = 0.6992483139038086   train_acc = 0.5746268656716418   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.7206034660339355   train_acc = 0.5746268656716418   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.6334085464477539   train_acc = 0.6268656716417911   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.6540780067443848   train_acc = 0.6194029850746269   val_acc = 0.35714285714285715\n",
      "iteration: 5/9   loss = 0.6806477308273315   train_acc = 0.6567164179104478   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.7684483528137207   train_acc = 0.6268656716417911   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.6657113432884216   train_acc = 0.6119402985074627   val_acc = 0.35714285714285715\n",
      "iteration: 8/9   loss = 0.6617524027824402   train_acc = 0.6044776119402985   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.7817441821098328   train_acc = 0.6343283582089553   val_acc = 0.35714285714285715\n",
      "Final val acc for lr=7.481537196579658e-05, wd=0.00022555743977178082: 0.3571\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=7.481537196579658e-05, wd=0.00022555743977178082, betas=(0.95, 0.995)\n",
      "iteration: 1/9   loss = 0.7138820290565491   train_acc = 0.5149253731343284   val_acc = 0.6428571428571429\n",
      "iteration: 2/9   loss = 0.7373605966567993   train_acc = 0.5671641791044776   val_acc = 0.5714285714285714\n",
      "iteration: 3/9   loss = 0.6214303374290466   train_acc = 0.6417910447761194   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.67032790184021   train_acc = 0.6119402985074627   val_acc = 0.35714285714285715\n",
      "iteration: 5/9   loss = 0.6677219867706299   train_acc = 0.6716417910447762   val_acc = 0.2857142857142857\n",
      "iteration: 6/9   loss = 0.6546791195869446   train_acc = 0.6492537313432836   val_acc = 0.35714285714285715\n",
      "iteration: 7/9   loss = 0.7179540395736694   train_acc = 0.664179104477612   val_acc = 0.2857142857142857\n",
      "iteration: 8/9   loss = 0.6461265087127686   train_acc = 0.664179104477612   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.7527668476104736   train_acc = 0.6791044776119403   val_acc = 0.5\n",
      "Final val acc for lr=7.481537196579658e-05, wd=0.00022555743977178082: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=7.481537196579658e-05, wd=0.00022555743977178082, betas=(0.95, 0.999)\n",
      "iteration: 1/9   loss = 0.6722187995910645   train_acc = 0.5895522388059702   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.7320221662521362   train_acc = 0.5970149253731343   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.8501360416412354   train_acc = 0.6343283582089553   val_acc = 0.7142857142857143\n",
      "iteration: 4/9   loss = 0.6642113327980042   train_acc = 0.5895522388059702   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.6806987524032593   train_acc = 0.582089552238806   val_acc = 0.35714285714285715\n",
      "iteration: 6/9   loss = 0.7678331136703491   train_acc = 0.5895522388059702   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.7056142091751099   train_acc = 0.6194029850746269   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.684434175491333   train_acc = 0.5746268656716418   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.6714238524436951   train_acc = 0.6194029850746269   val_acc = 0.5\n",
      "Final val acc for lr=7.481537196579658e-05, wd=0.00022555743977178082: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=7.481537196579658e-05, wd=0.00011822143451838108, betas=(0.85, 0.99)\n",
      "iteration: 1/9   loss = 0.7915672063827515   train_acc = 0.5   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.6715000867843628   train_acc = 0.5597014925373134   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.6287592649459839   train_acc = 0.6268656716417911   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.6450642943382263   train_acc = 0.6194029850746269   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.7024195194244385   train_acc = 0.6492537313432836   val_acc = 0.35714285714285715\n",
      "iteration: 6/9   loss = 0.754651665687561   train_acc = 0.6716417910447762   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.6851094961166382   train_acc = 0.6343283582089553   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.6796892285346985   train_acc = 0.6567164179104478   val_acc = 0.42857142857142855\n",
      "iteration: 9/9   loss = 0.5382492542266846   train_acc = 0.6492537313432836   val_acc = 0.5\n",
      "Final val acc for lr=7.481537196579658e-05, wd=0.00011822143451838108: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=7.481537196579658e-05, wd=0.00011822143451838108, betas=(0.85, 0.995)\n",
      "iteration: 1/9   loss = 0.7404179573059082   train_acc = 0.5597014925373134   val_acc = 0.7142857142857143\n",
      "iteration: 2/9   loss = 0.658913254737854   train_acc = 0.6044776119402985   val_acc = 0.7142857142857143\n",
      "iteration: 3/9   loss = 0.6445515155792236   train_acc = 0.5970149253731343   val_acc = 0.7857142857142857\n",
      "iteration: 4/9   loss = 0.7415531873703003   train_acc = 0.5970149253731343   val_acc = 0.6428571428571429\n",
      "iteration: 5/9   loss = 0.6699406504631042   train_acc = 0.5671641791044776   val_acc = 0.7142857142857143\n",
      "iteration: 6/9   loss = 0.7351627349853516   train_acc = 0.6044776119402985   val_acc = 0.7142857142857143\n",
      "iteration: 7/9   loss = 0.6742000579833984   train_acc = 0.6119402985074627   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.6350382566452026   train_acc = 0.6194029850746269   val_acc = 0.6428571428571429\n",
      "iteration: 9/9   loss = 0.8213928937911987   train_acc = 0.6268656716417911   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=7.481537196579658e-05, wd=0.00011822143451838108: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=7.481537196579658e-05, wd=0.00011822143451838108, betas=(0.9, 0.98)\n",
      "iteration: 1/9   loss = 0.7128320336341858   train_acc = 0.5223880597014925   val_acc = 0.5714285714285714\n",
      "iteration: 2/9   loss = 0.7211645841598511   train_acc = 0.6044776119402985   val_acc = 0.35714285714285715\n",
      "iteration: 3/9   loss = 0.7080129981040955   train_acc = 0.6119402985074627   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.7354131937026978   train_acc = 0.6268656716417911   val_acc = 0.35714285714285715\n",
      "iteration: 5/9   loss = 0.7563289403915405   train_acc = 0.582089552238806   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.6206375956535339   train_acc = 0.6044776119402985   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.6303045749664307   train_acc = 0.5895522388059702   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.6897328495979309   train_acc = 0.6119402985074627   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.7110008597373962   train_acc = 0.6119402985074627   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=7.481537196579658e-05, wd=0.00011822143451838108: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=7.481537196579658e-05, wd=0.00011822143451838108, betas=(0.9, 0.99)\n",
      "iteration: 1/9   loss = 0.7241195440292358   train_acc = 0.5149253731343284   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.7010687589645386   train_acc = 0.6567164179104478   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.8661691546440125   train_acc = 0.6865671641791045   val_acc = 0.2857142857142857\n",
      "iteration: 4/9   loss = 0.6607021689414978   train_acc = 0.6940298507462687   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.6847826838493347   train_acc = 0.7014925373134329   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.638080358505249   train_acc = 0.6940298507462687   val_acc = 0.2857142857142857\n",
      "iteration: 7/9   loss = 0.6533843278884888   train_acc = 0.746268656716418   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.6715232133865356   train_acc = 0.7164179104477612   val_acc = 0.21428571428571427\n",
      "iteration: 9/9   loss = 0.6707083582878113   train_acc = 0.6865671641791045   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=7.481537196579658e-05, wd=0.00011822143451838108: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=7.481537196579658e-05, wd=0.00011822143451838108, betas=(0.9, 0.995)\n",
      "iteration: 1/9   loss = 0.6831259727478027   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7001128196716309   train_acc = 0.6044776119402985   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.7127097845077515   train_acc = 0.5746268656716418   val_acc = 0.2857142857142857\n",
      "iteration: 4/9   loss = 0.6843528747558594   train_acc = 0.6268656716417911   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.7178723812103271   train_acc = 0.6044776119402985   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.889722466468811   train_acc = 0.582089552238806   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.6727721095085144   train_acc = 0.6194029850746269   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.6368969678878784   train_acc = 0.664179104477612   val_acc = 0.42857142857142855\n",
      "iteration: 9/9   loss = 0.6747636795043945   train_acc = 0.664179104477612   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=7.481537196579658e-05, wd=0.00011822143451838108: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=7.481537196579658e-05, wd=0.00011822143451838108, betas=(0.9, 0.999)\n",
      "iteration: 1/9   loss = 0.7137671709060669   train_acc = 0.47761194029850745   val_acc = 0.5714285714285714\n",
      "iteration: 2/9   loss = 0.684563934803009   train_acc = 0.5373134328358209   val_acc = 0.6428571428571429\n",
      "iteration: 3/9   loss = 0.7171646356582642   train_acc = 0.5970149253731343   val_acc = 0.7857142857142857\n",
      "iteration: 4/9   loss = 0.7280638217926025   train_acc = 0.6567164179104478   val_acc = 0.6428571428571429\n",
      "iteration: 5/9   loss = 0.6876304149627686   train_acc = 0.6194029850746269   val_acc = 0.5714285714285714\n",
      "iteration: 6/9   loss = 0.6124666929244995   train_acc = 0.6343283582089553   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.6625243425369263   train_acc = 0.6044776119402985   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.68348228931427   train_acc = 0.6268656716417911   val_acc = 0.42857142857142855\n",
      "iteration: 9/9   loss = 0.7772176265716553   train_acc = 0.5970149253731343   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=7.481537196579658e-05, wd=0.00011822143451838108: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=7.481537196579658e-05, wd=0.00011822143451838108, betas=(0.95, 0.98)\n",
      "iteration: 1/9   loss = 0.6903223395347595   train_acc = 0.5597014925373134   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7068568468093872   train_acc = 0.6119402985074627   val_acc = 0.7142857142857143\n",
      "iteration: 3/9   loss = 0.6368497610092163   train_acc = 0.5895522388059702   val_acc = 0.7142857142857143\n",
      "iteration: 4/9   loss = 0.6712042093276978   train_acc = 0.5074626865671642   val_acc = 0.6428571428571429\n",
      "iteration: 5/9   loss = 0.6953909397125244   train_acc = 0.5671641791044776   val_acc = 0.6428571428571429\n",
      "iteration: 6/9   loss = 0.689940333366394   train_acc = 0.6194029850746269   val_acc = 0.7142857142857143\n",
      "iteration: 7/9   loss = 0.6773959398269653   train_acc = 0.6044776119402985   val_acc = 0.7142857142857143\n",
      "iteration: 8/9   loss = 0.6447799205780029   train_acc = 0.5671641791044776   val_acc = 0.6428571428571429\n",
      "iteration: 9/9   loss = 0.6669113039970398   train_acc = 0.5970149253731343   val_acc = 0.6428571428571429\n",
      "Final val acc for lr=7.481537196579658e-05, wd=0.00011822143451838108: 0.6429\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=7.481537196579658e-05, wd=0.00011822143451838108, betas=(0.95, 0.99)\n",
      "iteration: 1/9   loss = 0.7333136200904846   train_acc = 0.582089552238806   val_acc = 0.7142857142857143\n",
      "iteration: 2/9   loss = 0.7140157222747803   train_acc = 0.6044776119402985   val_acc = 0.6428571428571429\n",
      "iteration: 3/9   loss = 0.6465679407119751   train_acc = 0.6119402985074627   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7539793848991394   train_acc = 0.6343283582089553   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.7053855657577515   train_acc = 0.582089552238806   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.893638014793396   train_acc = 0.5597014925373134   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.6792998313903809   train_acc = 0.5970149253731343   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.7309589982032776   train_acc = 0.5895522388059702   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.5556847453117371   train_acc = 0.5895522388059702   val_acc = 0.5\n",
      "Final val acc for lr=7.481537196579658e-05, wd=0.00011822143451838108: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=7.481537196579658e-05, wd=0.00011822143451838108, betas=(0.95, 0.995)\n",
      "iteration: 1/9   loss = 0.6652786731719971   train_acc = 0.5746268656716418   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7155623435974121   train_acc = 0.6119402985074627   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.6585780382156372   train_acc = 0.6492537313432836   val_acc = 0.35714285714285715\n",
      "iteration: 4/9   loss = 0.6524137258529663   train_acc = 0.6268656716417911   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.6492639780044556   train_acc = 0.6567164179104478   val_acc = 0.2857142857142857\n",
      "iteration: 6/9   loss = 0.5700770616531372   train_acc = 0.6417910447761194   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.6262433528900146   train_acc = 0.6492537313432836   val_acc = 0.2857142857142857\n",
      "iteration: 8/9   loss = 0.678637683391571   train_acc = 0.6567164179104478   val_acc = 0.14285714285714285\n",
      "iteration: 9/9   loss = 0.6477518081665039   train_acc = 0.6343283582089553   val_acc = 0.2857142857142857\n",
      "Final val acc for lr=7.481537196579658e-05, wd=0.00011822143451838108: 0.2857\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=7.481537196579658e-05, wd=0.00011822143451838108, betas=(0.95, 0.999)\n",
      "iteration: 1/9   loss = 0.7128204107284546   train_acc = 0.5895522388059702   val_acc = 0.7142857142857143\n",
      "iteration: 2/9   loss = 0.7588369846343994   train_acc = 0.582089552238806   val_acc = 0.5714285714285714\n",
      "iteration: 3/9   loss = 0.7325352430343628   train_acc = 0.6044776119402985   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.6551523208618164   train_acc = 0.6716417910447762   val_acc = 0.5714285714285714\n",
      "iteration: 5/9   loss = 0.7082152366638184   train_acc = 0.6194029850746269   val_acc = 0.7857142857142857\n",
      "iteration: 6/9   loss = 0.7217094302177429   train_acc = 0.6044776119402985   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.6882050037384033   train_acc = 0.6567164179104478   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.6683599948883057   train_acc = 0.6119402985074627   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.5024192333221436   train_acc = 0.5970149253731343   val_acc = 0.5\n",
      "Final val acc for lr=7.481537196579658e-05, wd=0.00011822143451838108: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=7.481537196579658e-05, wd=0.00032736198184168545, betas=(0.85, 0.99)\n",
      "iteration: 1/9   loss = 0.7308087348937988   train_acc = 0.47761194029850745   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7462975978851318   train_acc = 0.5223880597014925   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.6297967433929443   train_acc = 0.5970149253731343   val_acc = 0.21428571428571427\n",
      "iteration: 4/9   loss = 0.6600526571273804   train_acc = 0.6343283582089553   val_acc = 0.2857142857142857\n",
      "iteration: 5/9   loss = 0.6944541931152344   train_acc = 0.6044776119402985   val_acc = 0.2857142857142857\n",
      "iteration: 6/9   loss = 0.8263551592826843   train_acc = 0.5970149253731343   val_acc = 0.35714285714285715\n",
      "iteration: 7/9   loss = 0.6812853813171387   train_acc = 0.6194029850746269   val_acc = 0.35714285714285715\n",
      "iteration: 8/9   loss = 0.6647230386734009   train_acc = 0.6044776119402985   val_acc = 0.35714285714285715\n",
      "iteration: 9/9   loss = 0.6101396083831787   train_acc = 0.5746268656716418   val_acc = 0.35714285714285715\n",
      "Final val acc for lr=7.481537196579658e-05, wd=0.00032736198184168545: 0.3571\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=7.481537196579658e-05, wd=0.00032736198184168545, betas=(0.85, 0.995)\n",
      "iteration: 1/9   loss = 0.7074196338653564   train_acc = 0.5597014925373134   val_acc = 0.35714285714285715\n",
      "iteration: 2/9   loss = 0.6759780049324036   train_acc = 0.6194029850746269   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.6186527013778687   train_acc = 0.6119402985074627   val_acc = 0.2857142857142857\n",
      "iteration: 4/9   loss = 0.6782176494598389   train_acc = 0.5970149253731343   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.6923463344573975   train_acc = 0.6194029850746269   val_acc = 0.35714285714285715\n",
      "iteration: 6/9   loss = 0.6421384811401367   train_acc = 0.6567164179104478   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.6883751749992371   train_acc = 0.664179104477612   val_acc = 0.35714285714285715\n",
      "iteration: 8/9   loss = 0.6316608786582947   train_acc = 0.6343283582089553   val_acc = 0.35714285714285715\n",
      "iteration: 9/9   loss = 0.5487570762634277   train_acc = 0.6567164179104478   val_acc = 0.5714285714285714\n",
      "Final val acc for lr=7.481537196579658e-05, wd=0.00032736198184168545: 0.5714\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=7.481537196579658e-05, wd=0.00032736198184168545, betas=(0.9, 0.98)\n",
      "iteration: 1/9   loss = 0.6900137662887573   train_acc = 0.5373134328358209   val_acc = 0.9285714285714286\n",
      "iteration: 2/9   loss = 0.7519589066505432   train_acc = 0.6194029850746269   val_acc = 0.5714285714285714\n",
      "iteration: 3/9   loss = 0.7164749503135681   train_acc = 0.5746268656716418   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.6564677953720093   train_acc = 0.5597014925373134   val_acc = 0.6428571428571429\n",
      "iteration: 5/9   loss = 0.6731279492378235   train_acc = 0.6343283582089553   val_acc = 0.7857142857142857\n",
      "iteration: 6/9   loss = 0.708734393119812   train_acc = 0.5671641791044776   val_acc = 0.6428571428571429\n",
      "iteration: 7/9   loss = 0.674500584602356   train_acc = 0.6194029850746269   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.6513493657112122   train_acc = 0.6194029850746269   val_acc = 0.42857142857142855\n",
      "iteration: 9/9   loss = 0.7556236982345581   train_acc = 0.664179104477612   val_acc = 0.5714285714285714\n",
      "Final val acc for lr=7.481537196579658e-05, wd=0.00032736198184168545: 0.5714\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=7.481537196579658e-05, wd=0.00032736198184168545, betas=(0.9, 0.99)\n",
      "iteration: 1/9   loss = 0.7169094085693359   train_acc = 0.5522388059701493   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7207316160202026   train_acc = 0.6044776119402985   val_acc = 0.6428571428571429\n",
      "iteration: 3/9   loss = 0.7283759117126465   train_acc = 0.582089552238806   val_acc = 0.6428571428571429\n",
      "iteration: 4/9   loss = 0.6776539087295532   train_acc = 0.5895522388059702   val_acc = 0.5714285714285714\n",
      "iteration: 5/9   loss = 0.6603515148162842   train_acc = 0.582089552238806   val_acc = 0.5714285714285714\n",
      "iteration: 6/9   loss = 0.5138816833496094   train_acc = 0.6119402985074627   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.6748937368392944   train_acc = 0.6343283582089553   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.6573082208633423   train_acc = 0.6194029850746269   val_acc = 0.6428571428571429\n",
      "iteration: 9/9   loss = 0.7342468500137329   train_acc = 0.6343283582089553   val_acc = 0.5714285714285714\n",
      "Final val acc for lr=7.481537196579658e-05, wd=0.00032736198184168545: 0.5714\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=7.481537196579658e-05, wd=0.00032736198184168545, betas=(0.9, 0.995)\n",
      "iteration: 1/9   loss = 0.7218881845474243   train_acc = 0.6044776119402985   val_acc = 0.5714285714285714\n",
      "iteration: 2/9   loss = 0.7524111270904541   train_acc = 0.582089552238806   val_acc = 0.5714285714285714\n",
      "iteration: 3/9   loss = 0.6732019186019897   train_acc = 0.5970149253731343   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.6330492496490479   train_acc = 0.6044776119402985   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.6593652963638306   train_acc = 0.6268656716417911   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.6020454168319702   train_acc = 0.6194029850746269   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.6633390188217163   train_acc = 0.6567164179104478   val_acc = 0.6428571428571429\n",
      "iteration: 8/9   loss = 0.6537408828735352   train_acc = 0.6567164179104478   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.7336970567703247   train_acc = 0.7014925373134329   val_acc = 0.35714285714285715\n",
      "Final val acc for lr=7.481537196579658e-05, wd=0.00032736198184168545: 0.3571\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=7.481537196579658e-05, wd=0.00032736198184168545, betas=(0.9, 0.999)\n",
      "iteration: 1/9   loss = 0.7139955759048462   train_acc = 0.5671641791044776   val_acc = 0.14285714285714285\n",
      "iteration: 2/9   loss = 0.6844359636306763   train_acc = 0.5597014925373134   val_acc = 0.14285714285714285\n",
      "iteration: 3/9   loss = 0.6902573108673096   train_acc = 0.5970149253731343   val_acc = 0.14285714285714285\n",
      "iteration: 4/9   loss = 0.7135657668113708   train_acc = 0.5522388059701493   val_acc = 0.35714285714285715\n",
      "iteration: 5/9   loss = 0.6977033615112305   train_acc = 0.6343283582089553   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.6613067984580994   train_acc = 0.5746268656716418   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.6849496364593506   train_acc = 0.5597014925373134   val_acc = 0.35714285714285715\n",
      "iteration: 8/9   loss = 0.672501266002655   train_acc = 0.6044776119402985   val_acc = 0.2857142857142857\n",
      "iteration: 9/9   loss = 0.7915520668029785   train_acc = 0.582089552238806   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=7.481537196579658e-05, wd=0.00032736198184168545: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=7.481537196579658e-05, wd=0.00032736198184168545, betas=(0.95, 0.98)\n",
      "iteration: 1/9   loss = 0.7023447751998901   train_acc = 0.44776119402985076   val_acc = 0.35714285714285715\n",
      "iteration: 2/9   loss = 0.6699312925338745   train_acc = 0.5746268656716418   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.900953471660614   train_acc = 0.6268656716417911   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.6992104053497314   train_acc = 0.6268656716417911   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.6742427349090576   train_acc = 0.6343283582089553   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.7295724749565125   train_acc = 0.6417910447761194   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.6749505400657654   train_acc = 0.6119402985074627   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.6521838307380676   train_acc = 0.6417910447761194   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.6348414421081543   train_acc = 0.6194029850746269   val_acc = 0.5\n",
      "Final val acc for lr=7.481537196579658e-05, wd=0.00032736198184168545: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=7.481537196579658e-05, wd=0.00032736198184168545, betas=(0.95, 0.99)\n",
      "iteration: 1/9   loss = 0.7143195867538452   train_acc = 0.5671641791044776   val_acc = 0.6428571428571429\n",
      "iteration: 2/9   loss = 0.7115658521652222   train_acc = 0.582089552238806   val_acc = 0.7142857142857143\n",
      "iteration: 3/9   loss = 0.7311525344848633   train_acc = 0.6716417910447762   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.746131181716919   train_acc = 0.6194029850746269   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.6929440498352051   train_acc = 0.6492537313432836   val_acc = 0.5714285714285714\n",
      "iteration: 6/9   loss = 0.6195360422134399   train_acc = 0.6567164179104478   val_acc = 0.6428571428571429\n",
      "iteration: 7/9   loss = 0.6480786800384521   train_acc = 0.6119402985074627   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.658698558807373   train_acc = 0.664179104477612   val_acc = 0.7142857142857143\n",
      "iteration: 9/9   loss = 0.748579204082489   train_acc = 0.664179104477612   val_acc = 0.7142857142857143\n",
      "Final val acc for lr=7.481537196579658e-05, wd=0.00032736198184168545: 0.7143\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=7.481537196579658e-05, wd=0.00032736198184168545, betas=(0.95, 0.995)\n",
      "iteration: 1/9   loss = 0.6885026693344116   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7271062731742859   train_acc = 0.6194029850746269   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.843669056892395   train_acc = 0.5970149253731343   val_acc = 0.6428571428571429\n",
      "iteration: 4/9   loss = 0.714477002620697   train_acc = 0.5746268656716418   val_acc = 0.5714285714285714\n",
      "iteration: 5/9   loss = 0.7098416686058044   train_acc = 0.5522388059701493   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.7549330592155457   train_acc = 0.5522388059701493   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.6271077394485474   train_acc = 0.5671641791044776   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.6926783323287964   train_acc = 0.5597014925373134   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.560765266418457   train_acc = 0.5746268656716418   val_acc = 0.5\n",
      "Final val acc for lr=7.481537196579658e-05, wd=0.00032736198184168545: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=7.481537196579658e-05, wd=0.00032736198184168545, betas=(0.95, 0.999)\n",
      "iteration: 1/9   loss = 0.6703670620918274   train_acc = 0.5373134328358209   val_acc = 0.5714285714285714\n",
      "iteration: 2/9   loss = 0.7421371936798096   train_acc = 0.6417910447761194   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.7137818336486816   train_acc = 0.6417910447761194   val_acc = 0.35714285714285715\n",
      "iteration: 4/9   loss = 0.7370122671127319   train_acc = 0.5970149253731343   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.6908712983131409   train_acc = 0.6119402985074627   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.6063940525054932   train_acc = 0.582089552238806   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7545202970504761   train_acc = 0.5970149253731343   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.6469664573669434   train_acc = 0.5895522388059702   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.589743971824646   train_acc = 0.5895522388059702   val_acc = 0.5\n",
      "Final val acc for lr=7.481537196579658e-05, wd=0.00032736198184168545: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.469108725047993e-06, wd=0.0004609389889268166, betas=(0.85, 0.99)\n",
      "iteration: 1/9   loss = 0.7107014656066895   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7520819306373596   train_acc = 0.5074626865671642   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.6032665967941284   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7527227401733398   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7417083978652954   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.8302987813949585   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7280473709106445   train_acc = 0.4925373134328358   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.7554363012313843   train_acc = 0.5074626865671642   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.6204626560211182   train_acc = 0.4925373134328358   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=6.469108725047993e-06, wd=0.0004609389889268166: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.469108725047993e-06, wd=0.0004609389889268166, betas=(0.85, 0.995)\n",
      "iteration: 1/9   loss = 0.6585729122161865   train_acc = 0.5895522388059702   val_acc = 0.6428571428571429\n",
      "iteration: 2/9   loss = 0.7422587871551514   train_acc = 0.6119402985074627   val_acc = 0.2857142857142857\n",
      "iteration: 3/9   loss = 0.6492533087730408   train_acc = 0.5373134328358209   val_acc = 0.35714285714285715\n",
      "iteration: 4/9   loss = 0.741868257522583   train_acc = 0.5597014925373134   val_acc = 0.21428571428571427\n",
      "iteration: 5/9   loss = 0.7056382298469543   train_acc = 0.5671641791044776   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.6683968305587769   train_acc = 0.5895522388059702   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.6596030592918396   train_acc = 0.5895522388059702   val_acc = 0.35714285714285715\n",
      "iteration: 8/9   loss = 0.710027277469635   train_acc = 0.5895522388059702   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.7850138545036316   train_acc = 0.6044776119402985   val_acc = 0.35714285714285715\n",
      "Final val acc for lr=6.469108725047993e-06, wd=0.0004609389889268166: 0.3571\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.469108725047993e-06, wd=0.0004609389889268166, betas=(0.9, 0.98)\n",
      "iteration: 1/9   loss = 0.7396636009216309   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7064152956008911   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.72809898853302   train_acc = 0.47761194029850745   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7102334499359131   train_acc = 0.4701492537313433   val_acc = 0.5714285714285714\n",
      "iteration: 5/9   loss = 0.7321760654449463   train_acc = 0.47761194029850745   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.6234773397445679   train_acc = 0.47761194029850745   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.7485235929489136   train_acc = 0.48507462686567165   val_acc = 0.6428571428571429\n",
      "iteration: 8/9   loss = 0.7071301341056824   train_acc = 0.5223880597014925   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.737822413444519   train_acc = 0.5074626865671642   val_acc = 0.6428571428571429\n",
      "Final val acc for lr=6.469108725047993e-06, wd=0.0004609389889268166: 0.6429\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.469108725047993e-06, wd=0.0004609389889268166, betas=(0.9, 0.99)\n",
      "iteration: 1/9   loss = 0.6903396844863892   train_acc = 0.4701492537313433   val_acc = 0.35714285714285715\n",
      "iteration: 2/9   loss = 0.6982676982879639   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.9373691082000732   train_acc = 0.5074626865671642   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.7627052068710327   train_acc = 0.48507462686567165   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.7041664123535156   train_acc = 0.47761194029850745   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.7752785086631775   train_acc = 0.5074626865671642   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.7167256474494934   train_acc = 0.582089552238806   val_acc = 0.6428571428571429\n",
      "iteration: 8/9   loss = 0.6953508853912354   train_acc = 0.5522388059701493   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.7636605501174927   train_acc = 0.6044776119402985   val_acc = 0.5\n",
      "Final val acc for lr=6.469108725047993e-06, wd=0.0004609389889268166: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.469108725047993e-06, wd=0.0004609389889268166, betas=(0.9, 0.995)\n",
      "iteration: 1/9   loss = 0.6454117894172668   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7090768814086914   train_acc = 0.5373134328358209   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.8205375075340271   train_acc = 0.48507462686567165   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.7291334271430969   train_acc = 0.5074626865671642   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.7545514106750488   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.6374291181564331   train_acc = 0.5447761194029851   val_acc = 0.6428571428571429\n",
      "iteration: 7/9   loss = 0.7504559755325317   train_acc = 0.4552238805970149   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7352222204208374   train_acc = 0.5373134328358209   val_acc = 0.42857142857142855\n",
      "iteration: 9/9   loss = 0.6556506156921387   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "Final val acc for lr=6.469108725047993e-06, wd=0.0004609389889268166: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.469108725047993e-06, wd=0.0004609389889268166, betas=(0.9, 0.999)\n",
      "iteration: 1/9   loss = 0.6772993803024292   train_acc = 0.5223880597014925   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7231795787811279   train_acc = 0.47761194029850745   val_acc = 0.5714285714285714\n",
      "iteration: 3/9   loss = 0.7738749384880066   train_acc = 0.47761194029850745   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.6601119637489319   train_acc = 0.48507462686567165   val_acc = 0.6428571428571429\n",
      "iteration: 5/9   loss = 0.7160996198654175   train_acc = 0.4925373134328358   val_acc = 0.6428571428571429\n",
      "iteration: 6/9   loss = 0.5443533658981323   train_acc = 0.5223880597014925   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.6708844900131226   train_acc = 0.4925373134328358   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.7406696081161499   train_acc = 0.5373134328358209   val_acc = 0.6428571428571429\n",
      "iteration: 9/9   loss = 0.6117962598800659   train_acc = 0.5223880597014925   val_acc = 0.5\n",
      "Final val acc for lr=6.469108725047993e-06, wd=0.0004609389889268166: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.469108725047993e-06, wd=0.0004609389889268166, betas=(0.95, 0.98)\n",
      "iteration: 1/9   loss = 0.6116805672645569   train_acc = 0.5373134328358209   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.8265548944473267   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.6666676998138428   train_acc = 0.5447761194029851   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7987891435623169   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.712562084197998   train_acc = 0.5223880597014925   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.692827582359314   train_acc = 0.5223880597014925   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7453818917274475   train_acc = 0.5522388059701493   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7278821468353271   train_acc = 0.5298507462686567   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.7599139213562012   train_acc = 0.5223880597014925   val_acc = 0.5\n",
      "Final val acc for lr=6.469108725047993e-06, wd=0.0004609389889268166: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.469108725047993e-06, wd=0.0004609389889268166, betas=(0.95, 0.99)\n",
      "iteration: 1/9   loss = 0.7857115268707275   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7830641269683838   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.7207703590393066   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7102441191673279   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.809529185295105   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.7562559843063354   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.8180622458457947   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7400283217430115   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.956462562084198   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "Final val acc for lr=6.469108725047993e-06, wd=0.0004609389889268166: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.469108725047993e-06, wd=0.0004609389889268166, betas=(0.95, 0.995)\n",
      "iteration: 1/9   loss = 0.7162827253341675   train_acc = 0.5074626865671642   val_acc = 0.5714285714285714\n",
      "iteration: 2/9   loss = 0.7180673480033875   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.8997005224227905   train_acc = 0.5223880597014925   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7215439677238464   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.6868638396263123   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.8476893305778503   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7081623673439026   train_acc = 0.5223880597014925   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7156842350959778   train_acc = 0.5149253731343284   val_acc = 0.42857142857142855\n",
      "iteration: 9/9   loss = 0.5188665986061096   train_acc = 0.5223880597014925   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=6.469108725047993e-06, wd=0.0004609389889268166: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.469108725047993e-06, wd=0.0004609389889268166, betas=(0.95, 0.999)\n",
      "iteration: 1/9   loss = 0.7177935242652893   train_acc = 0.5074626865671642   val_acc = 0.35714285714285715\n",
      "iteration: 2/9   loss = 0.6977646350860596   train_acc = 0.5223880597014925   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.6737620234489441   train_acc = 0.5447761194029851   val_acc = 0.35714285714285715\n",
      "iteration: 4/9   loss = 0.7865725755691528   train_acc = 0.5373134328358209   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.6888346672058105   train_acc = 0.5149253731343284   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.8726456165313721   train_acc = 0.5597014925373134   val_acc = 0.21428571428571427\n",
      "iteration: 7/9   loss = 0.6917673349380493   train_acc = 0.5671641791044776   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.6935182213783264   train_acc = 0.5522388059701493   val_acc = 0.2857142857142857\n",
      "iteration: 9/9   loss = 0.6540160179138184   train_acc = 0.5373134328358209   val_acc = 0.35714285714285715\n",
      "Final val acc for lr=6.469108725047993e-06, wd=0.0004609389889268166: 0.3571\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.469108725047993e-06, wd=0.0002300292096821906, betas=(0.85, 0.99)\n",
      "iteration: 1/9   loss = 0.7399371862411499   train_acc = 0.47761194029850745   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.7627159953117371   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.6256954669952393   train_acc = 0.4925373134328358   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.7158702611923218   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7761075496673584   train_acc = 0.4701492537313433   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.8247857093811035   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7349295020103455   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7249029874801636   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.6608461737632751   train_acc = 0.4701492537313433   val_acc = 0.5\n",
      "Final val acc for lr=6.469108725047993e-06, wd=0.0002300292096821906: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.469108725047993e-06, wd=0.0002300292096821906, betas=(0.85, 0.995)\n",
      "iteration: 1/9   loss = 0.6833983659744263   train_acc = 0.5597014925373134   val_acc = 0.5714285714285714\n",
      "iteration: 2/9   loss = 0.800613522529602   train_acc = 0.4925373134328358   val_acc = 0.5714285714285714\n",
      "iteration: 3/9   loss = 0.7359495162963867   train_acc = 0.5223880597014925   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.7497444152832031   train_acc = 0.5149253731343284   val_acc = 0.5714285714285714\n",
      "iteration: 5/9   loss = 0.7315943837165833   train_acc = 0.5597014925373134   val_acc = 0.6428571428571429\n",
      "iteration: 6/9   loss = 0.7889394760131836   train_acc = 0.5597014925373134   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.7049803733825684   train_acc = 0.5447761194029851   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.7148786783218384   train_acc = 0.5447761194029851   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.6859240531921387   train_acc = 0.5522388059701493   val_acc = 0.5\n",
      "Final val acc for lr=6.469108725047993e-06, wd=0.0002300292096821906: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.469108725047993e-06, wd=0.0002300292096821906, betas=(0.9, 0.98)\n",
      "iteration: 1/9   loss = 0.7349037528038025   train_acc = 0.5   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.6916400194168091   train_acc = 0.582089552238806   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.690224826335907   train_acc = 0.5373134328358209   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.7131120562553406   train_acc = 0.5447761194029851   val_acc = 0.35714285714285715\n",
      "iteration: 5/9   loss = 0.7017848491668701   train_acc = 0.5447761194029851   val_acc = 0.35714285714285715\n",
      "iteration: 6/9   loss = 0.608093798160553   train_acc = 0.5597014925373134   val_acc = 0.35714285714285715\n",
      "iteration: 7/9   loss = 0.6883307695388794   train_acc = 0.5447761194029851   val_acc = 0.6428571428571429\n",
      "iteration: 8/9   loss = 0.7116280794143677   train_acc = 0.5447761194029851   val_acc = 0.35714285714285715\n",
      "iteration: 9/9   loss = 0.7871209383010864   train_acc = 0.44776119402985076   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=6.469108725047993e-06, wd=0.0002300292096821906: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.469108725047993e-06, wd=0.0002300292096821906, betas=(0.9, 0.99)\n",
      "iteration: 1/9   loss = 0.6482717990875244   train_acc = 0.5149253731343284   val_acc = 0.5714285714285714\n",
      "iteration: 2/9   loss = 0.7414834499359131   train_acc = 0.5746268656716418   val_acc = 0.5714285714285714\n",
      "iteration: 3/9   loss = 0.6547496914863586   train_acc = 0.5671641791044776   val_acc = 0.6428571428571429\n",
      "iteration: 4/9   loss = 0.7329807281494141   train_acc = 0.6044776119402985   val_acc = 0.6428571428571429\n",
      "iteration: 5/9   loss = 0.7119402885437012   train_acc = 0.6194029850746269   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.8980284929275513   train_acc = 0.5298507462686567   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.6971243023872375   train_acc = 0.5447761194029851   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.6861838698387146   train_acc = 0.582089552238806   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.7310004234313965   train_acc = 0.5746268656716418   val_acc = 0.5\n",
      "Final val acc for lr=6.469108725047993e-06, wd=0.0002300292096821906: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.469108725047993e-06, wd=0.0002300292096821906, betas=(0.9, 0.995)\n",
      "iteration: 1/9   loss = 0.7007912397384644   train_acc = 0.4552238805970149   val_acc = 0.5714285714285714\n",
      "iteration: 2/9   loss = 0.7100151777267456   train_acc = 0.5373134328358209   val_acc = 0.6428571428571429\n",
      "iteration: 3/9   loss = 0.741106390953064   train_acc = 0.5149253731343284   val_acc = 0.6428571428571429\n",
      "iteration: 4/9   loss = 0.7013592720031738   train_acc = 0.5223880597014925   val_acc = 0.7142857142857143\n",
      "iteration: 5/9   loss = 0.7223970890045166   train_acc = 0.5074626865671642   val_acc = 0.7142857142857143\n",
      "iteration: 6/9   loss = 0.8166298270225525   train_acc = 0.5373134328358209   val_acc = 0.6428571428571429\n",
      "iteration: 7/9   loss = 0.6854763031005859   train_acc = 0.5597014925373134   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.7119084596633911   train_acc = 0.5671641791044776   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.7432624697685242   train_acc = 0.5671641791044776   val_acc = 0.5\n",
      "Final val acc for lr=6.469108725047993e-06, wd=0.0002300292096821906: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.469108725047993e-06, wd=0.0002300292096821906, betas=(0.9, 0.999)\n",
      "iteration: 1/9   loss = 0.7452102899551392   train_acc = 0.44776119402985076   val_acc = 0.6428571428571429\n",
      "iteration: 2/9   loss = 0.7023870348930359   train_acc = 0.44776119402985076   val_acc = 0.6428571428571429\n",
      "iteration: 3/9   loss = 0.8731472492218018   train_acc = 0.5   val_acc = 0.6428571428571429\n",
      "iteration: 4/9   loss = 0.7408393621444702   train_acc = 0.4552238805970149   val_acc = 0.6428571428571429\n",
      "iteration: 5/9   loss = 0.7034485936164856   train_acc = 0.5074626865671642   val_acc = 0.6428571428571429\n",
      "iteration: 6/9   loss = 0.8640956878662109   train_acc = 0.417910447761194   val_acc = 0.6428571428571429\n",
      "iteration: 7/9   loss = 0.670648455619812   train_acc = 0.48507462686567165   val_acc = 0.6428571428571429\n",
      "iteration: 8/9   loss = 0.7583400011062622   train_acc = 0.4701492537313433   val_acc = 0.7142857142857143\n",
      "iteration: 9/9   loss = 0.8181625604629517   train_acc = 0.4925373134328358   val_acc = 0.6428571428571429\n",
      "Final val acc for lr=6.469108725047993e-06, wd=0.0002300292096821906: 0.6429\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.469108725047993e-06, wd=0.0002300292096821906, betas=(0.95, 0.98)\n",
      "iteration: 1/9   loss = 0.7221067547798157   train_acc = 0.5074626865671642   val_acc = 0.35714285714285715\n",
      "iteration: 2/9   loss = 0.7145195603370667   train_acc = 0.5298507462686567   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.7682157754898071   train_acc = 0.5223880597014925   val_acc = 0.2857142857142857\n",
      "iteration: 4/9   loss = 0.6800737380981445   train_acc = 0.6044776119402985   val_acc = 0.35714285714285715\n",
      "iteration: 5/9   loss = 0.7188735604286194   train_acc = 0.5895522388059702   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.7202765345573425   train_acc = 0.5895522388059702   val_acc = 0.21428571428571427\n",
      "iteration: 7/9   loss = 0.6707071661949158   train_acc = 0.5895522388059702   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.6893415451049805   train_acc = 0.6044776119402985   val_acc = 0.35714285714285715\n",
      "iteration: 9/9   loss = 0.8367688059806824   train_acc = 0.6044776119402985   val_acc = 0.35714285714285715\n",
      "Final val acc for lr=6.469108725047993e-06, wd=0.0002300292096821906: 0.3571\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.469108725047993e-06, wd=0.0002300292096821906, betas=(0.95, 0.99)\n",
      "iteration: 1/9   loss = 0.7510123252868652   train_acc = 0.5298507462686567   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7282730340957642   train_acc = 0.47761194029850745   val_acc = 0.5714285714285714\n",
      "iteration: 3/9   loss = 0.7143932580947876   train_acc = 0.5   val_acc = 0.7857142857142857\n",
      "iteration: 4/9   loss = 0.7246454954147339   train_acc = 0.48507462686567165   val_acc = 0.7142857142857143\n",
      "iteration: 5/9   loss = 0.6930786371231079   train_acc = 0.5373134328358209   val_acc = 0.7142857142857143\n",
      "iteration: 6/9   loss = 0.7676366567611694   train_acc = 0.5074626865671642   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.7527673244476318   train_acc = 0.5   val_acc = 0.6428571428571429\n",
      "iteration: 8/9   loss = 0.7647728323936462   train_acc = 0.5373134328358209   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.7402064204216003   train_acc = 0.5373134328358209   val_acc = 0.5\n",
      "Final val acc for lr=6.469108725047993e-06, wd=0.0002300292096821906: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.469108725047993e-06, wd=0.0002300292096821906, betas=(0.95, 0.995)\n",
      "iteration: 1/9   loss = 0.7239084839820862   train_acc = 0.5149253731343284   val_acc = 0.6428571428571429\n",
      "iteration: 2/9   loss = 0.7144986987113953   train_acc = 0.5298507462686567   val_acc = 0.6428571428571429\n",
      "iteration: 3/9   loss = 0.7256696224212646   train_acc = 0.5373134328358209   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.7097026109695435   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7382467985153198   train_acc = 0.4925373134328358   val_acc = 0.6428571428571429\n",
      "iteration: 6/9   loss = 0.685029923915863   train_acc = 0.5447761194029851   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.7050777673721313   train_acc = 0.5149253731343284   val_acc = 0.6428571428571429\n",
      "iteration: 8/9   loss = 0.717181921005249   train_acc = 0.5298507462686567   val_acc = 0.6428571428571429\n",
      "iteration: 9/9   loss = 0.720913827419281   train_acc = 0.5149253731343284   val_acc = 0.5714285714285714\n",
      "Final val acc for lr=6.469108725047993e-06, wd=0.0002300292096821906: 0.5714\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.469108725047993e-06, wd=0.0002300292096821906, betas=(0.95, 0.999)\n",
      "iteration: 1/9   loss = 0.686343789100647   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7252421975135803   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.7757391929626465   train_acc = 0.48507462686567165   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.749613881111145   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.717825174331665   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.7889954447746277   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7080835103988647   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7089189291000366   train_acc = 0.4925373134328358   val_acc = 0.42857142857142855\n",
      "iteration: 9/9   loss = 0.5944983959197998   train_acc = 0.5373134328358209   val_acc = 0.5\n",
      "Final val acc for lr=6.469108725047993e-06, wd=0.0002300292096821906: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.469108725047993e-06, wd=0.00022555743977178082, betas=(0.85, 0.99)\n",
      "iteration: 1/9   loss = 0.7502610683441162   train_acc = 0.4925373134328358   val_acc = 0.5714285714285714\n",
      "iteration: 2/9   loss = 0.7262215614318848   train_acc = 0.43283582089552236   val_acc = 0.5714285714285714\n",
      "iteration: 3/9   loss = 0.7808241844177246   train_acc = 0.47761194029850745   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.7162654399871826   train_acc = 0.5074626865671642   val_acc = 0.5714285714285714\n",
      "iteration: 5/9   loss = 0.7215010523796082   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.7777239680290222   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7286686897277832   train_acc = 0.47761194029850745   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.718176007270813   train_acc = 0.4925373134328358   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.6659950613975525   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "Final val acc for lr=6.469108725047993e-06, wd=0.00022555743977178082: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.469108725047993e-06, wd=0.00022555743977178082, betas=(0.85, 0.995)\n",
      "iteration: 1/9   loss = 0.7639540433883667   train_acc = 0.44776119402985076   val_acc = 0.7142857142857143\n",
      "iteration: 2/9   loss = 0.7156423330307007   train_acc = 0.47761194029850745   val_acc = 0.8571428571428571\n",
      "iteration: 3/9   loss = 0.8349816203117371   train_acc = 0.4626865671641791   val_acc = 0.7857142857142857\n",
      "iteration: 4/9   loss = 0.697967529296875   train_acc = 0.4552238805970149   val_acc = 0.6428571428571429\n",
      "iteration: 5/9   loss = 0.7152125835418701   train_acc = 0.4626865671641791   val_acc = 0.7857142857142857\n",
      "iteration: 6/9   loss = 0.7702394723892212   train_acc = 0.4552238805970149   val_acc = 0.7857142857142857\n",
      "iteration: 7/9   loss = 0.7338057160377502   train_acc = 0.4626865671641791   val_acc = 0.7142857142857143\n",
      "iteration: 8/9   loss = 0.7178976535797119   train_acc = 0.5074626865671642   val_acc = 0.7857142857142857\n",
      "iteration: 9/9   loss = 0.8308500051498413   train_acc = 0.44029850746268656   val_acc = 0.7142857142857143\n",
      "Final val acc for lr=6.469108725047993e-06, wd=0.00022555743977178082: 0.7143\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.469108725047993e-06, wd=0.00022555743977178082, betas=(0.9, 0.98)\n",
      "iteration: 1/9   loss = 0.7358137965202332   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7730964422225952   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.6477421522140503   train_acc = 0.5223880597014925   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7771031856536865   train_acc = 0.47761194029850745   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7112658619880676   train_acc = 0.4701492537313433   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.6996917724609375   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7250609397888184   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7050799131393433   train_acc = 0.5373134328358209   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.6072574257850647   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "Final val acc for lr=6.469108725047993e-06, wd=0.00022555743977178082: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.469108725047993e-06, wd=0.00022555743977178082, betas=(0.9, 0.99)\n",
      "iteration: 1/9   loss = 0.7948927879333496   train_acc = 0.5746268656716418   val_acc = 0.7857142857142857\n",
      "iteration: 2/9   loss = 0.6911685466766357   train_acc = 0.5223880597014925   val_acc = 0.5714285714285714\n",
      "iteration: 3/9   loss = 0.6418964862823486   train_acc = 0.5597014925373134   val_acc = 0.7142857142857143\n",
      "iteration: 4/9   loss = 0.7546138167381287   train_acc = 0.5223880597014925   val_acc = 0.7142857142857143\n",
      "iteration: 5/9   loss = 0.7502585649490356   train_acc = 0.5447761194029851   val_acc = 0.5714285714285714\n",
      "iteration: 6/9   loss = 0.6272661685943604   train_acc = 0.582089552238806   val_acc = 0.7857142857142857\n",
      "iteration: 7/9   loss = 0.7259449362754822   train_acc = 0.4925373134328358   val_acc = 0.6428571428571429\n",
      "iteration: 8/9   loss = 0.724403440952301   train_acc = 0.5522388059701493   val_acc = 0.6428571428571429\n",
      "iteration: 9/9   loss = 0.9517784118652344   train_acc = 0.5746268656716418   val_acc = 0.35714285714285715\n",
      "Final val acc for lr=6.469108725047993e-06, wd=0.00022555743977178082: 0.3571\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.469108725047993e-06, wd=0.00022555743977178082, betas=(0.9, 0.995)\n",
      "iteration: 1/9   loss = 0.674548864364624   train_acc = 0.5223880597014925   val_acc = 0.14285714285714285\n",
      "iteration: 2/9   loss = 0.7126415967941284   train_acc = 0.5597014925373134   val_acc = 0.14285714285714285\n",
      "iteration: 3/9   loss = 0.746879518032074   train_acc = 0.47761194029850745   val_acc = 0.2857142857142857\n",
      "iteration: 4/9   loss = 0.7442513704299927   train_acc = 0.5597014925373134   val_acc = 0.21428571428571427\n",
      "iteration: 5/9   loss = 0.7061549425125122   train_acc = 0.5522388059701493   val_acc = 0.2857142857142857\n",
      "iteration: 6/9   loss = 0.7544364929199219   train_acc = 0.6119402985074627   val_acc = 0.14285714285714285\n",
      "iteration: 7/9   loss = 0.6938943266868591   train_acc = 0.5671641791044776   val_acc = 0.14285714285714285\n",
      "iteration: 8/9   loss = 0.742790162563324   train_acc = 0.5671641791044776   val_acc = 0.14285714285714285\n",
      "iteration: 9/9   loss = 0.6947506666183472   train_acc = 0.6044776119402985   val_acc = 0.14285714285714285\n",
      "Final val acc for lr=6.469108725047993e-06, wd=0.00022555743977178082: 0.1429\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.469108725047993e-06, wd=0.00022555743977178082, betas=(0.9, 0.999)\n",
      "iteration: 1/9   loss = 0.7193182706832886   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7381635904312134   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.6770563125610352   train_acc = 0.4552238805970149   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7108162641525269   train_acc = 0.5   val_acc = 0.35714285714285715\n",
      "iteration: 5/9   loss = 0.7366555333137512   train_acc = 0.4925373134328358   val_acc = 0.5714285714285714\n",
      "iteration: 6/9   loss = 0.7293194532394409   train_acc = 0.5373134328358209   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7981007099151611   train_acc = 0.5074626865671642   val_acc = 0.21428571428571427\n",
      "iteration: 8/9   loss = 0.7318069338798523   train_acc = 0.5373134328358209   val_acc = 0.35714285714285715\n",
      "iteration: 9/9   loss = 0.5801368951797485   train_acc = 0.5223880597014925   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=6.469108725047993e-06, wd=0.00022555743977178082: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.469108725047993e-06, wd=0.00022555743977178082, betas=(0.95, 0.98)\n",
      "iteration: 1/9   loss = 0.680439829826355   train_acc = 0.5671641791044776   val_acc = 0.21428571428571427\n",
      "iteration: 2/9   loss = 0.7499121427536011   train_acc = 0.5597014925373134   val_acc = 0.2857142857142857\n",
      "iteration: 3/9   loss = 0.6006835699081421   train_acc = 0.5671641791044776   val_acc = 0.2857142857142857\n",
      "iteration: 4/9   loss = 0.7240599393844604   train_acc = 0.6044776119402985   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.7060917615890503   train_acc = 0.6119402985074627   val_acc = 0.35714285714285715\n",
      "iteration: 6/9   loss = 0.6248375773429871   train_acc = 0.5298507462686567   val_acc = 0.35714285714285715\n",
      "iteration: 7/9   loss = 0.7399982213973999   train_acc = 0.582089552238806   val_acc = 0.35714285714285715\n",
      "iteration: 8/9   loss = 0.7228029370307922   train_acc = 0.6044776119402985   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.777942419052124   train_acc = 0.5671641791044776   val_acc = 0.35714285714285715\n",
      "Final val acc for lr=6.469108725047993e-06, wd=0.00022555743977178082: 0.3571\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.469108725047993e-06, wd=0.00022555743977178082, betas=(0.95, 0.99)\n",
      "iteration: 1/9   loss = 0.7274388074874878   train_acc = 0.44776119402985076   val_acc = 0.35714285714285715\n",
      "iteration: 2/9   loss = 0.7310167551040649   train_acc = 0.5223880597014925   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.7247971296310425   train_acc = 0.5149253731343284   val_acc = 0.6428571428571429\n",
      "iteration: 4/9   loss = 0.7160708904266357   train_acc = 0.5373134328358209   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7204990386962891   train_acc = 0.5447761194029851   val_acc = 0.35714285714285715\n",
      "iteration: 6/9   loss = 0.6903144717216492   train_acc = 0.4253731343283582   val_acc = 0.6428571428571429\n",
      "iteration: 7/9   loss = 0.7333620190620422   train_acc = 0.5223880597014925   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7272477149963379   train_acc = 0.5074626865671642   val_acc = 0.35714285714285715\n",
      "iteration: 9/9   loss = 0.6153098940849304   train_acc = 0.5   val_acc = 0.6428571428571429\n",
      "Final val acc for lr=6.469108725047993e-06, wd=0.00022555743977178082: 0.6429\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.469108725047993e-06, wd=0.00022555743977178082, betas=(0.95, 0.995)\n",
      "iteration: 1/9   loss = 0.6562960147857666   train_acc = 0.44029850746268656   val_acc = 0.6428571428571429\n",
      "iteration: 2/9   loss = 0.779214084148407   train_acc = 0.4552238805970149   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.8642891645431519   train_acc = 0.4701492537313433   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.7497413158416748   train_acc = 0.43283582089552236   val_acc = 0.5714285714285714\n",
      "iteration: 5/9   loss = 0.7402337789535522   train_acc = 0.3656716417910448   val_acc = 0.6428571428571429\n",
      "iteration: 6/9   loss = 0.6830636262893677   train_acc = 0.44776119402985076   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7080479860305786   train_acc = 0.5223880597014925   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.7551020383834839   train_acc = 0.4701492537313433   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.8583888411521912   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "Final val acc for lr=6.469108725047993e-06, wd=0.00022555743977178082: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.469108725047993e-06, wd=0.00022555743977178082, betas=(0.95, 0.999)\n",
      "iteration: 1/9   loss = 0.6695438623428345   train_acc = 0.5074626865671642   val_acc = 0.35714285714285715\n",
      "iteration: 2/9   loss = 0.723213791847229   train_acc = 0.47761194029850745   val_acc = 0.21428571428571427\n",
      "iteration: 3/9   loss = 0.8196038007736206   train_acc = 0.5671641791044776   val_acc = 0.35714285714285715\n",
      "iteration: 4/9   loss = 0.7269419431686401   train_acc = 0.5149253731343284   val_acc = 0.21428571428571427\n",
      "iteration: 5/9   loss = 0.710216760635376   train_acc = 0.5   val_acc = 0.35714285714285715\n",
      "iteration: 6/9   loss = 0.6001344919204712   train_acc = 0.5074626865671642   val_acc = 0.35714285714285715\n",
      "iteration: 7/9   loss = 0.7007430791854858   train_acc = 0.5597014925373134   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.7109840512275696   train_acc = 0.5223880597014925   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.7577700614929199   train_acc = 0.5447761194029851   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=6.469108725047993e-06, wd=0.00022555743977178082: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.469108725047993e-06, wd=0.00011822143451838108, betas=(0.85, 0.99)\n",
      "iteration: 1/9   loss = 0.7418006658554077   train_acc = 0.44776119402985076   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7140870094299316   train_acc = 0.4253731343283582   val_acc = 0.35714285714285715\n",
      "iteration: 3/9   loss = 0.8399118185043335   train_acc = 0.4253731343283582   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.7662936449050903   train_acc = 0.4701492537313433   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.7274702191352844   train_acc = 0.4552238805970149   val_acc = 0.35714285714285715\n",
      "iteration: 6/9   loss = 0.7782747745513916   train_acc = 0.47761194029850745   val_acc = 0.2857142857142857\n",
      "iteration: 7/9   loss = 0.6839004755020142   train_acc = 0.44776119402985076   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7433530688285828   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.8043953776359558   train_acc = 0.5447761194029851   val_acc = 0.14285714285714285\n",
      "Final val acc for lr=6.469108725047993e-06, wd=0.00011822143451838108: 0.1429\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.469108725047993e-06, wd=0.00011822143451838108, betas=(0.85, 0.995)\n",
      "iteration: 1/9   loss = 0.7496773600578308   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7067437171936035   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.6935020685195923   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7814123630523682   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.6815328598022461   train_acc = 0.4925373134328358   val_acc = 0.5714285714285714\n",
      "iteration: 6/9   loss = 0.9419386386871338   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.6922967433929443   train_acc = 0.5   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.7675788998603821   train_acc = 0.5373134328358209   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.4638879895210266   train_acc = 0.5298507462686567   val_acc = 0.5\n",
      "Final val acc for lr=6.469108725047993e-06, wd=0.00011822143451838108: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.469108725047993e-06, wd=0.00011822143451838108, betas=(0.9, 0.98)\n",
      "iteration: 1/9   loss = 0.7124207615852356   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.6936663389205933   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.6743563413619995   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7966601848602295   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.6921954154968262   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.5560259819030762   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7434735298156738   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7455770969390869   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.6056161522865295   train_acc = 0.5   val_acc = 0.5\n",
      "Final val acc for lr=6.469108725047993e-06, wd=0.00011822143451838108: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.469108725047993e-06, wd=0.00011822143451838108, betas=(0.9, 0.99)\n",
      "iteration: 1/9   loss = 0.7257148027420044   train_acc = 0.48507462686567165   val_acc = 0.6428571428571429\n",
      "iteration: 2/9   loss = 0.7443240880966187   train_acc = 0.43283582089552236   val_acc = 0.6428571428571429\n",
      "iteration: 3/9   loss = 0.6649857759475708   train_acc = 0.4552238805970149   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.694344162940979   train_acc = 0.43283582089552236   val_acc = 0.7142857142857143\n",
      "iteration: 5/9   loss = 0.7598427534103394   train_acc = 0.48507462686567165   val_acc = 0.5714285714285714\n",
      "iteration: 6/9   loss = 0.7047280669212341   train_acc = 0.5223880597014925   val_acc = 0.6428571428571429\n",
      "iteration: 7/9   loss = 0.719460129737854   train_acc = 0.48507462686567165   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.7310203313827515   train_acc = 0.5074626865671642   val_acc = 0.7142857142857143\n",
      "iteration: 9/9   loss = 0.5751487612724304   train_acc = 0.5522388059701493   val_acc = 0.5714285714285714\n",
      "Final val acc for lr=6.469108725047993e-06, wd=0.00011822143451838108: 0.5714\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.469108725047993e-06, wd=0.00011822143451838108, betas=(0.9, 0.995)\n",
      "iteration: 1/9   loss = 0.7517932653427124   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7971892356872559   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.6778932809829712   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.8010951280593872   train_acc = 0.5223880597014925   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7640469670295715   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.7507081031799316   train_acc = 0.5223880597014925   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.7316734194755554   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7616932988166809   train_acc = 0.5074626865671642   val_acc = 0.42857142857142855\n",
      "iteration: 9/9   loss = 0.9153043627738953   train_acc = 0.5223880597014925   val_acc = 0.5\n",
      "Final val acc for lr=6.469108725047993e-06, wd=0.00011822143451838108: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.469108725047993e-06, wd=0.00011822143451838108, betas=(0.9, 0.999)\n",
      "iteration: 1/9   loss = 0.6913963556289673   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7235993146896362   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.7489187717437744   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.6621052026748657   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7488226294517517   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.8127438426017761   train_acc = 0.5149253731343284   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.7372068166732788   train_acc = 0.5149253731343284   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.6990622282028198   train_acc = 0.5074626865671642   val_acc = 0.6428571428571429\n",
      "iteration: 9/9   loss = 0.8102197647094727   train_acc = 0.5   val_acc = 0.6428571428571429\n",
      "Final val acc for lr=6.469108725047993e-06, wd=0.00011822143451838108: 0.6429\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.469108725047993e-06, wd=0.00011822143451838108, betas=(0.95, 0.98)\n",
      "iteration: 1/9   loss = 0.7612080574035645   train_acc = 0.4701492537313433   val_acc = 0.5714285714285714\n",
      "iteration: 2/9   loss = 0.7553004026412964   train_acc = 0.4701492537313433   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.5795206427574158   train_acc = 0.47761194029850745   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.7013667225837708   train_acc = 0.4701492537313433   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7660820484161377   train_acc = 0.47761194029850745   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.7046722173690796   train_acc = 0.4701492537313433   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.7004668116569519   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7374542951583862   train_acc = 0.47761194029850745   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.5954207181930542   train_acc = 0.4626865671641791   val_acc = 0.6428571428571429\n",
      "Final val acc for lr=6.469108725047993e-06, wd=0.00011822143451838108: 0.6429\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.469108725047993e-06, wd=0.00011822143451838108, betas=(0.95, 0.99)\n",
      "iteration: 1/9   loss = 0.7163642644882202   train_acc = 0.5671641791044776   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.7184228897094727   train_acc = 0.5149253731343284   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.7957978248596191   train_acc = 0.5522388059701493   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.7304713129997253   train_acc = 0.5671641791044776   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.6453292965888977   train_acc = 0.5223880597014925   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.7892504930496216   train_acc = 0.5746268656716418   val_acc = 0.35714285714285715\n",
      "iteration: 7/9   loss = 0.709196925163269   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.6843293905258179   train_acc = 0.5522388059701493   val_acc = 0.35714285714285715\n",
      "iteration: 9/9   loss = 0.631486713886261   train_acc = 0.5447761194029851   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=6.469108725047993e-06, wd=0.00011822143451838108: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.469108725047993e-06, wd=0.00011822143451838108, betas=(0.95, 0.995)\n",
      "iteration: 1/9   loss = 0.6906085014343262   train_acc = 0.5373134328358209   val_acc = 0.5714285714285714\n",
      "iteration: 2/9   loss = 0.7135868668556213   train_acc = 0.5522388059701493   val_acc = 0.5714285714285714\n",
      "iteration: 3/9   loss = 0.7619285583496094   train_acc = 0.5746268656716418   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.7061606049537659   train_acc = 0.5522388059701493   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7001490592956543   train_acc = 0.582089552238806   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.7514300346374512   train_acc = 0.5746268656716418   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.7264285683631897   train_acc = 0.5746268656716418   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7205877900123596   train_acc = 0.5671641791044776   val_acc = 0.42857142857142855\n",
      "iteration: 9/9   loss = 0.8008057475090027   train_acc = 0.5746268656716418   val_acc = 0.5714285714285714\n",
      "Final val acc for lr=6.469108725047993e-06, wd=0.00011822143451838108: 0.5714\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.469108725047993e-06, wd=0.00011822143451838108, betas=(0.95, 0.999)\n",
      "iteration: 1/9   loss = 0.7188461422920227   train_acc = 0.4253731343283582   val_acc = 0.6428571428571429\n",
      "iteration: 2/9   loss = 0.7495913505554199   train_acc = 0.417910447761194   val_acc = 0.5714285714285714\n",
      "iteration: 3/9   loss = 0.7004995346069336   train_acc = 0.4253731343283582   val_acc = 0.7142857142857143\n",
      "iteration: 4/9   loss = 0.7347285747528076   train_acc = 0.417910447761194   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7263574600219727   train_acc = 0.4253731343283582   val_acc = 0.6428571428571429\n",
      "iteration: 6/9   loss = 0.7054208517074585   train_acc = 0.4253731343283582   val_acc = 0.6428571428571429\n",
      "iteration: 7/9   loss = 0.7339576482772827   train_acc = 0.43283582089552236   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.7229624390602112   train_acc = 0.44029850746268656   val_acc = 0.6428571428571429\n",
      "iteration: 9/9   loss = 0.8167084455490112   train_acc = 0.40298507462686567   val_acc = 0.5714285714285714\n",
      "Final val acc for lr=6.469108725047993e-06, wd=0.00011822143451838108: 0.5714\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.469108725047993e-06, wd=0.00032736198184168545, betas=(0.85, 0.99)\n",
      "iteration: 1/9   loss = 0.7914815545082092   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7186907529830933   train_acc = 0.47761194029850745   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.7972836494445801   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.6982966661453247   train_acc = 0.47761194029850745   val_acc = 0.5714285714285714\n",
      "iteration: 5/9   loss = 0.7280681133270264   train_acc = 0.4925373134328358   val_acc = 0.5714285714285714\n",
      "iteration: 6/9   loss = 0.720144510269165   train_acc = 0.5223880597014925   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.7343971729278564   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.728830099105835   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.6664484739303589   train_acc = 0.5074626865671642   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=6.469108725047993e-06, wd=0.00032736198184168545: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.469108725047993e-06, wd=0.00032736198184168545, betas=(0.85, 0.995)\n",
      "iteration: 1/9   loss = 0.7374159097671509   train_acc = 0.4552238805970149   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7183014154434204   train_acc = 0.44776119402985076   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.6908192038536072   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7143887281417847   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.6744358539581299   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.7043511867523193   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.6752107739448547   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7616928219795227   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.6852967143058777   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "Final val acc for lr=6.469108725047993e-06, wd=0.00032736198184168545: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.469108725047993e-06, wd=0.00032736198184168545, betas=(0.9, 0.98)\n",
      "iteration: 1/9   loss = 0.7590142488479614   train_acc = 0.4626865671641791   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.7119606137275696   train_acc = 0.5298507462686567   val_acc = 0.35714285714285715\n",
      "iteration: 3/9   loss = 0.7033385038375854   train_acc = 0.4925373134328358   val_acc = 0.35714285714285715\n",
      "iteration: 4/9   loss = 0.7005397081375122   train_acc = 0.47761194029850745   val_acc = 0.21428571428571427\n",
      "iteration: 5/9   loss = 0.6756526231765747   train_acc = 0.5373134328358209   val_acc = 0.35714285714285715\n",
      "iteration: 6/9   loss = 0.7426137328147888   train_acc = 0.5298507462686567   val_acc = 0.21428571428571427\n",
      "iteration: 7/9   loss = 0.7041047811508179   train_acc = 0.5522388059701493   val_acc = 0.35714285714285715\n",
      "iteration: 8/9   loss = 0.7253089547157288   train_acc = 0.5373134328358209   val_acc = 0.35714285714285715\n",
      "iteration: 9/9   loss = 0.704669177532196   train_acc = 0.5522388059701493   val_acc = 0.2857142857142857\n",
      "Final val acc for lr=6.469108725047993e-06, wd=0.00032736198184168545: 0.2857\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.469108725047993e-06, wd=0.00032736198184168545, betas=(0.9, 0.99)\n",
      "iteration: 1/9   loss = 0.7549105286598206   train_acc = 0.4552238805970149   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.7009153962135315   train_acc = 0.5149253731343284   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.6915864944458008   train_acc = 0.48507462686567165   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.7723840475082397   train_acc = 0.4925373134328358   val_acc = 0.5714285714285714\n",
      "iteration: 5/9   loss = 0.7329267263412476   train_acc = 0.47761194029850745   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.7623070478439331   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7069587707519531   train_acc = 0.4626865671641791   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.7403672933578491   train_acc = 0.5074626865671642   val_acc = 0.35714285714285715\n",
      "iteration: 9/9   loss = 0.6477319002151489   train_acc = 0.5074626865671642   val_acc = 0.5714285714285714\n",
      "Final val acc for lr=6.469108725047993e-06, wd=0.00032736198184168545: 0.5714\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.469108725047993e-06, wd=0.00032736198184168545, betas=(0.9, 0.995)\n",
      "iteration: 1/9   loss = 0.7087701559066772   train_acc = 0.4552238805970149   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.784323513507843   train_acc = 0.4552238805970149   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.7402552366256714   train_acc = 0.4253731343283582   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.7342771291732788   train_acc = 0.48507462686567165   val_acc = 0.5714285714285714\n",
      "iteration: 5/9   loss = 0.7634559869766235   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.7135269045829773   train_acc = 0.41044776119402987   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7369910478591919   train_acc = 0.4552238805970149   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.6942092776298523   train_acc = 0.4701492537313433   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.8790439963340759   train_acc = 0.44776119402985076   val_acc = 0.5\n",
      "Final val acc for lr=6.469108725047993e-06, wd=0.00032736198184168545: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.469108725047993e-06, wd=0.00032736198184168545, betas=(0.9, 0.999)\n",
      "iteration: 1/9   loss = 0.7593899965286255   train_acc = 0.48507462686567165   val_acc = 0.6428571428571429\n",
      "iteration: 2/9   loss = 0.743323564529419   train_acc = 0.4925373134328358   val_acc = 0.6428571428571429\n",
      "iteration: 3/9   loss = 0.7667967081069946   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7641556262969971   train_acc = 0.5149253731343284   val_acc = 0.5714285714285714\n",
      "iteration: 5/9   loss = 0.7073574066162109   train_acc = 0.4701492537313433   val_acc = 0.6428571428571429\n",
      "iteration: 6/9   loss = 0.5520042181015015   train_acc = 0.47761194029850745   val_acc = 0.7142857142857143\n",
      "iteration: 7/9   loss = 0.7727791666984558   train_acc = 0.5149253731343284   val_acc = 0.7142857142857143\n",
      "iteration: 8/9   loss = 0.6816442608833313   train_acc = 0.5298507462686567   val_acc = 0.7857142857142857\n",
      "iteration: 9/9   loss = 0.6682916879653931   train_acc = 0.48507462686567165   val_acc = 0.5714285714285714\n",
      "Final val acc for lr=6.469108725047993e-06, wd=0.00032736198184168545: 0.5714\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.469108725047993e-06, wd=0.00032736198184168545, betas=(0.95, 0.98)\n",
      "iteration: 1/9   loss = 0.7519740462303162   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7925548553466797   train_acc = 0.5   val_acc = 0.5714285714285714\n",
      "iteration: 3/9   loss = 0.6541009545326233   train_acc = 0.5074626865671642   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.690713107585907   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7267841100692749   train_acc = 0.5074626865671642   val_acc = 0.5714285714285714\n",
      "iteration: 6/9   loss = 0.8817394971847534   train_acc = 0.5149253731343284   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.7221646904945374   train_acc = 0.5   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.7297524809837341   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.677684485912323   train_acc = 0.5   val_acc = 0.5\n",
      "Final val acc for lr=6.469108725047993e-06, wd=0.00032736198184168545: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.469108725047993e-06, wd=0.00032736198184168545, betas=(0.95, 0.99)\n",
      "iteration: 1/9   loss = 0.7038959264755249   train_acc = 0.5522388059701493   val_acc = 0.6428571428571429\n",
      "iteration: 2/9   loss = 0.7250465154647827   train_acc = 0.5597014925373134   val_acc = 0.7142857142857143\n",
      "iteration: 3/9   loss = 0.7030088901519775   train_acc = 0.5671641791044776   val_acc = 0.6428571428571429\n",
      "iteration: 4/9   loss = 0.6890791654586792   train_acc = 0.5298507462686567   val_acc = 0.5714285714285714\n",
      "iteration: 5/9   loss = 0.7148975729942322   train_acc = 0.5746268656716418   val_acc = 0.6428571428571429\n",
      "iteration: 6/9   loss = 0.6203880310058594   train_acc = 0.5746268656716418   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.6970852017402649   train_acc = 0.5597014925373134   val_acc = 0.7142857142857143\n",
      "iteration: 8/9   loss = 0.6895262598991394   train_acc = 0.5671641791044776   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.6528282165527344   train_acc = 0.6417910447761194   val_acc = 0.6428571428571429\n",
      "Final val acc for lr=6.469108725047993e-06, wd=0.00032736198184168545: 0.6429\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.469108725047993e-06, wd=0.00032736198184168545, betas=(0.95, 0.995)\n",
      "iteration: 1/9   loss = 0.7381161451339722   train_acc = 0.5597014925373134   val_acc = 0.6428571428571429\n",
      "iteration: 2/9   loss = 0.6974368095397949   train_acc = 0.5223880597014925   val_acc = 0.7142857142857143\n",
      "iteration: 3/9   loss = 0.6506156325340271   train_acc = 0.5447761194029851   val_acc = 0.7857142857142857\n",
      "iteration: 4/9   loss = 0.7103910446166992   train_acc = 0.5373134328358209   val_acc = 0.8571428571428571\n",
      "iteration: 5/9   loss = 0.7443339824676514   train_acc = 0.5373134328358209   val_acc = 0.7142857142857143\n",
      "iteration: 6/9   loss = 0.7628383636474609   train_acc = 0.5522388059701493   val_acc = 0.7142857142857143\n",
      "iteration: 7/9   loss = 0.7008589506149292   train_acc = 0.5522388059701493   val_acc = 0.6428571428571429\n",
      "iteration: 8/9   loss = 0.6756609678268433   train_acc = 0.5746268656716418   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.791404128074646   train_acc = 0.5970149253731343   val_acc = 0.6428571428571429\n",
      "Final val acc for lr=6.469108725047993e-06, wd=0.00032736198184168545: 0.6429\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=6.469108725047993e-06, wd=0.00032736198184168545, betas=(0.95, 0.999)\n",
      "iteration: 1/9   loss = 0.7078694701194763   train_acc = 0.4626865671641791   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.6887201070785522   train_acc = 0.4925373134328358   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.8642274141311646   train_acc = 0.5298507462686567   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.6969527006149292   train_acc = 0.5   val_acc = 0.35714285714285715\n",
      "iteration: 5/9   loss = 0.7286348342895508   train_acc = 0.5223880597014925   val_acc = 0.21428571428571427\n",
      "iteration: 6/9   loss = 0.652458131313324   train_acc = 0.5223880597014925   val_acc = 0.2857142857142857\n",
      "iteration: 7/9   loss = 0.6786912679672241   train_acc = 0.5074626865671642   val_acc = 0.35714285714285715\n",
      "iteration: 8/9   loss = 0.719081699848175   train_acc = 0.5223880597014925   val_acc = 0.35714285714285715\n",
      "iteration: 9/9   loss = 0.7084355354309082   train_acc = 0.5597014925373134   val_acc = 0.2857142857142857\n",
      "Final val acc for lr=6.469108725047993e-06, wd=0.00032736198184168545: 0.2857\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=2.6556669323644005e-06, wd=0.0004609389889268166, betas=(0.85, 0.99)\n",
      "iteration: 1/9   loss = 0.7385659217834473   train_acc = 0.5447761194029851   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7291584014892578   train_acc = 0.5447761194029851   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.7415204048156738   train_acc = 0.5746268656716418   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.7078574299812317   train_acc = 0.5671641791044776   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7169659733772278   train_acc = 0.582089552238806   val_acc = 0.5714285714285714\n",
      "iteration: 6/9   loss = 0.6143020391464233   train_acc = 0.582089552238806   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.7762975096702576   train_acc = 0.5597014925373134   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7110216021537781   train_acc = 0.5597014925373134   val_acc = 0.42857142857142855\n",
      "iteration: 9/9   loss = 0.5862407684326172   train_acc = 0.5671641791044776   val_acc = 0.5\n",
      "Final val acc for lr=2.6556669323644005e-06, wd=0.0004609389889268166: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=2.6556669323644005e-06, wd=0.0004609389889268166, betas=(0.85, 0.995)\n",
      "iteration: 1/9   loss = 0.7126399278640747   train_acc = 0.5   val_acc = 0.2857142857142857\n",
      "iteration: 2/9   loss = 0.7076452970504761   train_acc = 0.5373134328358209   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.7658945918083191   train_acc = 0.5671641791044776   val_acc = 0.35714285714285715\n",
      "iteration: 4/9   loss = 0.7019162178039551   train_acc = 0.5671641791044776   val_acc = 0.35714285714285715\n",
      "iteration: 5/9   loss = 0.7325450778007507   train_acc = 0.5597014925373134   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.7732226848602295   train_acc = 0.5447761194029851   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.750164270401001   train_acc = 0.5746268656716418   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.6866964101791382   train_acc = 0.582089552238806   val_acc = 0.35714285714285715\n",
      "iteration: 9/9   loss = 0.6157582402229309   train_acc = 0.5970149253731343   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=2.6556669323644005e-06, wd=0.0004609389889268166: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=2.6556669323644005e-06, wd=0.0004609389889268166, betas=(0.9, 0.98)\n",
      "iteration: 1/9   loss = 0.7479269504547119   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.8503565788269043   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.8719415664672852   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.8126553893089294   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7243157625198364   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.7631649971008301   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7196203470230103   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.8250252604484558   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.7253595590591431   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "Final val acc for lr=2.6556669323644005e-06, wd=0.0004609389889268166: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=2.6556669323644005e-06, wd=0.0004609389889268166, betas=(0.9, 0.99)\n",
      "iteration: 1/9   loss = 0.7169243097305298   train_acc = 0.5373134328358209   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7292860746383667   train_acc = 0.5074626865671642   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.8221110701560974   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7263978123664856   train_acc = 0.5447761194029851   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7124711871147156   train_acc = 0.5522388059701493   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.7799496054649353   train_acc = 0.5074626865671642   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.697006344795227   train_acc = 0.5298507462686567   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7186828851699829   train_acc = 0.4925373134328358   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.5442577004432678   train_acc = 0.5522388059701493   val_acc = 0.5714285714285714\n",
      "Final val acc for lr=2.6556669323644005e-06, wd=0.0004609389889268166: 0.5714\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=2.6556669323644005e-06, wd=0.0004609389889268166, betas=(0.9, 0.995)\n",
      "iteration: 1/9   loss = 0.6573609113693237   train_acc = 0.5074626865671642   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.7459487915039062   train_acc = 0.5074626865671642   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.6884025931358337   train_acc = 0.5223880597014925   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7610390186309814   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.6635583639144897   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.5917201042175293   train_acc = 0.5373134328358209   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.7342162728309631   train_acc = 0.5298507462686567   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.7570655345916748   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.7657086253166199   train_acc = 0.5223880597014925   val_acc = 0.5\n",
      "Final val acc for lr=2.6556669323644005e-06, wd=0.0004609389889268166: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=2.6556669323644005e-06, wd=0.0004609389889268166, betas=(0.9, 0.999)\n",
      "iteration: 1/9   loss = 0.7637577056884766   train_acc = 0.5223880597014925   val_acc = 0.35714285714285715\n",
      "iteration: 2/9   loss = 0.7482508420944214   train_acc = 0.5298507462686567   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.7490948438644409   train_acc = 0.5373134328358209   val_acc = 0.35714285714285715\n",
      "iteration: 4/9   loss = 0.6951913237571716   train_acc = 0.5746268656716418   val_acc = 0.35714285714285715\n",
      "iteration: 5/9   loss = 0.7284046411514282   train_acc = 0.5522388059701493   val_acc = 0.5714285714285714\n",
      "iteration: 6/9   loss = 0.8158807754516602   train_acc = 0.5970149253731343   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7660086154937744   train_acc = 0.5522388059701493   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7068623304367065   train_acc = 0.5671641791044776   val_acc = 0.42857142857142855\n",
      "iteration: 9/9   loss = 0.6575556993484497   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "Final val acc for lr=2.6556669323644005e-06, wd=0.0004609389889268166: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=2.6556669323644005e-06, wd=0.0004609389889268166, betas=(0.95, 0.98)\n",
      "iteration: 1/9   loss = 0.7655351161956787   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7996428608894348   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.9720489978790283   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.8451576232910156   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.8265698552131653   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.8194491267204285   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.8457903861999512   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7803871035575867   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.5916426181793213   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "Final val acc for lr=2.6556669323644005e-06, wd=0.0004609389889268166: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=2.6556669323644005e-06, wd=0.0004609389889268166, betas=(0.95, 0.99)\n",
      "iteration: 1/9   loss = 0.6680291295051575   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.8293738961219788   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.7039941549301147   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7383475303649902   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.80218505859375   train_acc = 0.5074626865671642   val_acc = 0.5714285714285714\n",
      "iteration: 6/9   loss = 0.7990667223930359   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7299365401268005   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.743582010269165   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.8212147951126099   train_acc = 0.5298507462686567   val_acc = 0.5\n",
      "Final val acc for lr=2.6556669323644005e-06, wd=0.0004609389889268166: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=2.6556669323644005e-06, wd=0.0004609389889268166, betas=(0.95, 0.995)\n",
      "iteration: 1/9   loss = 0.7812799215316772   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.6866003274917603   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.669432520866394   train_acc = 0.47761194029850745   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7602887153625488   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7201451063156128   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.6848536729812622   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7990537881851196   train_acc = 0.5   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.6954811215400696   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.5778195858001709   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "Final val acc for lr=2.6556669323644005e-06, wd=0.0004609389889268166: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=2.6556669323644005e-06, wd=0.0004609389889268166, betas=(0.95, 0.999)\n",
      "iteration: 1/9   loss = 0.6881217956542969   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7175735235214233   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.7240114212036133   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.6758916974067688   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7426718473434448   train_acc = 0.5223880597014925   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.6804443597793579   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7207835912704468   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7031388282775879   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.7814635038375854   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "Final val acc for lr=2.6556669323644005e-06, wd=0.0004609389889268166: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=2.6556669323644005e-06, wd=0.0002300292096821906, betas=(0.85, 0.99)\n",
      "iteration: 1/9   loss = 0.7037519216537476   train_acc = 0.44029850746268656   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.6761974096298218   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.693163275718689   train_acc = 0.4626865671641791   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7931323051452637   train_acc = 0.4701492537313433   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.6808534860610962   train_acc = 0.47761194029850745   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.7120016813278198   train_acc = 0.40298507462686567   val_acc = 0.35714285714285715\n",
      "iteration: 7/9   loss = 0.7037566304206848   train_acc = 0.47761194029850745   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.737523078918457   train_acc = 0.48507462686567165   val_acc = 0.42857142857142855\n",
      "iteration: 9/9   loss = 0.7036272883415222   train_acc = 0.4701492537313433   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=2.6556669323644005e-06, wd=0.0002300292096821906: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=2.6556669323644005e-06, wd=0.0002300292096821906, betas=(0.85, 0.995)\n",
      "iteration: 1/9   loss = 0.7469513416290283   train_acc = 0.47761194029850745   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.709532618522644   train_acc = 0.4701492537313433   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.7484899759292603   train_acc = 0.4701492537313433   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.729124903678894   train_acc = 0.4701492537313433   val_acc = 0.6428571428571429\n",
      "iteration: 5/9   loss = 0.6840388774871826   train_acc = 0.47761194029850745   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.6113815307617188   train_acc = 0.48507462686567165   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.6971545219421387   train_acc = 0.47761194029850745   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7315131425857544   train_acc = 0.48507462686567165   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.5987861156463623   train_acc = 0.47761194029850745   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=2.6556669323644005e-06, wd=0.0002300292096821906: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=2.6556669323644005e-06, wd=0.0002300292096821906, betas=(0.9, 0.98)\n",
      "iteration: 1/9   loss = 0.7512580156326294   train_acc = 0.5   val_acc = 0.35714285714285715\n",
      "iteration: 2/9   loss = 0.73511803150177   train_acc = 0.4701492537313433   val_acc = 0.2857142857142857\n",
      "iteration: 3/9   loss = 0.8221989870071411   train_acc = 0.4701492537313433   val_acc = 0.35714285714285715\n",
      "iteration: 4/9   loss = 0.6828644871711731   train_acc = 0.4925373134328358   val_acc = 0.21428571428571427\n",
      "iteration: 5/9   loss = 0.7621062994003296   train_acc = 0.5149253731343284   val_acc = 0.35714285714285715\n",
      "iteration: 6/9   loss = 0.8238017559051514   train_acc = 0.5   val_acc = 0.2857142857142857\n",
      "iteration: 7/9   loss = 0.7107033729553223   train_acc = 0.5447761194029851   val_acc = 0.35714285714285715\n",
      "iteration: 8/9   loss = 0.6864264011383057   train_acc = 0.5149253731343284   val_acc = 0.35714285714285715\n",
      "iteration: 9/9   loss = 0.8074808120727539   train_acc = 0.5   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=2.6556669323644005e-06, wd=0.0002300292096821906: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=2.6556669323644005e-06, wd=0.0002300292096821906, betas=(0.9, 0.99)\n",
      "iteration: 1/9   loss = 0.7379957437515259   train_acc = 0.5074626865671642   val_acc = 0.5714285714285714\n",
      "iteration: 2/9   loss = 0.7277063727378845   train_acc = 0.5   val_acc = 0.5714285714285714\n",
      "iteration: 3/9   loss = 0.6022685766220093   train_acc = 0.5074626865671642   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.7572227716445923   train_acc = 0.5   val_acc = 0.5714285714285714\n",
      "iteration: 5/9   loss = 0.6567461490631104   train_acc = 0.5   val_acc = 0.5714285714285714\n",
      "iteration: 6/9   loss = 0.7666360139846802   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7362933158874512   train_acc = 0.5   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.7102404832839966   train_acc = 0.4925373134328358   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.8093976378440857   train_acc = 0.5   val_acc = 0.5\n",
      "Final val acc for lr=2.6556669323644005e-06, wd=0.0002300292096821906: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=2.6556669323644005e-06, wd=0.0002300292096821906, betas=(0.9, 0.995)\n",
      "iteration: 1/9   loss = 0.6616876125335693   train_acc = 0.4925373134328358   val_acc = 0.8571428571428571\n",
      "iteration: 2/9   loss = 0.7682514190673828   train_acc = 0.4626865671641791   val_acc = 0.6428571428571429\n",
      "iteration: 3/9   loss = 0.6460686326026917   train_acc = 0.4552238805970149   val_acc = 0.7142857142857143\n",
      "iteration: 4/9   loss = 0.7048218250274658   train_acc = 0.48507462686567165   val_acc = 0.7142857142857143\n",
      "iteration: 5/9   loss = 0.7629927396774292   train_acc = 0.4626865671641791   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.6660381555557251   train_acc = 0.4552238805970149   val_acc = 0.7857142857142857\n",
      "iteration: 7/9   loss = 0.7137446403503418   train_acc = 0.48507462686567165   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.6921812295913696   train_acc = 0.5298507462686567   val_acc = 0.7857142857142857\n",
      "iteration: 9/9   loss = 0.7314010858535767   train_acc = 0.5   val_acc = 0.5\n",
      "Final val acc for lr=2.6556669323644005e-06, wd=0.0002300292096821906: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=2.6556669323644005e-06, wd=0.0002300292096821906, betas=(0.9, 0.999)\n",
      "iteration: 1/9   loss = 0.7201888561248779   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.717962920665741   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.7665182948112488   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7568198442459106   train_acc = 0.5223880597014925   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7171952724456787   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.7324202060699463   train_acc = 0.4925373134328358   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.7100672721862793   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7301379442214966   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.6614774465560913   train_acc = 0.48507462686567165   val_acc = 0.5714285714285714\n",
      "Final val acc for lr=2.6556669323644005e-06, wd=0.0002300292096821906: 0.5714\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=2.6556669323644005e-06, wd=0.0002300292096821906, betas=(0.95, 0.98)\n",
      "iteration: 1/9   loss = 0.7888486981391907   train_acc = 0.5074626865671642   val_acc = 0.5714285714285714\n",
      "iteration: 2/9   loss = 0.7167212963104248   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.6757924556732178   train_acc = 0.5149253731343284   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.6670681238174438   train_acc = 0.5   val_acc = 0.5714285714285714\n",
      "iteration: 5/9   loss = 0.7172157168388367   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.9245822429656982   train_acc = 0.5   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.769287109375   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7476527690887451   train_acc = 0.4925373134328358   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.693480372428894   train_acc = 0.5   val_acc = 0.5714285714285714\n",
      "Final val acc for lr=2.6556669323644005e-06, wd=0.0002300292096821906: 0.5714\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=2.6556669323644005e-06, wd=0.0002300292096821906, betas=(0.95, 0.99)\n",
      "iteration: 1/9   loss = 0.7425452470779419   train_acc = 0.6044776119402985   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.7585284113883972   train_acc = 0.6417910447761194   val_acc = 0.6428571428571429\n",
      "iteration: 3/9   loss = 0.6732720136642456   train_acc = 0.6417910447761194   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7173704504966736   train_acc = 0.5970149253731343   val_acc = 0.6428571428571429\n",
      "iteration: 5/9   loss = 0.7120084762573242   train_acc = 0.5970149253731343   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.6829414367675781   train_acc = 0.5895522388059702   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7516494989395142   train_acc = 0.6044776119402985   val_acc = 0.2857142857142857\n",
      "iteration: 8/9   loss = 0.7244009375572205   train_acc = 0.6194029850746269   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.6631248593330383   train_acc = 0.5746268656716418   val_acc = 0.5\n",
      "Final val acc for lr=2.6556669323644005e-06, wd=0.0002300292096821906: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=2.6556669323644005e-06, wd=0.0002300292096821906, betas=(0.95, 0.995)\n",
      "iteration: 1/9   loss = 0.749350368976593   train_acc = 0.4626865671641791   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.7584075927734375   train_acc = 0.5074626865671642   val_acc = 0.5714285714285714\n",
      "iteration: 3/9   loss = 0.7126781344413757   train_acc = 0.4253731343283582   val_acc = 0.35714285714285715\n",
      "iteration: 4/9   loss = 0.6934998035430908   train_acc = 0.4552238805970149   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.697990357875824   train_acc = 0.48507462686567165   val_acc = 0.14285714285714285\n",
      "iteration: 6/9   loss = 0.8785219192504883   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7494877576828003   train_acc = 0.5074626865671642   val_acc = 0.2857142857142857\n",
      "iteration: 8/9   loss = 0.6673423051834106   train_acc = 0.44029850746268656   val_acc = 0.35714285714285715\n",
      "iteration: 9/9   loss = 0.5954670310020447   train_acc = 0.5074626865671642   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=2.6556669323644005e-06, wd=0.0002300292096821906: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=2.6556669323644005e-06, wd=0.0002300292096821906, betas=(0.95, 0.999)\n",
      "iteration: 1/9   loss = 0.770379900932312   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7343642711639404   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.7864577770233154   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.6954960823059082   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7041728496551514   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.7752176523208618   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7487882375717163   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.6630713939666748   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.7216564416885376   train_acc = 0.5223880597014925   val_acc = 0.5\n",
      "Final val acc for lr=2.6556669323644005e-06, wd=0.0002300292096821906: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=2.6556669323644005e-06, wd=0.00022555743977178082, betas=(0.85, 0.99)\n",
      "iteration: 1/9   loss = 0.740506112575531   train_acc = 0.47761194029850745   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7177795171737671   train_acc = 0.47761194029850745   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.9737646579742432   train_acc = 0.4701492537313433   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7048699855804443   train_acc = 0.5223880597014925   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.702338457107544   train_acc = 0.4552238805970149   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.8021743297576904   train_acc = 0.47761194029850745   val_acc = 0.6428571428571429\n",
      "iteration: 7/9   loss = 0.7497711181640625   train_acc = 0.5298507462686567   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.7314386367797852   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.7013300657272339   train_acc = 0.4701492537313433   val_acc = 0.5714285714285714\n",
      "Final val acc for lr=2.6556669323644005e-06, wd=0.00022555743977178082: 0.5714\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=2.6556669323644005e-06, wd=0.00022555743977178082, betas=(0.85, 0.995)\n",
      "iteration: 1/9   loss = 0.7539626955986023   train_acc = 0.5373134328358209   val_acc = 0.7142857142857143\n",
      "iteration: 2/9   loss = 0.7112401127815247   train_acc = 0.44776119402985076   val_acc = 0.35714285714285715\n",
      "iteration: 3/9   loss = 0.7119812965393066   train_acc = 0.4552238805970149   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.7246403694152832   train_acc = 0.5   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.7513637542724609   train_acc = 0.47761194029850745   val_acc = 0.7142857142857143\n",
      "iteration: 6/9   loss = 0.8487241268157959   train_acc = 0.48507462686567165   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.700157642364502   train_acc = 0.47761194029850745   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7277629375457764   train_acc = 0.47761194029850745   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.7662521600723267   train_acc = 0.5373134328358209   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=2.6556669323644005e-06, wd=0.00022555743977178082: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=2.6556669323644005e-06, wd=0.00022555743977178082, betas=(0.9, 0.98)\n",
      "iteration: 1/9   loss = 0.7291772365570068   train_acc = 0.5074626865671642   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.7261407375335693   train_acc = 0.5074626865671642   val_acc = 0.35714285714285715\n",
      "iteration: 3/9   loss = 0.5864884257316589   train_acc = 0.5671641791044776   val_acc = 0.7142857142857143\n",
      "iteration: 4/9   loss = 0.7340801358222961   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7079858779907227   train_acc = 0.5373134328358209   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.7834419012069702   train_acc = 0.5074626865671642   val_acc = 0.35714285714285715\n",
      "iteration: 7/9   loss = 0.7169163227081299   train_acc = 0.48507462686567165   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.7249470949172974   train_acc = 0.5   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.591991662979126   train_acc = 0.5447761194029851   val_acc = 0.5714285714285714\n",
      "Final val acc for lr=2.6556669323644005e-06, wd=0.00022555743977178082: 0.5714\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=2.6556669323644005e-06, wd=0.00022555743977178082, betas=(0.9, 0.99)\n",
      "iteration: 1/9   loss = 0.7552121877670288   train_acc = 0.4701492537313433   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7506282329559326   train_acc = 0.4626865671641791   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.6195612549781799   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.6791619062423706   train_acc = 0.4626865671641791   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.7430940270423889   train_acc = 0.4626865671641791   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.7717587947845459   train_acc = 0.4626865671641791   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.7254495620727539   train_acc = 0.44029850746268656   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.774976372718811   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.788956344127655   train_acc = 0.4701492537313433   val_acc = 0.5\n",
      "Final val acc for lr=2.6556669323644005e-06, wd=0.00022555743977178082: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=2.6556669323644005e-06, wd=0.00022555743977178082, betas=(0.9, 0.995)\n",
      "iteration: 1/9   loss = 0.6939284801483154   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.6846125721931458   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.7314270734786987   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7058697938919067   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.6823433637619019   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.6666502356529236   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7090981602668762   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7069312930107117   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.7289069890975952   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "Final val acc for lr=2.6556669323644005e-06, wd=0.00022555743977178082: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=2.6556669323644005e-06, wd=0.00022555743977178082, betas=(0.9, 0.999)\n",
      "iteration: 1/9   loss = 0.6919846534729004   train_acc = 0.5447761194029851   val_acc = 0.6428571428571429\n",
      "iteration: 2/9   loss = 0.695134162902832   train_acc = 0.47761194029850745   val_acc = 0.5714285714285714\n",
      "iteration: 3/9   loss = 0.7642285823822021   train_acc = 0.4626865671641791   val_acc = 0.6428571428571429\n",
      "iteration: 4/9   loss = 0.7616630792617798   train_acc = 0.5   val_acc = 0.6428571428571429\n",
      "iteration: 5/9   loss = 0.7180811762809753   train_acc = 0.5373134328358209   val_acc = 0.7142857142857143\n",
      "iteration: 6/9   loss = 0.6576720476150513   train_acc = 0.5895522388059702   val_acc = 0.6428571428571429\n",
      "iteration: 7/9   loss = 0.7095304727554321   train_acc = 0.47761194029850745   val_acc = 0.6428571428571429\n",
      "iteration: 8/9   loss = 0.7172461152076721   train_acc = 0.5149253731343284   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.7422146201133728   train_acc = 0.5447761194029851   val_acc = 0.6428571428571429\n",
      "Final val acc for lr=2.6556669323644005e-06, wd=0.00022555743977178082: 0.6429\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=2.6556669323644005e-06, wd=0.00022555743977178082, betas=(0.95, 0.98)\n",
      "iteration: 1/9   loss = 0.7038326263427734   train_acc = 0.4701492537313433   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.761034369468689   train_acc = 0.4552238805970149   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.734186053276062   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7213002443313599   train_acc = 0.4701492537313433   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7322529554367065   train_acc = 0.47761194029850745   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.6860984563827515   train_acc = 0.44776119402985076   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.665591835975647   train_acc = 0.4701492537313433   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.6697811484336853   train_acc = 0.47761194029850745   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.6588496565818787   train_acc = 0.47761194029850745   val_acc = 0.5\n",
      "Final val acc for lr=2.6556669323644005e-06, wd=0.00022555743977178082: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=2.6556669323644005e-06, wd=0.00022555743977178082, betas=(0.95, 0.99)\n",
      "iteration: 1/9   loss = 0.731613039970398   train_acc = 0.5149253731343284   val_acc = 0.35714285714285715\n",
      "iteration: 2/9   loss = 0.6872755885124207   train_acc = 0.5597014925373134   val_acc = 0.21428571428571427\n",
      "iteration: 3/9   loss = 0.851146399974823   train_acc = 0.5373134328358209   val_acc = 0.2857142857142857\n",
      "iteration: 4/9   loss = 0.7265292406082153   train_acc = 0.5447761194029851   val_acc = 0.21428571428571427\n",
      "iteration: 5/9   loss = 0.7687960863113403   train_acc = 0.5223880597014925   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.6545748710632324   train_acc = 0.5671641791044776   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.7316068410873413   train_acc = 0.5298507462686567   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.7138340473175049   train_acc = 0.5298507462686567   val_acc = 0.35714285714285715\n",
      "iteration: 9/9   loss = 0.763293445110321   train_acc = 0.5597014925373134   val_acc = 0.14285714285714285\n",
      "Final val acc for lr=2.6556669323644005e-06, wd=0.00022555743977178082: 0.1429\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=2.6556669323644005e-06, wd=0.00022555743977178082, betas=(0.95, 0.995)\n",
      "iteration: 1/9   loss = 0.7519925832748413   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7210361361503601   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.8183726668357849   train_acc = 0.5223880597014925   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.7323809266090393   train_acc = 0.5074626865671642   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.7287750244140625   train_acc = 0.5298507462686567   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.6994446516036987   train_acc = 0.5149253731343284   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.7069095373153687   train_acc = 0.5447761194029851   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7406274080276489   train_acc = 0.5447761194029851   val_acc = 0.6428571428571429\n",
      "iteration: 9/9   loss = 0.6985197067260742   train_acc = 0.5746268656716418   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=2.6556669323644005e-06, wd=0.00022555743977178082: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=2.6556669323644005e-06, wd=0.00022555743977178082, betas=(0.95, 0.999)\n",
      "iteration: 1/9   loss = 0.7609845995903015   train_acc = 0.5   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.7452669143676758   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.9150974154472351   train_acc = 0.5223880597014925   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.7523967623710632   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7327439785003662   train_acc = 0.5223880597014925   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.6155872344970703   train_acc = 0.47761194029850745   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7385909557342529   train_acc = 0.5223880597014925   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7182585000991821   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.8279402256011963   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "Final val acc for lr=2.6556669323644005e-06, wd=0.00022555743977178082: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=2.6556669323644005e-06, wd=0.00011822143451838108, betas=(0.85, 0.99)\n",
      "iteration: 1/9   loss = 0.6985381245613098   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.723274827003479   train_acc = 0.47761194029850745   val_acc = 0.5714285714285714\n",
      "iteration: 3/9   loss = 0.9685623049736023   train_acc = 0.5   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.7212291955947876   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7156479954719543   train_acc = 0.48507462686567165   val_acc = 0.5714285714285714\n",
      "iteration: 6/9   loss = 0.6570780873298645   train_acc = 0.5074626865671642   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.7261016368865967   train_acc = 0.5223880597014925   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7299923300743103   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.6395490169525146   train_acc = 0.47761194029850745   val_acc = 0.5\n",
      "Final val acc for lr=2.6556669323644005e-06, wd=0.00011822143451838108: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=2.6556669323644005e-06, wd=0.00011822143451838108, betas=(0.85, 0.995)\n",
      "iteration: 1/9   loss = 0.7443709969520569   train_acc = 0.5074626865671642   val_acc = 0.6428571428571429\n",
      "iteration: 2/9   loss = 0.7479609251022339   train_acc = 0.47761194029850745   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.8522205352783203   train_acc = 0.48507462686567165   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.6857843399047852   train_acc = 0.5074626865671642   val_acc = 0.7142857142857143\n",
      "iteration: 5/9   loss = 0.7074196934700012   train_acc = 0.5   val_acc = 0.5714285714285714\n",
      "iteration: 6/9   loss = 0.7371845245361328   train_acc = 0.5074626865671642   val_acc = 0.6428571428571429\n",
      "iteration: 7/9   loss = 0.7454365491867065   train_acc = 0.47761194029850745   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7743762731552124   train_acc = 0.5074626865671642   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.8402235507965088   train_acc = 0.4925373134328358   val_acc = 0.5714285714285714\n",
      "Final val acc for lr=2.6556669323644005e-06, wd=0.00011822143451838108: 0.5714\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=2.6556669323644005e-06, wd=0.00011822143451838108, betas=(0.9, 0.98)\n",
      "iteration: 1/9   loss = 0.6735702753067017   train_acc = 0.5149253731343284   val_acc = 0.35714285714285715\n",
      "iteration: 2/9   loss = 0.7212626934051514   train_acc = 0.5074626865671642   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.8053877353668213   train_acc = 0.4925373134328358   val_acc = 0.2857142857142857\n",
      "iteration: 4/9   loss = 0.7163063287734985   train_acc = 0.4626865671641791   val_acc = 0.2857142857142857\n",
      "iteration: 5/9   loss = 0.7401043176651001   train_acc = 0.5223880597014925   val_acc = 0.2857142857142857\n",
      "iteration: 6/9   loss = 0.838819146156311   train_acc = 0.4552238805970149   val_acc = 0.35714285714285715\n",
      "iteration: 7/9   loss = 0.7167248129844666   train_acc = 0.5074626865671642   val_acc = 0.2857142857142857\n",
      "iteration: 8/9   loss = 0.7239415645599365   train_acc = 0.4701492537313433   val_acc = 0.35714285714285715\n",
      "iteration: 9/9   loss = 0.7883631587028503   train_acc = 0.5149253731343284   val_acc = 0.35714285714285715\n",
      "Final val acc for lr=2.6556669323644005e-06, wd=0.00011822143451838108: 0.3571\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=2.6556669323644005e-06, wd=0.00011822143451838108, betas=(0.9, 0.99)\n",
      "iteration: 1/9   loss = 0.7623636722564697   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7223418951034546   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.6742246150970459   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.6926853656768799   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.8014901876449585   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.5158068537712097   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7570852041244507   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7647215723991394   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.814236044883728   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "Final val acc for lr=2.6556669323644005e-06, wd=0.00011822143451838108: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=2.6556669323644005e-06, wd=0.00011822143451838108, betas=(0.9, 0.995)\n",
      "iteration: 1/9   loss = 0.7432394623756409   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.715547502040863   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.7781875133514404   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.6970144510269165   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7125450372695923   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.7413133382797241   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7057548761367798   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7013399600982666   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.7530085444450378   train_acc = 0.5   val_acc = 0.5\n",
      "Final val acc for lr=2.6556669323644005e-06, wd=0.00011822143451838108: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=2.6556669323644005e-06, wd=0.00011822143451838108, betas=(0.9, 0.999)\n",
      "iteration: 1/9   loss = 0.7225070595741272   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7106359004974365   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.7792061567306519   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7349569797515869   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7373346090316772   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.7683472037315369   train_acc = 0.5   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.6971305012702942   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7344424724578857   train_acc = 0.5   val_acc = 0.42857142857142855\n",
      "iteration: 9/9   loss = 0.6941050291061401   train_acc = 0.5074626865671642   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=2.6556669323644005e-06, wd=0.00011822143451838108: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=2.6556669323644005e-06, wd=0.00011822143451838108, betas=(0.95, 0.98)\n",
      "iteration: 1/9   loss = 0.7705972790718079   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.6983094215393066   train_acc = 0.5074626865671642   val_acc = 0.5714285714285714\n",
      "iteration: 3/9   loss = 0.6748104095458984   train_acc = 0.5   val_acc = 0.6428571428571429\n",
      "iteration: 4/9   loss = 0.68731689453125   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7718107104301453   train_acc = 0.4925373134328358   val_acc = 0.5714285714285714\n",
      "iteration: 6/9   loss = 0.8070847392082214   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7100785970687866   train_acc = 0.47761194029850745   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.7345781326293945   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.8548489809036255   train_acc = 0.5223880597014925   val_acc = 0.5714285714285714\n",
      "Final val acc for lr=2.6556669323644005e-06, wd=0.00011822143451838108: 0.5714\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=2.6556669323644005e-06, wd=0.00011822143451838108, betas=(0.95, 0.99)\n",
      "iteration: 1/9   loss = 0.7284717559814453   train_acc = 0.5074626865671642   val_acc = 0.35714285714285715\n",
      "iteration: 2/9   loss = 0.7287600636482239   train_acc = 0.5298507462686567   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.5234667062759399   train_acc = 0.5   val_acc = 0.35714285714285715\n",
      "iteration: 4/9   loss = 0.709083616733551   train_acc = 0.5149253731343284   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.7048969864845276   train_acc = 0.5074626865671642   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.9583145976066589   train_acc = 0.5149253731343284   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.7748986482620239   train_acc = 0.5074626865671642   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.7233933210372925   train_acc = 0.5074626865671642   val_acc = 0.42857142857142855\n",
      "iteration: 9/9   loss = 0.7372035384178162   train_acc = 0.5298507462686567   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=2.6556669323644005e-06, wd=0.00011822143451838108: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=2.6556669323644005e-06, wd=0.00011822143451838108, betas=(0.95, 0.995)\n",
      "iteration: 1/9   loss = 0.6781037449836731   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7566449642181396   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.736876368522644   train_acc = 0.44776119402985076   val_acc = 0.6428571428571429\n",
      "iteration: 4/9   loss = 0.6776809692382812   train_acc = 0.4626865671641791   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.7381203174591064   train_acc = 0.4253731343283582   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.7860140800476074   train_acc = 0.47761194029850745   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7356849908828735   train_acc = 0.47761194029850745   val_acc = 0.6428571428571429\n",
      "iteration: 8/9   loss = 0.73162841796875   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.7952688932418823   train_acc = 0.47761194029850745   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=2.6556669323644005e-06, wd=0.00011822143451838108: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=2.6556669323644005e-06, wd=0.00011822143451838108, betas=(0.95, 0.999)\n",
      "iteration: 1/9   loss = 0.7454198598861694   train_acc = 0.44776119402985076   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.73188716173172   train_acc = 0.4552238805970149   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.8660430312156677   train_acc = 0.44029850746268656   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7703573107719421   train_acc = 0.44776119402985076   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.783132791519165   train_acc = 0.43283582089552236   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.6059774160385132   train_acc = 0.43283582089552236   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7355461120605469   train_acc = 0.44029850746268656   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7521519064903259   train_acc = 0.4552238805970149   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.8691238760948181   train_acc = 0.43283582089552236   val_acc = 0.5\n",
      "Final val acc for lr=2.6556669323644005e-06, wd=0.00011822143451838108: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=2.6556669323644005e-06, wd=0.00032736198184168545, betas=(0.85, 0.99)\n",
      "iteration: 1/9   loss = 0.6957874894142151   train_acc = 0.5298507462686567   val_acc = 0.35714285714285715\n",
      "iteration: 2/9   loss = 0.6832447648048401   train_acc = 0.5746268656716418   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.689457893371582   train_acc = 0.582089552238806   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.68426513671875   train_acc = 0.5373134328358209   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.7394071817398071   train_acc = 0.5671641791044776   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.6528916358947754   train_acc = 0.5895522388059702   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.7268484830856323   train_acc = 0.5522388059701493   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.729275643825531   train_acc = 0.5298507462686567   val_acc = 0.35714285714285715\n",
      "iteration: 9/9   loss = 0.6827975511550903   train_acc = 0.5671641791044776   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=2.6556669323644005e-06, wd=0.00032736198184168545: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=2.6556669323644005e-06, wd=0.00032736198184168545, betas=(0.85, 0.995)\n",
      "iteration: 1/9   loss = 0.7429375648498535   train_acc = 0.5671641791044776   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.6982407569885254   train_acc = 0.5373134328358209   val_acc = 0.35714285714285715\n",
      "iteration: 3/9   loss = 0.8048496246337891   train_acc = 0.5895522388059702   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.6932826042175293   train_acc = 0.582089552238806   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.6667048335075378   train_acc = 0.5597014925373134   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.5829863548278809   train_acc = 0.582089552238806   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.7294222116470337   train_acc = 0.582089552238806   val_acc = 0.6428571428571429\n",
      "iteration: 8/9   loss = 0.7142555117607117   train_acc = 0.6044776119402985   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.6053769588470459   train_acc = 0.5746268656716418   val_acc = 0.5\n",
      "Final val acc for lr=2.6556669323644005e-06, wd=0.00032736198184168545: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=2.6556669323644005e-06, wd=0.00032736198184168545, betas=(0.9, 0.98)\n",
      "iteration: 1/9   loss = 0.6705297231674194   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.704460859298706   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.8024915456771851   train_acc = 0.5223880597014925   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.6813183426856995   train_acc = 0.5298507462686567   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7341132164001465   train_acc = 0.5447761194029851   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.6184978485107422   train_acc = 0.5223880597014925   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.6686937808990479   train_acc = 0.5298507462686567   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7049279808998108   train_acc = 0.5597014925373134   val_acc = 0.42857142857142855\n",
      "iteration: 9/9   loss = 0.7864417433738708   train_acc = 0.5074626865671642   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=2.6556669323644005e-06, wd=0.00032736198184168545: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=2.6556669323644005e-06, wd=0.00032736198184168545, betas=(0.9, 0.99)\n",
      "iteration: 1/9   loss = 0.6845202445983887   train_acc = 0.4925373134328358   val_acc = 0.6428571428571429\n",
      "iteration: 2/9   loss = 0.7164813280105591   train_acc = 0.4626865671641791   val_acc = 0.7142857142857143\n",
      "iteration: 3/9   loss = 0.7838877439498901   train_acc = 0.4701492537313433   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7438973188400269   train_acc = 0.5074626865671642   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.7174805402755737   train_acc = 0.4925373134328358   val_acc = 0.5714285714285714\n",
      "iteration: 6/9   loss = 0.6462827920913696   train_acc = 0.4925373134328358   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.7215609550476074   train_acc = 0.4701492537313433   val_acc = 0.7142857142857143\n",
      "iteration: 8/9   loss = 0.8021745681762695   train_acc = 0.4626865671641791   val_acc = 0.6428571428571429\n",
      "iteration: 9/9   loss = 0.7942479848861694   train_acc = 0.48507462686567165   val_acc = 0.6428571428571429\n",
      "Final val acc for lr=2.6556669323644005e-06, wd=0.00032736198184168545: 0.6429\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=2.6556669323644005e-06, wd=0.00032736198184168545, betas=(0.9, 0.995)\n",
      "iteration: 1/9   loss = 0.6924163103103638   train_acc = 0.4626865671641791   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.6835291981697083   train_acc = 0.4701492537313433   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.8463848829269409   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7230844497680664   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7163816094398499   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.8099408149719238   train_acc = 0.5223880597014925   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.738924503326416   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7318136096000671   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.7957162857055664   train_acc = 0.48507462686567165   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=2.6556669323644005e-06, wd=0.00032736198184168545: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=2.6556669323644005e-06, wd=0.00032736198184168545, betas=(0.9, 0.999)\n",
      "iteration: 1/9   loss = 0.7327679991722107   train_acc = 0.5149253731343284   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.7056012749671936   train_acc = 0.5746268656716418   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.7158660888671875   train_acc = 0.5522388059701493   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7188844680786133   train_acc = 0.5671641791044776   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.7049493789672852   train_acc = 0.5895522388059702   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.7119432687759399   train_acc = 0.5671641791044776   val_acc = 0.35714285714285715\n",
      "iteration: 7/9   loss = 0.6669338345527649   train_acc = 0.5671641791044776   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7445129156112671   train_acc = 0.5522388059701493   val_acc = 0.35714285714285715\n",
      "iteration: 9/9   loss = 0.7928128242492676   train_acc = 0.5746268656716418   val_acc = 0.35714285714285715\n",
      "Final val acc for lr=2.6556669323644005e-06, wd=0.00032736198184168545: 0.3571\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=2.6556669323644005e-06, wd=0.00032736198184168545, betas=(0.95, 0.98)\n",
      "iteration: 1/9   loss = 0.8134319186210632   train_acc = 0.4626865671641791   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7531806826591492   train_acc = 0.4701492537313433   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.6281832456588745   train_acc = 0.47761194029850745   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.674872636795044   train_acc = 0.47761194029850745   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.8208578824996948   train_acc = 0.4626865671641791   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.7768082618713379   train_acc = 0.47761194029850745   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.7777552604675293   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7677152752876282   train_acc = 0.44776119402985076   val_acc = 0.42857142857142855\n",
      "iteration: 9/9   loss = 0.584170401096344   train_acc = 0.4701492537313433   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=2.6556669323644005e-06, wd=0.00032736198184168545: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=2.6556669323644005e-06, wd=0.00032736198184168545, betas=(0.95, 0.99)\n",
      "iteration: 1/9   loss = 0.7137635350227356   train_acc = 0.44776119402985076   val_acc = 0.7142857142857143\n",
      "iteration: 2/9   loss = 0.6751589775085449   train_acc = 0.43283582089552236   val_acc = 0.7142857142857143\n",
      "iteration: 3/9   loss = 0.6137114763259888   train_acc = 0.4626865671641791   val_acc = 0.6428571428571429\n",
      "iteration: 4/9   loss = 0.6965368390083313   train_acc = 0.44776119402985076   val_acc = 0.5714285714285714\n",
      "iteration: 5/9   loss = 0.7131615877151489   train_acc = 0.417910447761194   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.8318538069725037   train_acc = 0.4253731343283582   val_acc = 0.7142857142857143\n",
      "iteration: 7/9   loss = 0.8029735684394836   train_acc = 0.40298507462686567   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7054768204689026   train_acc = 0.47761194029850745   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.6730993986129761   train_acc = 0.4626865671641791   val_acc = 0.6428571428571429\n",
      "Final val acc for lr=2.6556669323644005e-06, wd=0.00032736198184168545: 0.6429\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=2.6556669323644005e-06, wd=0.00032736198184168545, betas=(0.95, 0.995)\n",
      "iteration: 1/9   loss = 0.7530035972595215   train_acc = 0.5074626865671642   val_acc = 0.5714285714285714\n",
      "iteration: 2/9   loss = 0.6614227294921875   train_acc = 0.48507462686567165   val_acc = 0.5714285714285714\n",
      "iteration: 3/9   loss = 0.7287658452987671   train_acc = 0.4925373134328358   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.7302813529968262   train_acc = 0.4925373134328358   val_acc = 0.6428571428571429\n",
      "iteration: 5/9   loss = 0.7069162726402283   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.7651047110557556   train_acc = 0.4925373134328358   val_acc = 0.6428571428571429\n",
      "iteration: 7/9   loss = 0.7634330987930298   train_acc = 0.5   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.74165940284729   train_acc = 0.5223880597014925   val_acc = 0.6428571428571429\n",
      "iteration: 9/9   loss = 0.9796778559684753   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "Final val acc for lr=2.6556669323644005e-06, wd=0.00032736198184168545: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=2.6556669323644005e-06, wd=0.00032736198184168545, betas=(0.95, 0.999)\n",
      "iteration: 1/9   loss = 0.7371923923492432   train_acc = 0.43283582089552236   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7268420457839966   train_acc = 0.3805970149253731   val_acc = 0.6428571428571429\n",
      "iteration: 3/9   loss = 0.6040331125259399   train_acc = 0.44776119402985076   val_acc = 0.7857142857142857\n",
      "iteration: 4/9   loss = 0.7639432549476624   train_acc = 0.43283582089552236   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.6911996603012085   train_acc = 0.41044776119402987   val_acc = 0.5714285714285714\n",
      "iteration: 6/9   loss = 0.7920364737510681   train_acc = 0.41044776119402987   val_acc = 0.6428571428571429\n",
      "iteration: 7/9   loss = 0.7712922096252441   train_acc = 0.4253731343283582   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.718498945236206   train_acc = 0.417910447761194   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.6880426406860352   train_acc = 0.43283582089552236   val_acc = 0.5714285714285714\n",
      "Final val acc for lr=2.6556669323644005e-06, wd=0.00032736198184168545: 0.5714\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.00027082899806832295, wd=0.0004609389889268166, betas=(0.85, 0.99)\n",
      "iteration: 1/9   loss = 0.6884244680404663   train_acc = 0.5746268656716418   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.6701142191886902   train_acc = 0.664179104477612   val_acc = 0.35714285714285715\n",
      "iteration: 3/9   loss = 0.6526128053665161   train_acc = 0.5895522388059702   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7292904853820801   train_acc = 0.6044776119402985   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.6516695022583008   train_acc = 0.6119402985074627   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.6610751152038574   train_acc = 0.6268656716417911   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.6814737915992737   train_acc = 0.6417910447761194   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.584252119064331   train_acc = 0.6567164179104478   val_acc = 0.6428571428571429\n",
      "iteration: 9/9   loss = 0.9310619235038757   train_acc = 0.6791044776119403   val_acc = 0.6428571428571429\n",
      "Final val acc for lr=0.00027082899806832295, wd=0.0004609389889268166: 0.6429\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.00027082899806832295, wd=0.0004609389889268166, betas=(0.85, 0.995)\n",
      "iteration: 1/9   loss = 0.7739949226379395   train_acc = 0.6343283582089553   val_acc = 0.7142857142857143\n",
      "iteration: 2/9   loss = 0.6949273943901062   train_acc = 0.5970149253731343   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.5997761487960815   train_acc = 0.5970149253731343   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7240753173828125   train_acc = 0.6343283582089553   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.6452631950378418   train_acc = 0.6940298507462687   val_acc = 0.2857142857142857\n",
      "iteration: 6/9   loss = 0.6115177869796753   train_acc = 0.6417910447761194   val_acc = 0.21428571428571427\n",
      "iteration: 7/9   loss = 0.5896800756454468   train_acc = 0.6417910447761194   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.6744972467422485   train_acc = 0.6417910447761194   val_acc = 0.35714285714285715\n",
      "iteration: 9/9   loss = 0.7429734468460083   train_acc = 0.6417910447761194   val_acc = 0.2857142857142857\n",
      "Final val acc for lr=0.00027082899806832295, wd=0.0004609389889268166: 0.2857\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.00027082899806832295, wd=0.0004609389889268166, betas=(0.9, 0.98)\n",
      "iteration: 1/9   loss = 0.7377654314041138   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7656034231185913   train_acc = 0.5895522388059702   val_acc = 0.2857142857142857\n",
      "iteration: 3/9   loss = 0.824323832988739   train_acc = 0.6119402985074627   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.6381820440292358   train_acc = 0.6940298507462687   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.6392761468887329   train_acc = 0.6417910447761194   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.8019556999206543   train_acc = 0.6940298507462687   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.5959411263465881   train_acc = 0.6492537313432836   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.6204414367675781   train_acc = 0.6567164179104478   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.7687525749206543   train_acc = 0.6791044776119403   val_acc = 0.5\n",
      "Final val acc for lr=0.00027082899806832295, wd=0.0004609389889268166: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.00027082899806832295, wd=0.0004609389889268166, betas=(0.9, 0.99)\n",
      "iteration: 1/9   loss = 0.7000795006752014   train_acc = 0.5447761194029851   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.8028644323348999   train_acc = 0.6119402985074627   val_acc = 0.35714285714285715\n",
      "iteration: 3/9   loss = 0.7545716762542725   train_acc = 0.6567164179104478   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.6310440897941589   train_acc = 0.6268656716417911   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.6485483050346375   train_acc = 0.7164179104477612   val_acc = 0.2857142857142857\n",
      "iteration: 6/9   loss = 0.5581988096237183   train_acc = 0.7313432835820896   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.6413831114768982   train_acc = 0.7388059701492538   val_acc = 0.35714285714285715\n",
      "iteration: 8/9   loss = 0.6027837991714478   train_acc = 0.6791044776119403   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.48096585273742676   train_acc = 0.6716417910447762   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=0.00027082899806832295, wd=0.0004609389889268166: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.00027082899806832295, wd=0.0004609389889268166, betas=(0.9, 0.995)\n",
      "iteration: 1/9   loss = 0.6867601871490479   train_acc = 0.6119402985074627   val_acc = 0.35714285714285715\n",
      "iteration: 2/9   loss = 0.7060496211051941   train_acc = 0.5895522388059702   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.8051759600639343   train_acc = 0.6044776119402985   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.6476707458496094   train_acc = 0.5970149253731343   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.6504413485527039   train_acc = 0.6194029850746269   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.6371511816978455   train_acc = 0.6119402985074627   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.6351155042648315   train_acc = 0.6940298507462687   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.6663755178451538   train_acc = 0.6716417910447762   val_acc = 0.42857142857142855\n",
      "iteration: 9/9   loss = 0.698830783367157   train_acc = 0.6119402985074627   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=0.00027082899806832295, wd=0.0004609389889268166: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.00027082899806832295, wd=0.0004609389889268166, betas=(0.9, 0.999)\n",
      "iteration: 1/9   loss = 0.7426556348800659   train_acc = 0.5223880597014925   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.6997911930084229   train_acc = 0.5970149253731343   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.5244086384773254   train_acc = 0.5746268656716418   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.6735237836837769   train_acc = 0.5671641791044776   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.6762550473213196   train_acc = 0.5970149253731343   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.7828024625778198   train_acc = 0.664179104477612   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.6220148801803589   train_acc = 0.6268656716417911   val_acc = 0.2857142857142857\n",
      "iteration: 8/9   loss = 0.606101393699646   train_acc = 0.6119402985074627   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.5794128775596619   train_acc = 0.5895522388059702   val_acc = 0.5\n",
      "Final val acc for lr=0.00027082899806832295, wd=0.0004609389889268166: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.00027082899806832295, wd=0.0004609389889268166, betas=(0.95, 0.98)\n",
      "iteration: 1/9   loss = 0.7558040618896484   train_acc = 0.5223880597014925   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.6426905393600464   train_acc = 0.6268656716417911   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.7450963854789734   train_acc = 0.664179104477612   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.6416056156158447   train_acc = 0.7014925373134329   val_acc = 0.35714285714285715\n",
      "iteration: 5/9   loss = 0.6852854490280151   train_acc = 0.6716417910447762   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.5555497407913208   train_acc = 0.7238805970149254   val_acc = 0.2857142857142857\n",
      "iteration: 7/9   loss = 0.6634038686752319   train_acc = 0.7313432835820896   val_acc = 0.2857142857142857\n",
      "iteration: 8/9   loss = 0.6110895872116089   train_acc = 0.6940298507462687   val_acc = 0.42857142857142855\n",
      "iteration: 9/9   loss = 0.7679874300956726   train_acc = 0.7313432835820896   val_acc = 0.2857142857142857\n",
      "Final val acc for lr=0.00027082899806832295, wd=0.0004609389889268166: 0.2857\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.00027082899806832295, wd=0.0004609389889268166, betas=(0.95, 0.99)\n",
      "iteration: 1/9   loss = 0.708501935005188   train_acc = 0.4925373134328358   val_acc = 0.5714285714285714\n",
      "iteration: 2/9   loss = 0.7152697443962097   train_acc = 0.6044776119402985   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.7300529479980469   train_acc = 0.6716417910447762   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.7090410590171814   train_acc = 0.6119402985074627   val_acc = 0.5714285714285714\n",
      "iteration: 5/9   loss = 0.6369739174842834   train_acc = 0.582089552238806   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.49688100814819336   train_acc = 0.5895522388059702   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.6750771999359131   train_acc = 0.582089552238806   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7021700739860535   train_acc = 0.582089552238806   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.9094228148460388   train_acc = 0.6119402985074627   val_acc = 0.5\n",
      "Final val acc for lr=0.00027082899806832295, wd=0.0004609389889268166: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.00027082899806832295, wd=0.0004609389889268166, betas=(0.95, 0.995)\n",
      "iteration: 1/9   loss = 0.7030869126319885   train_acc = 0.47761194029850745   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.7716904878616333   train_acc = 0.5895522388059702   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.6428337097167969   train_acc = 0.6268656716417911   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.674014687538147   train_acc = 0.5895522388059702   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.6319930553436279   train_acc = 0.6044776119402985   val_acc = 0.5714285714285714\n",
      "iteration: 6/9   loss = 0.6245858073234558   train_acc = 0.6044776119402985   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.691443920135498   train_acc = 0.664179104477612   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.6112123727798462   train_acc = 0.6567164179104478   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.6849913597106934   train_acc = 0.7014925373134329   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=0.00027082899806832295, wd=0.0004609389889268166: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.00027082899806832295, wd=0.0004609389889268166, betas=(0.95, 0.999)\n",
      "iteration: 1/9   loss = 0.7252458333969116   train_acc = 0.6119402985074627   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.6669957041740417   train_acc = 0.6268656716417911   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.6853584051132202   train_acc = 0.6268656716417911   val_acc = 0.6428571428571429\n",
      "iteration: 4/9   loss = 0.6372105479240417   train_acc = 0.6492537313432836   val_acc = 0.5714285714285714\n",
      "iteration: 5/9   loss = 0.6942859888076782   train_acc = 0.6343283582089553   val_acc = 0.6428571428571429\n",
      "iteration: 6/9   loss = 0.6878067255020142   train_acc = 0.6343283582089553   val_acc = 0.6428571428571429\n",
      "iteration: 7/9   loss = 0.6154496669769287   train_acc = 0.6865671641791045   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.6253832578659058   train_acc = 0.6940298507462687   val_acc = 0.35714285714285715\n",
      "iteration: 9/9   loss = 0.7856258153915405   train_acc = 0.6716417910447762   val_acc = 0.6428571428571429\n",
      "Final val acc for lr=0.00027082899806832295, wd=0.0004609389889268166: 0.6429\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.00027082899806832295, wd=0.0002300292096821906, betas=(0.85, 0.99)\n",
      "iteration: 1/9   loss = 0.6989369988441467   train_acc = 0.5223880597014925   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.7436746954917908   train_acc = 0.582089552238806   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.7006735801696777   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7191402912139893   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7285382747650146   train_acc = 0.5671641791044776   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.6232532858848572   train_acc = 0.5746268656716418   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.6577218770980835   train_acc = 0.6044776119402985   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.6688017845153809   train_acc = 0.6716417910447762   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.5424599647521973   train_acc = 0.6791044776119403   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=0.00027082899806832295, wd=0.0002300292096821906: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.00027082899806832295, wd=0.0002300292096821906, betas=(0.85, 0.995)\n",
      "iteration: 1/9   loss = 0.7252028584480286   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7089530825614929   train_acc = 0.6343283582089553   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.7806708216667175   train_acc = 0.6791044776119403   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.6242831945419312   train_acc = 0.6791044776119403   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.6306220889091492   train_acc = 0.6343283582089553   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.7721945643424988   train_acc = 0.6194029850746269   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.6333038210868835   train_acc = 0.6343283582089553   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.6769686937332153   train_acc = 0.6791044776119403   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.5523903369903564   train_acc = 0.7014925373134329   val_acc = 0.5714285714285714\n",
      "Final val acc for lr=0.00027082899806832295, wd=0.0002300292096821906: 0.5714\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.00027082899806832295, wd=0.0002300292096821906, betas=(0.9, 0.98)\n",
      "iteration: 1/9   loss = 0.707344651222229   train_acc = 0.5373134328358209   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.723782479763031   train_acc = 0.6791044776119403   val_acc = 0.35714285714285715\n",
      "iteration: 3/9   loss = 0.5770126581192017   train_acc = 0.6044776119402985   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.7228911519050598   train_acc = 0.582089552238806   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.6864255666732788   train_acc = 0.5895522388059702   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.5728098154067993   train_acc = 0.6268656716417911   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.6616356372833252   train_acc = 0.664179104477612   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.6146637201309204   train_acc = 0.6865671641791045   val_acc = 0.2857142857142857\n",
      "iteration: 9/9   loss = 0.7237650156021118   train_acc = 0.664179104477612   val_acc = 0.35714285714285715\n",
      "Final val acc for lr=0.00027082899806832295, wd=0.0002300292096821906: 0.3571\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.00027082899806832295, wd=0.0002300292096821906, betas=(0.9, 0.99)\n",
      "iteration: 1/9   loss = 0.7422261834144592   train_acc = 0.5746268656716418   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.7059698104858398   train_acc = 0.5970149253731343   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.7573531866073608   train_acc = 0.582089552238806   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.6921675801277161   train_acc = 0.6044776119402985   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.6243118047714233   train_acc = 0.6417910447761194   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.5379773378372192   train_acc = 0.7089552238805971   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.6655776500701904   train_acc = 0.664179104477612   val_acc = 0.35714285714285715\n",
      "iteration: 8/9   loss = 0.595894455909729   train_acc = 0.6417910447761194   val_acc = 0.2857142857142857\n",
      "iteration: 9/9   loss = 0.6651526689529419   train_acc = 0.6417910447761194   val_acc = 0.35714285714285715\n",
      "Final val acc for lr=0.00027082899806832295, wd=0.0002300292096821906: 0.3571\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.00027082899806832295, wd=0.0002300292096821906, betas=(0.9, 0.995)\n",
      "iteration: 1/9   loss = 0.7272817492485046   train_acc = 0.5447761194029851   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.7215298414230347   train_acc = 0.5223880597014925   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.7030397653579712   train_acc = 0.6417910447761194   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.6100298166275024   train_acc = 0.6268656716417911   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.6785194277763367   train_acc = 0.6268656716417911   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.8369995951652527   train_acc = 0.6716417910447762   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.6732292175292969   train_acc = 0.6417910447761194   val_acc = 0.2857142857142857\n",
      "iteration: 8/9   loss = 0.629025399684906   train_acc = 0.6417910447761194   val_acc = 0.2857142857142857\n",
      "iteration: 9/9   loss = 0.8472312688827515   train_acc = 0.6417910447761194   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=0.00027082899806832295, wd=0.0002300292096821906: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.00027082899806832295, wd=0.0002300292096821906, betas=(0.9, 0.999)\n",
      "iteration: 1/9   loss = 0.7244576215744019   train_acc = 0.5671641791044776   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.728284478187561   train_acc = 0.5597014925373134   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.9433320760726929   train_acc = 0.5895522388059702   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.6916024088859558   train_acc = 0.6119402985074627   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.6641111969947815   train_acc = 0.6119402985074627   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.7733418941497803   train_acc = 0.753731343283582   val_acc = 0.21428571428571427\n",
      "iteration: 7/9   loss = 0.6656516194343567   train_acc = 0.7238805970149254   val_acc = 0.2857142857142857\n",
      "iteration: 8/9   loss = 0.6416404247283936   train_acc = 0.6791044776119403   val_acc = 0.2857142857142857\n",
      "iteration: 9/9   loss = 0.6636136174201965   train_acc = 0.6343283582089553   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=0.00027082899806832295, wd=0.0002300292096821906: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.00027082899806832295, wd=0.0002300292096821906, betas=(0.95, 0.98)\n",
      "iteration: 1/9   loss = 0.7029786705970764   train_acc = 0.582089552238806   val_acc = 0.35714285714285715\n",
      "iteration: 2/9   loss = 0.7204143404960632   train_acc = 0.7164179104477612   val_acc = 0.5714285714285714\n",
      "iteration: 3/9   loss = 0.7012874484062195   train_acc = 0.5895522388059702   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.6722899675369263   train_acc = 0.582089552238806   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.6831872463226318   train_acc = 0.5895522388059702   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.5979095697402954   train_acc = 0.5970149253731343   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.6467064619064331   train_acc = 0.7014925373134329   val_acc = 0.35714285714285715\n",
      "iteration: 8/9   loss = 0.6544926166534424   train_acc = 0.6940298507462687   val_acc = 0.6428571428571429\n",
      "iteration: 9/9   loss = 0.5907509326934814   train_acc = 0.664179104477612   val_acc = 0.5\n",
      "Final val acc for lr=0.00027082899806832295, wd=0.0002300292096821906: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.00027082899806832295, wd=0.0002300292096821906, betas=(0.95, 0.99)\n",
      "iteration: 1/9   loss = 0.6712521910667419   train_acc = 0.5223880597014925   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7388709783554077   train_acc = 0.6194029850746269   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.7635828256607056   train_acc = 0.6343283582089553   val_acc = 0.35714285714285715\n",
      "iteration: 4/9   loss = 0.6601938009262085   train_acc = 0.6343283582089553   val_acc = 0.35714285714285715\n",
      "iteration: 5/9   loss = 0.6714621186256409   train_acc = 0.664179104477612   val_acc = 0.2857142857142857\n",
      "iteration: 6/9   loss = 0.7285282611846924   train_acc = 0.6417910447761194   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.6228526830673218   train_acc = 0.6343283582089553   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.632534384727478   train_acc = 0.6268656716417911   val_acc = 0.42857142857142855\n",
      "iteration: 9/9   loss = 0.6806725859642029   train_acc = 0.6492537313432836   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=0.00027082899806832295, wd=0.0002300292096821906: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.00027082899806832295, wd=0.0002300292096821906, betas=(0.95, 0.995)\n",
      "iteration: 1/9   loss = 0.7108824253082275   train_acc = 0.6343283582089553   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.6999484896659851   train_acc = 0.6268656716417911   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.7612902522087097   train_acc = 0.6417910447761194   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.6724105477333069   train_acc = 0.5895522388059702   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.6786115765571594   train_acc = 0.6268656716417911   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.5740445852279663   train_acc = 0.6119402985074627   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.6844226717948914   train_acc = 0.6194029850746269   val_acc = 0.35714285714285715\n",
      "iteration: 8/9   loss = 0.6383177042007446   train_acc = 0.6417910447761194   val_acc = 0.42857142857142855\n",
      "iteration: 9/9   loss = 0.6838594675064087   train_acc = 0.582089552238806   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=0.00027082899806832295, wd=0.0002300292096821906: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.00027082899806832295, wd=0.0002300292096821906, betas=(0.95, 0.999)\n",
      "iteration: 1/9   loss = 0.7193416953086853   train_acc = 0.5671641791044776   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.6881066560745239   train_acc = 0.6343283582089553   val_acc = 0.5714285714285714\n",
      "iteration: 3/9   loss = 0.7197847366333008   train_acc = 0.6567164179104478   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.5855278372764587   train_acc = 0.7388059701492538   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.6691217422485352   train_acc = 0.6791044776119403   val_acc = 0.5714285714285714\n",
      "iteration: 6/9   loss = 0.5405271053314209   train_acc = 0.6940298507462687   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.6102031469345093   train_acc = 0.6343283582089553   val_acc = 0.35714285714285715\n",
      "iteration: 8/9   loss = 0.5971503257751465   train_acc = 0.6567164179104478   val_acc = 0.35714285714285715\n",
      "iteration: 9/9   loss = 0.6277866363525391   train_acc = 0.6716417910447762   val_acc = 0.35714285714285715\n",
      "Final val acc for lr=0.00027082899806832295, wd=0.0002300292096821906: 0.3571\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.00027082899806832295, wd=0.00022555743977178082, betas=(0.85, 0.99)\n",
      "iteration: 1/9   loss = 0.745048999786377   train_acc = 0.5597014925373134   val_acc = 0.2857142857142857\n",
      "iteration: 2/9   loss = 0.7311505079269409   train_acc = 0.6194029850746269   val_acc = 0.2857142857142857\n",
      "iteration: 3/9   loss = 0.5670275688171387   train_acc = 0.5895522388059702   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.6521726250648499   train_acc = 0.582089552238806   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.6567473411560059   train_acc = 0.664179104477612   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.5513792037963867   train_acc = 0.6417910447761194   val_acc = 0.2857142857142857\n",
      "iteration: 7/9   loss = 0.6754626035690308   train_acc = 0.6940298507462687   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.5876320600509644   train_acc = 0.7014925373134329   val_acc = 0.42857142857142855\n",
      "iteration: 9/9   loss = 0.9334664344787598   train_acc = 0.6716417910447762   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=0.00027082899806832295, wd=0.00022555743977178082: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.00027082899806832295, wd=0.00022555743977178082, betas=(0.85, 0.995)\n",
      "iteration: 1/9   loss = 0.7190226316452026   train_acc = 0.5298507462686567   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7892028093338013   train_acc = 0.582089552238806   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.6069577932357788   train_acc = 0.6194029850746269   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.6780821084976196   train_acc = 0.6343283582089553   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.6383740305900574   train_acc = 0.6940298507462687   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.675920844078064   train_acc = 0.6940298507462687   val_acc = 0.6428571428571429\n",
      "iteration: 7/9   loss = 0.620455801486969   train_acc = 0.7014925373134329   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.6448579430580139   train_acc = 0.7164179104477612   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.6803536415100098   train_acc = 0.7014925373134329   val_acc = 0.35714285714285715\n",
      "Final val acc for lr=0.00027082899806832295, wd=0.00022555743977178082: 0.3571\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.00027082899806832295, wd=0.00022555743977178082, betas=(0.9, 0.98)\n",
      "iteration: 1/9   loss = 0.7469466924667358   train_acc = 0.5746268656716418   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.7081001996994019   train_acc = 0.5746268656716418   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.6217347383499146   train_acc = 0.582089552238806   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.6677958369255066   train_acc = 0.6417910447761194   val_acc = 0.35714285714285715\n",
      "iteration: 5/9   loss = 0.6664886474609375   train_acc = 0.7238805970149254   val_acc = 0.21428571428571427\n",
      "iteration: 6/9   loss = 0.6048476099967957   train_acc = 0.6343283582089553   val_acc = 0.2857142857142857\n",
      "iteration: 7/9   loss = 0.6718642711639404   train_acc = 0.5746268656716418   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.6778564453125   train_acc = 0.5746268656716418   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.6283180713653564   train_acc = 0.5895522388059702   val_acc = 0.5\n",
      "Final val acc for lr=0.00027082899806832295, wd=0.00022555743977178082: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.00027082899806832295, wd=0.00022555743977178082, betas=(0.9, 0.99)\n",
      "iteration: 1/9   loss = 0.7417580485343933   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.6988315582275391   train_acc = 0.582089552238806   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.5825470685958862   train_acc = 0.5746268656716418   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7017219066619873   train_acc = 0.5671641791044776   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.676550030708313   train_acc = 0.6119402985074627   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.6392824053764343   train_acc = 0.5970149253731343   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.6809594631195068   train_acc = 0.6417910447761194   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.6331719756126404   train_acc = 0.7238805970149254   val_acc = 0.35714285714285715\n",
      "iteration: 9/9   loss = 0.5341004729270935   train_acc = 0.7014925373134329   val_acc = 0.5\n",
      "Final val acc for lr=0.00027082899806832295, wd=0.00022555743977178082: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.00027082899806832295, wd=0.00022555743977178082, betas=(0.9, 0.995)\n",
      "iteration: 1/9   loss = 0.7733602523803711   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7736670970916748   train_acc = 0.5298507462686567   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.7471483945846558   train_acc = 0.5746268656716418   val_acc = 0.35714285714285715\n",
      "iteration: 4/9   loss = 0.6269347667694092   train_acc = 0.6119402985074627   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.6499431729316711   train_acc = 0.5746268656716418   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.6363452672958374   train_acc = 0.5298507462686567   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.6763798594474792   train_acc = 0.5373134328358209   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.7294345498085022   train_acc = 0.582089552238806   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.6285365223884583   train_acc = 0.5895522388059702   val_acc = 0.5\n",
      "Final val acc for lr=0.00027082899806832295, wd=0.00022555743977178082: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.00027082899806832295, wd=0.00022555743977178082, betas=(0.9, 0.999)\n",
      "iteration: 1/9   loss = 0.6712459921836853   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.8274680376052856   train_acc = 0.6119402985074627   val_acc = 0.6428571428571429\n",
      "iteration: 3/9   loss = 0.6307727098464966   train_acc = 0.6268656716417911   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.6528664827346802   train_acc = 0.5895522388059702   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7093589901924133   train_acc = 0.5746268656716418   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.6194536685943604   train_acc = 0.5746268656716418   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.6953735947608948   train_acc = 0.5746268656716418   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.6638625264167786   train_acc = 0.582089552238806   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.5657916069030762   train_acc = 0.6492537313432836   val_acc = 0.5714285714285714\n",
      "Final val acc for lr=0.00027082899806832295, wd=0.00022555743977178082: 0.5714\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.00027082899806832295, wd=0.00022555743977178082, betas=(0.95, 0.98)\n",
      "iteration: 1/9   loss = 0.7684253454208374   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7167825102806091   train_acc = 0.5597014925373134   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.5082084536552429   train_acc = 0.5746268656716418   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.6418981552124023   train_acc = 0.5746268656716418   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.670973539352417   train_acc = 0.5970149253731343   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.6846134066581726   train_acc = 0.6940298507462687   val_acc = 0.35714285714285715\n",
      "iteration: 7/9   loss = 0.6596141457557678   train_acc = 0.6940298507462687   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.59820556640625   train_acc = 0.664179104477612   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.30854636430740356   train_acc = 0.6940298507462687   val_acc = 0.6428571428571429\n",
      "Final val acc for lr=0.00027082899806832295, wd=0.00022555743977178082: 0.6429\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.00027082899806832295, wd=0.00022555743977178082, betas=(0.95, 0.99)\n",
      "iteration: 1/9   loss = 0.7196017503738403   train_acc = 0.5746268656716418   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.6978611946105957   train_acc = 0.6044776119402985   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.6868494749069214   train_acc = 0.6044776119402985   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.6393646001815796   train_acc = 0.6194029850746269   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.67520672082901   train_acc = 0.6194029850746269   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.49364060163497925   train_acc = 0.6343283582089553   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.6731096506118774   train_acc = 0.6567164179104478   val_acc = 0.2857142857142857\n",
      "iteration: 8/9   loss = 0.6287662386894226   train_acc = 0.6716417910447762   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.6052366495132446   train_acc = 0.6343283582089553   val_acc = 0.35714285714285715\n",
      "Final val acc for lr=0.00027082899806832295, wd=0.00022555743977178082: 0.3571\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.00027082899806832295, wd=0.00022555743977178082, betas=(0.95, 0.995)\n",
      "iteration: 1/9   loss = 0.6999318599700928   train_acc = 0.6417910447761194   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.6791029572486877   train_acc = 0.6567164179104478   val_acc = 0.35714285714285715\n",
      "iteration: 3/9   loss = 0.5333406925201416   train_acc = 0.6940298507462687   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.6407615542411804   train_acc = 0.753731343283582   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.6358159780502319   train_acc = 0.6791044776119403   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.661712110042572   train_acc = 0.6791044776119403   val_acc = 0.35714285714285715\n",
      "iteration: 7/9   loss = 0.6004161834716797   train_acc = 0.6268656716417911   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.7117839455604553   train_acc = 0.6567164179104478   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.9376580715179443   train_acc = 0.6865671641791045   val_acc = 0.35714285714285715\n",
      "Final val acc for lr=0.00027082899806832295, wd=0.00022555743977178082: 0.3571\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.00027082899806832295, wd=0.00022555743977178082, betas=(0.95, 0.999)\n",
      "iteration: 1/9   loss = 0.6948904991149902   train_acc = 0.6044776119402985   val_acc = 0.35714285714285715\n",
      "iteration: 2/9   loss = 0.7198889255523682   train_acc = 0.5522388059701493   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.874802827835083   train_acc = 0.6044776119402985   val_acc = 0.6428571428571429\n",
      "iteration: 4/9   loss = 0.6516753435134888   train_acc = 0.6417910447761194   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.6805646419525146   train_acc = 0.5746268656716418   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.6070762872695923   train_acc = 0.5970149253731343   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.6464228630065918   train_acc = 0.6119402985074627   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.6474182605743408   train_acc = 0.6492537313432836   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.6492786407470703   train_acc = 0.7164179104477612   val_acc = 0.35714285714285715\n",
      "Final val acc for lr=0.00027082899806832295, wd=0.00022555743977178082: 0.3571\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.00027082899806832295, wd=0.00011822143451838108, betas=(0.85, 0.99)\n",
      "iteration: 1/9   loss = 0.7081127166748047   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7244913578033447   train_acc = 0.582089552238806   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.7463030815124512   train_acc = 0.6567164179104478   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.6571221351623535   train_acc = 0.6268656716417911   val_acc = 0.6428571428571429\n",
      "iteration: 5/9   loss = 0.629982590675354   train_acc = 0.6865671641791045   val_acc = 0.7857142857142857\n",
      "iteration: 6/9   loss = 0.8201659917831421   train_acc = 0.6268656716417911   val_acc = 0.6428571428571429\n",
      "iteration: 7/9   loss = 0.6633272171020508   train_acc = 0.6268656716417911   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.632956862449646   train_acc = 0.5671641791044776   val_acc = 0.6428571428571429\n",
      "iteration: 9/9   loss = 0.7116207480430603   train_acc = 0.5970149253731343   val_acc = 0.5\n",
      "Final val acc for lr=0.00027082899806832295, wd=0.00011822143451838108: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.00027082899806832295, wd=0.00011822143451838108, betas=(0.85, 0.995)\n",
      "iteration: 1/9   loss = 0.744709849357605   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.720110297203064   train_acc = 0.6119402985074627   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.6551081538200378   train_acc = 0.6492537313432836   val_acc = 0.7142857142857143\n",
      "iteration: 4/9   loss = 0.6912143230438232   train_acc = 0.6417910447761194   val_acc = 0.6428571428571429\n",
      "iteration: 5/9   loss = 0.6319608688354492   train_acc = 0.6492537313432836   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.5452473163604736   train_acc = 0.6268656716417911   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.637481689453125   train_acc = 0.6417910447761194   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.6343705058097839   train_acc = 0.7089552238805971   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.6115626096725464   train_acc = 0.7238805970149254   val_acc = 0.6428571428571429\n",
      "Final val acc for lr=0.00027082899806832295, wd=0.00011822143451838108: 0.6429\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.00027082899806832295, wd=0.00011822143451838108, betas=(0.9, 0.98)\n",
      "iteration: 1/9   loss = 0.7581377029418945   train_acc = 0.6119402985074627   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.698943555355072   train_acc = 0.6567164179104478   val_acc = 0.2857142857142857\n",
      "iteration: 3/9   loss = 0.7102149724960327   train_acc = 0.6940298507462687   val_acc = 0.2857142857142857\n",
      "iteration: 4/9   loss = 0.6364694237709045   train_acc = 0.6865671641791045   val_acc = 0.5714285714285714\n",
      "iteration: 5/9   loss = 0.6776127219200134   train_acc = 0.7238805970149254   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.6228753924369812   train_acc = 0.6940298507462687   val_acc = 0.6428571428571429\n",
      "iteration: 7/9   loss = 0.5952572226524353   train_acc = 0.6940298507462687   val_acc = 0.6428571428571429\n",
      "iteration: 8/9   loss = 0.659794807434082   train_acc = 0.6865671641791045   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.6394888162612915   train_acc = 0.6417910447761194   val_acc = 0.7142857142857143\n",
      "Final val acc for lr=0.00027082899806832295, wd=0.00011822143451838108: 0.7143\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.00027082899806832295, wd=0.00011822143451838108, betas=(0.9, 0.99)\n",
      "iteration: 1/9   loss = 0.6812964081764221   train_acc = 0.5597014925373134   val_acc = 0.6428571428571429\n",
      "iteration: 2/9   loss = 0.7051916122436523   train_acc = 0.6417910447761194   val_acc = 0.5714285714285714\n",
      "iteration: 3/9   loss = 0.7489389777183533   train_acc = 0.5746268656716418   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.6982654333114624   train_acc = 0.5522388059701493   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.6619847416877747   train_acc = 0.5746268656716418   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.9118671417236328   train_acc = 0.5895522388059702   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.6802878379821777   train_acc = 0.6343283582089553   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.6336286664009094   train_acc = 0.6940298507462687   val_acc = 0.21428571428571427\n",
      "iteration: 9/9   loss = 0.6035137176513672   train_acc = 0.6194029850746269   val_acc = 0.5\n",
      "Final val acc for lr=0.00027082899806832295, wd=0.00011822143451838108: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.00027082899806832295, wd=0.00011822143451838108, betas=(0.9, 0.995)\n",
      "iteration: 1/9   loss = 0.7355265021324158   train_acc = 0.6343283582089553   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.7105231881141663   train_acc = 0.6119402985074627   val_acc = 0.7142857142857143\n",
      "iteration: 3/9   loss = 0.6725376844406128   train_acc = 0.6119402985074627   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.6710420846939087   train_acc = 0.582089552238806   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.6782780885696411   train_acc = 0.6044776119402985   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.8253268599510193   train_acc = 0.6567164179104478   val_acc = 0.2857142857142857\n",
      "iteration: 7/9   loss = 0.6514604091644287   train_acc = 0.6791044776119403   val_acc = 0.35714285714285715\n",
      "iteration: 8/9   loss = 0.6345464587211609   train_acc = 0.6492537313432836   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.6766924262046814   train_acc = 0.6492537313432836   val_acc = 0.5\n",
      "Final val acc for lr=0.00027082899806832295, wd=0.00011822143451838108: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.00027082899806832295, wd=0.00011822143451838108, betas=(0.9, 0.999)\n",
      "iteration: 1/9   loss = 0.8218771815299988   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.8025791645050049   train_acc = 0.5298507462686567   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.5980618000030518   train_acc = 0.5746268656716418   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.6671275496482849   train_acc = 0.6417910447761194   val_acc = 0.5714285714285714\n",
      "iteration: 5/9   loss = 0.6699753999710083   train_acc = 0.664179104477612   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.7943381071090698   train_acc = 0.6343283582089553   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.6708412170410156   train_acc = 0.6940298507462687   val_acc = 0.7142857142857143\n",
      "iteration: 8/9   loss = 0.6580619812011719   train_acc = 0.6791044776119403   val_acc = 0.6428571428571429\n",
      "iteration: 9/9   loss = 0.749751091003418   train_acc = 0.6791044776119403   val_acc = 0.5714285714285714\n",
      "Final val acc for lr=0.00027082899806832295, wd=0.00011822143451838108: 0.5714\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.00027082899806832295, wd=0.00011822143451838108, betas=(0.95, 0.98)\n",
      "iteration: 1/9   loss = 0.7773186564445496   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.765365481376648   train_acc = 0.5447761194029851   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.6591038107872009   train_acc = 0.582089552238806   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.6782400608062744   train_acc = 0.582089552238806   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.6850540637969971   train_acc = 0.6268656716417911   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.6560503244400024   train_acc = 0.6716417910447762   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.6158039569854736   train_acc = 0.6791044776119403   val_acc = 0.5714285714285714\n",
      "iteration: 8/9   loss = 0.6554232835769653   train_acc = 0.6492537313432836   val_acc = 0.7857142857142857\n",
      "iteration: 9/9   loss = 0.7375931739807129   train_acc = 0.5970149253731343   val_acc = 0.6428571428571429\n",
      "Final val acc for lr=0.00027082899806832295, wd=0.00011822143451838108: 0.6429\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.00027082899806832295, wd=0.00011822143451838108, betas=(0.95, 0.99)\n",
      "iteration: 1/9   loss = 0.707095742225647   train_acc = 0.5522388059701493   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.7934050559997559   train_acc = 0.5447761194029851   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.8844113349914551   train_acc = 0.5970149253731343   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.6489419937133789   train_acc = 0.6343283582089553   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.6368873715400696   train_acc = 0.6492537313432836   val_acc = 0.5714285714285714\n",
      "iteration: 6/9   loss = 0.6844625473022461   train_acc = 0.5895522388059702   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.6574780941009521   train_acc = 0.5970149253731343   val_acc = 0.42857142857142855\n",
      "iteration: 8/9   loss = 0.6249319314956665   train_acc = 0.664179104477612   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.749377429485321   train_acc = 0.5970149253731343   val_acc = 0.5\n",
      "Final val acc for lr=0.00027082899806832295, wd=0.00011822143451838108: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.00027082899806832295, wd=0.00011822143451838108, betas=(0.95, 0.995)\n",
      "iteration: 1/9   loss = 0.7776541113853455   train_acc = 0.5149253731343284   val_acc = 0.2857142857142857\n",
      "iteration: 2/9   loss = 0.7518662214279175   train_acc = 0.5671641791044776   val_acc = 0.21428571428571427\n",
      "iteration: 3/9   loss = 0.5820479393005371   train_acc = 0.6194029850746269   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.6847966909408569   train_acc = 0.5671641791044776   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.6714608669281006   train_acc = 0.5746268656716418   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.5717889070510864   train_acc = 0.5746268656716418   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.6167677640914917   train_acc = 0.5671641791044776   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.6441060304641724   train_acc = 0.5746268656716418   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.7412563562393188   train_acc = 0.664179104477612   val_acc = 0.35714285714285715\n",
      "Final val acc for lr=0.00027082899806832295, wd=0.00011822143451838108: 0.3571\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.00027082899806832295, wd=0.00011822143451838108, betas=(0.95, 0.999)\n",
      "iteration: 1/9   loss = 0.6738076210021973   train_acc = 0.6492537313432836   val_acc = 0.7857142857142857\n",
      "iteration: 2/9   loss = 0.7361391186714172   train_acc = 0.7014925373134329   val_acc = 0.6428571428571429\n",
      "iteration: 3/9   loss = 0.8672420978546143   train_acc = 0.6716417910447762   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.6566486358642578   train_acc = 0.5895522388059702   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.6324908137321472   train_acc = 0.6417910447761194   val_acc = 0.6428571428571429\n",
      "iteration: 6/9   loss = 0.665940523147583   train_acc = 0.7089552238805971   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.6756106615066528   train_acc = 0.7089552238805971   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.5854929685592651   train_acc = 0.7089552238805971   val_acc = 0.35714285714285715\n",
      "iteration: 9/9   loss = 0.7302761077880859   train_acc = 0.6567164179104478   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=0.00027082899806832295, wd=0.00011822143451838108: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.00027082899806832295, wd=0.00032736198184168545, betas=(0.85, 0.99)\n",
      "iteration: 1/9   loss = 0.8176462054252625   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.951295793056488   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.7569968700408936   train_acc = 0.5970149253731343   val_acc = 0.35714285714285715\n",
      "iteration: 4/9   loss = 0.660215437412262   train_acc = 0.6417910447761194   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.6150000095367432   train_acc = 0.6194029850746269   val_acc = 0.5714285714285714\n",
      "iteration: 6/9   loss = 0.6737228631973267   train_acc = 0.6268656716417911   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.6396143436431885   train_acc = 0.6119402985074627   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.6278001070022583   train_acc = 0.6194029850746269   val_acc = 0.5714285714285714\n",
      "iteration: 9/9   loss = 0.8196702003479004   train_acc = 0.7313432835820896   val_acc = 0.5\n",
      "Final val acc for lr=0.00027082899806832295, wd=0.00032736198184168545: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.00027082899806832295, wd=0.00032736198184168545, betas=(0.85, 0.995)\n",
      "iteration: 1/9   loss = 0.8530513048171997   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7447724938392639   train_acc = 0.5447761194029851   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.6964237093925476   train_acc = 0.5447761194029851   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.7504888772964478   train_acc = 0.6044776119402985   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.6544308066368103   train_acc = 0.6268656716417911   val_acc = 0.35714285714285715\n",
      "iteration: 6/9   loss = 0.8059924840927124   train_acc = 0.5895522388059702   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.6386542320251465   train_acc = 0.582089552238806   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.6799404621124268   train_acc = 0.6044776119402985   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.6224271059036255   train_acc = 0.6119402985074627   val_acc = 0.5\n",
      "Final val acc for lr=0.00027082899806832295, wd=0.00032736198184168545: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.00027082899806832295, wd=0.00032736198184168545, betas=(0.9, 0.98)\n",
      "iteration: 1/9   loss = 0.7256280779838562   train_acc = 0.5970149253731343   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.7174588441848755   train_acc = 0.6716417910447762   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.6163477897644043   train_acc = 0.5447761194029851   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.6589363217353821   train_acc = 0.5597014925373134   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.7912981510162354   train_acc = 0.5746268656716418   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.6060836315155029   train_acc = 0.582089552238806   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.7185261845588684   train_acc = 0.5895522388059702   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.6355434060096741   train_acc = 0.664179104477612   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.6392748951911926   train_acc = 0.664179104477612   val_acc = 0.6428571428571429\n",
      "Final val acc for lr=0.00027082899806832295, wd=0.00032736198184168545: 0.6429\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.00027082899806832295, wd=0.00032736198184168545, betas=(0.9, 0.99)\n",
      "iteration: 1/9   loss = 0.7486518025398254   train_acc = 0.582089552238806   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.73845374584198   train_acc = 0.5746268656716418   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.9518941640853882   train_acc = 0.6417910447761194   val_acc = 0.35714285714285715\n",
      "iteration: 4/9   loss = 0.6433067321777344   train_acc = 0.6567164179104478   val_acc = 0.2857142857142857\n",
      "iteration: 5/9   loss = 0.6049122214317322   train_acc = 0.6194029850746269   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.6492934823036194   train_acc = 0.6044776119402985   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.6020783185958862   train_acc = 0.6119402985074627   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.664192795753479   train_acc = 0.6417910447761194   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.5749221444129944   train_acc = 0.6865671641791045   val_acc = 0.5\n",
      "Final val acc for lr=0.00027082899806832295, wd=0.00032736198184168545: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.00027082899806832295, wd=0.00032736198184168545, betas=(0.9, 0.995)\n",
      "iteration: 1/9   loss = 0.7620440125465393   train_acc = 0.6119402985074627   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.7875550985336304   train_acc = 0.6268656716417911   val_acc = 0.21428571428571427\n",
      "iteration: 3/9   loss = 0.7915592193603516   train_acc = 0.6268656716417911   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.6517902612686157   train_acc = 0.6343283582089553   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.6765040755271912   train_acc = 0.6865671641791045   val_acc = 0.35714285714285715\n",
      "iteration: 6/9   loss = 0.5734273195266724   train_acc = 0.6119402985074627   val_acc = 0.35714285714285715\n",
      "iteration: 7/9   loss = 0.6157962083816528   train_acc = 0.6268656716417911   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.6553107500076294   train_acc = 0.6044776119402985   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.7251481413841248   train_acc = 0.6194029850746269   val_acc = 0.5\n",
      "Final val acc for lr=0.00027082899806832295, wd=0.00032736198184168545: 0.5000\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.00027082899806832295, wd=0.00032736198184168545, betas=(0.9, 0.999)\n",
      "iteration: 1/9   loss = 0.690609335899353   train_acc = 0.5671641791044776   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.823399543762207   train_acc = 0.6044776119402985   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.6411454677581787   train_acc = 0.6492537313432836   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.6279712915420532   train_acc = 0.664179104477612   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.633110761642456   train_acc = 0.6567164179104478   val_acc = 0.2857142857142857\n",
      "iteration: 6/9   loss = 0.447922945022583   train_acc = 0.6417910447761194   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.6547315120697021   train_acc = 0.6716417910447762   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.5896538496017456   train_acc = 0.6492537313432836   val_acc = 0.6428571428571429\n",
      "iteration: 9/9   loss = 0.6865077018737793   train_acc = 0.6194029850746269   val_acc = 0.5714285714285714\n",
      "Final val acc for lr=0.00027082899806832295, wd=0.00032736198184168545: 0.5714\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.00027082899806832295, wd=0.00032736198184168545, betas=(0.95, 0.98)\n",
      "iteration: 1/9   loss = 0.7196117639541626   train_acc = 0.6194029850746269   val_acc = 0.6428571428571429\n",
      "iteration: 2/9   loss = 0.7110006809234619   train_acc = 0.5970149253731343   val_acc = 0.5\n",
      "iteration: 3/9   loss = 0.9628477096557617   train_acc = 0.6343283582089553   val_acc = 0.5714285714285714\n",
      "iteration: 4/9   loss = 0.5875346064567566   train_acc = 0.582089552238806   val_acc = 0.5714285714285714\n",
      "iteration: 5/9   loss = 0.7122892141342163   train_acc = 0.5895522388059702   val_acc = 0.5\n",
      "iteration: 6/9   loss = 1.1002662181854248   train_acc = 0.5970149253731343   val_acc = 0.5714285714285714\n",
      "iteration: 7/9   loss = 0.6611363887786865   train_acc = 0.5970149253731343   val_acc = 0.7142857142857143\n",
      "iteration: 8/9   loss = 0.6399720311164856   train_acc = 0.6268656716417911   val_acc = 0.35714285714285715\n",
      "iteration: 9/9   loss = 0.5826635956764221   train_acc = 0.6492537313432836   val_acc = 0.21428571428571427\n",
      "Final val acc for lr=0.00027082899806832295, wd=0.00032736198184168545: 0.2143\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.00027082899806832295, wd=0.00032736198184168545, betas=(0.95, 0.99)\n",
      "iteration: 1/9   loss = 0.7019327878952026   train_acc = 0.5746268656716418   val_acc = 0.35714285714285715\n",
      "iteration: 2/9   loss = 0.6868417263031006   train_acc = 0.6119402985074627   val_acc = 0.42857142857142855\n",
      "iteration: 3/9   loss = 0.788575291633606   train_acc = 0.5895522388059702   val_acc = 0.42857142857142855\n",
      "iteration: 4/9   loss = 0.6896035671234131   train_acc = 0.6119402985074627   val_acc = 0.35714285714285715\n",
      "iteration: 5/9   loss = 0.6609954237937927   train_acc = 0.6119402985074627   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.6694344282150269   train_acc = 0.6343283582089553   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.6481543779373169   train_acc = 0.6940298507462687   val_acc = 0.35714285714285715\n",
      "iteration: 8/9   loss = 0.6148710250854492   train_acc = 0.7388059701492538   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.6388182640075684   train_acc = 0.6791044776119403   val_acc = 0.42857142857142855\n",
      "Final val acc for lr=0.00027082899806832295, wd=0.00032736198184168545: 0.4286\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.00027082899806832295, wd=0.00032736198184168545, betas=(0.95, 0.995)\n",
      "iteration: 1/9   loss = 0.7084659337997437   train_acc = 0.6268656716417911   val_acc = 0.42857142857142855\n",
      "iteration: 2/9   loss = 0.6820880174636841   train_acc = 0.6492537313432836   val_acc = 0.5714285714285714\n",
      "iteration: 3/9   loss = 0.6572617292404175   train_acc = 0.5746268656716418   val_acc = 0.5\n",
      "iteration: 4/9   loss = 0.6408418416976929   train_acc = 0.6343283582089553   val_acc = 0.5\n",
      "iteration: 5/9   loss = 0.6787019968032837   train_acc = 0.7089552238805971   val_acc = 0.5\n",
      "iteration: 6/9   loss = 0.8092954158782959   train_acc = 0.6268656716417911   val_acc = 0.5\n",
      "iteration: 7/9   loss = 0.6587226390838623   train_acc = 0.6343283582089553   val_acc = 0.5\n",
      "iteration: 8/9   loss = 0.6033352613449097   train_acc = 0.6119402985074627   val_acc = 0.5\n",
      "iteration: 9/9   loss = 0.6046411991119385   train_acc = 0.6940298507462687   val_acc = 0.21428571428571427\n",
      "Final val acc for lr=0.00027082899806832295, wd=0.00032736198184168545: 0.2143\n",
      "\n",
      "ðŸ”§ Testing AdamW: lr=0.00027082899806832295, wd=0.00032736198184168545, betas=(0.95, 0.999)\n",
      "iteration: 1/9   loss = 0.7489635944366455   train_acc = 0.5447761194029851   val_acc = 0.5\n",
      "iteration: 2/9   loss = 0.692916750907898   train_acc = 0.5970149253731343   val_acc = 0.35714285714285715\n",
      "iteration: 3/9   loss = 0.6233708262443542   train_acc = 0.6194029850746269   val_acc = 0.35714285714285715\n",
      "iteration: 4/9   loss = 0.6143518090248108   train_acc = 0.6268656716417911   val_acc = 0.42857142857142855\n",
      "iteration: 5/9   loss = 0.6649495363235474   train_acc = 0.664179104477612   val_acc = 0.42857142857142855\n",
      "iteration: 6/9   loss = 0.707088828086853   train_acc = 0.7089552238805971   val_acc = 0.42857142857142855\n",
      "iteration: 7/9   loss = 0.5932484865188599   train_acc = 0.664179104477612   val_acc = 0.7142857142857143\n",
      "iteration: 8/9   loss = 0.6274592876434326   train_acc = 0.6567164179104478   val_acc = 0.42857142857142855\n",
      "iteration: 9/9   loss = 0.6171059608459473   train_acc = 0.6417910447761194   val_acc = 0.5\n",
      "Final val acc for lr=0.00027082899806832295, wd=0.00032736198184168545: 0.5000\n",
      "\n",
      " Best Hyperparameters:\n",
      "{'lr': 1.515914167041141e-06, 'weight_decay': 0.0002300292096821906}\n",
      "Best val acc: 0.8571\n",
      " Saved best model to best_dino_model.pt\n"
     ]
    }
   ],
   "source": [
    "best_model, best_params, best_training_data = hyperparameter_search(DinoClassifier, loader_train, loader_val, device, opt=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5adb0446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample preds: [0.5196051597595215, 0.5701690912246704, 0.45812299847602844, 0.46519896388053894, 0.5586340427398682, 0.46486222743988037, 0.594819188117981, 0.40599325299263, 0.5451045036315918, 0.4947015047073364, 0.5711573362350464, 0.5781726837158203, 0.5510219931602478, 0.5724083185195923]\n",
      "Sample labels: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5714285714285714"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_accuracy_final(loader_test, best_model, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4daa63",
   "metadata": {},
   "source": [
    "Train best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7959c867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1/150   loss = 0.7057721018791199   train_acc = 0.5074626865671642   val_acc = 0.5714285714285714\n",
      "iteration: 2/150   loss = 0.7178230285644531   train_acc = 0.5149253731343284   val_acc = 0.5714285714285714\n",
      "iteration: 3/150   loss = 0.8218382596969604   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 4/150   loss = 0.7563289403915405   train_acc = 0.5   val_acc = 0.5714285714285714\n",
      "iteration: 5/150   loss = 0.6881057024002075   train_acc = 0.5223880597014925   val_acc = 0.5\n",
      "iteration: 6/150   loss = 0.6377691626548767   train_acc = 0.5074626865671642   val_acc = 0.5714285714285714\n",
      "iteration: 7/150   loss = 0.7016010284423828   train_acc = 0.5223880597014925   val_acc = 0.5\n",
      "iteration: 8/150   loss = 0.7544953227043152   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 9/150   loss = 0.604245126247406   train_acc = 0.5   val_acc = 0.5714285714285714\n",
      "iteration: 10/150   loss = 0.7466095685958862   train_acc = 0.5149253731343284   val_acc = 0.5714285714285714\n",
      "iteration: 11/150   loss = 0.7659311890602112   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 12/150   loss = 0.4080122411251068   train_acc = 0.5   val_acc = 0.5714285714285714\n",
      "iteration: 13/150   loss = 0.7102959156036377   train_acc = 0.5074626865671642   val_acc = 0.5714285714285714\n",
      "iteration: 14/150   loss = 0.7112255096435547   train_acc = 0.5074626865671642   val_acc = 0.5714285714285714\n",
      "iteration: 15/150   loss = 0.5931624174118042   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 16/150   loss = 0.707039475440979   train_acc = 0.5   val_acc = 0.6428571428571429\n",
      "iteration: 17/150   loss = 0.7311975955963135   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 18/150   loss = 0.8729026317596436   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 19/150   loss = 0.7287343144416809   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 20/150   loss = 0.6749476790428162   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 21/150   loss = 0.5489527583122253   train_acc = 0.5373134328358209   val_acc = 0.5714285714285714\n",
      "iteration: 22/150   loss = 0.6868646144866943   train_acc = 0.5223880597014925   val_acc = 0.5\n",
      "iteration: 23/150   loss = 0.7695693969726562   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 24/150   loss = 0.7044999003410339   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 25/150   loss = 0.7750072479248047   train_acc = 0.5298507462686567   val_acc = 0.42857142857142855\n",
      "iteration: 26/150   loss = 0.67205411195755   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 27/150   loss = 0.704062819480896   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 28/150   loss = 0.7178153991699219   train_acc = 0.5074626865671642   val_acc = 0.42857142857142855\n",
      "iteration: 29/150   loss = 0.7743121981620789   train_acc = 0.5223880597014925   val_acc = 0.42857142857142855\n",
      "iteration: 30/150   loss = 0.7539113759994507   train_acc = 0.5074626865671642   val_acc = 0.5714285714285714\n",
      "iteration: 31/150   loss = 0.7076285481452942   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 32/150   loss = 0.7483177185058594   train_acc = 0.4925373134328358   val_acc = 0.5\n",
      "iteration: 33/150   loss = 0.6879546046257019   train_acc = 0.5074626865671642   val_acc = 0.42857142857142855\n",
      "iteration: 34/150   loss = 0.6903187036514282   train_acc = 0.5   val_acc = 0.5714285714285714\n",
      "iteration: 35/150   loss = 0.7232340574264526   train_acc = 0.5   val_acc = 0.5714285714285714\n",
      "iteration: 36/150   loss = 0.6458662152290344   train_acc = 0.4925373134328358   val_acc = 0.42857142857142855\n",
      "iteration: 37/150   loss = 0.6843756437301636   train_acc = 0.4552238805970149   val_acc = 0.5714285714285714\n",
      "iteration: 38/150   loss = 0.6646596193313599   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 39/150   loss = 0.6746282577514648   train_acc = 0.48507462686567165   val_acc = 0.42857142857142855\n",
      "iteration: 40/150   loss = 0.731236457824707   train_acc = 0.5   val_acc = 0.5714285714285714\n",
      "iteration: 41/150   loss = 0.709587574005127   train_acc = 0.5   val_acc = 0.42857142857142855\n",
      "iteration: 42/150   loss = 0.6026057600975037   train_acc = 0.5373134328358209   val_acc = 0.5\n",
      "iteration: 43/150   loss = 0.7200719118118286   train_acc = 0.5074626865671642   val_acc = 0.5714285714285714\n",
      "iteration: 44/150   loss = 0.7481356859207153   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 45/150   loss = 0.7203439474105835   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 46/150   loss = 0.7487822771072388   train_acc = 0.5522388059701493   val_acc = 0.5\n",
      "iteration: 47/150   loss = 0.7320287227630615   train_acc = 0.5   val_acc = 0.5714285714285714\n",
      "iteration: 48/150   loss = 0.8176509737968445   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 49/150   loss = 0.6918961405754089   train_acc = 0.5074626865671642   val_acc = 0.42857142857142855\n",
      "iteration: 50/150   loss = 0.7385005354881287   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 51/150   loss = 0.676834762096405   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 52/150   loss = 0.7772934436798096   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 53/150   loss = 0.7524634599685669   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 54/150   loss = 0.6603154540061951   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 55/150   loss = 0.7041707038879395   train_acc = 0.48507462686567165   val_acc = 0.5714285714285714\n",
      "iteration: 56/150   loss = 0.7174286842346191   train_acc = 0.5223880597014925   val_acc = 0.42857142857142855\n",
      "iteration: 57/150   loss = 0.7124003171920776   train_acc = 0.5373134328358209   val_acc = 0.35714285714285715\n",
      "iteration: 58/150   loss = 0.704197347164154   train_acc = 0.5447761194029851   val_acc = 0.35714285714285715\n",
      "iteration: 59/150   loss = 0.6670043468475342   train_acc = 0.5074626865671642   val_acc = 0.42857142857142855\n",
      "iteration: 60/150   loss = 0.6401003003120422   train_acc = 0.5298507462686567   val_acc = 0.5\n",
      "iteration: 61/150   loss = 0.7056330442428589   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 62/150   loss = 0.7533087730407715   train_acc = 0.4925373134328358   val_acc = 0.42857142857142855\n",
      "iteration: 63/150   loss = 0.6568354368209839   train_acc = 0.5223880597014925   val_acc = 0.42857142857142855\n",
      "iteration: 64/150   loss = 0.7373330593109131   train_acc = 0.4925373134328358   val_acc = 0.42857142857142855\n",
      "iteration: 65/150   loss = 0.7098153829574585   train_acc = 0.5   val_acc = 0.42857142857142855\n",
      "iteration: 66/150   loss = 0.62821364402771   train_acc = 0.5522388059701493   val_acc = 0.35714285714285715\n",
      "iteration: 67/150   loss = 0.6934677362442017   train_acc = 0.5   val_acc = 0.35714285714285715\n",
      "iteration: 68/150   loss = 0.7471124529838562   train_acc = 0.5298507462686567   val_acc = 0.42857142857142855\n",
      "iteration: 69/150   loss = 0.6115817427635193   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 70/150   loss = 0.7314715385437012   train_acc = 0.48507462686567165   val_acc = 0.42857142857142855\n",
      "iteration: 71/150   loss = 0.7074906229972839   train_acc = 0.5   val_acc = 0.42857142857142855\n",
      "iteration: 72/150   loss = 0.6481715440750122   train_acc = 0.5   val_acc = 0.35714285714285715\n",
      "iteration: 73/150   loss = 0.6275274753570557   train_acc = 0.5   val_acc = 0.35714285714285715\n",
      "iteration: 74/150   loss = 0.7252148389816284   train_acc = 0.5074626865671642   val_acc = 0.2857142857142857\n",
      "iteration: 75/150   loss = 0.785772979259491   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 76/150   loss = 0.6671817898750305   train_acc = 0.4925373134328358   val_acc = 0.42857142857142855\n",
      "iteration: 77/150   loss = 0.7001963257789612   train_acc = 0.48507462686567165   val_acc = 0.35714285714285715\n",
      "iteration: 78/150   loss = 0.6675208806991577   train_acc = 0.5   val_acc = 0.2857142857142857\n",
      "iteration: 79/150   loss = 0.6769281625747681   train_acc = 0.5223880597014925   val_acc = 0.5\n",
      "iteration: 80/150   loss = 0.7151989936828613   train_acc = 0.5447761194029851   val_acc = 0.5714285714285714\n",
      "iteration: 81/150   loss = 0.7320052981376648   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 82/150   loss = 0.7287134528160095   train_acc = 0.5223880597014925   val_acc = 0.42857142857142855\n",
      "iteration: 83/150   loss = 0.7220731973648071   train_acc = 0.5   val_acc = 0.42857142857142855\n",
      "iteration: 84/150   loss = 0.8339666128158569   train_acc = 0.48507462686567165   val_acc = 0.42857142857142855\n",
      "iteration: 85/150   loss = 0.7514287233352661   train_acc = 0.48507462686567165   val_acc = 0.42857142857142855\n",
      "iteration: 86/150   loss = 0.7221361994743347   train_acc = 0.4925373134328358   val_acc = 0.35714285714285715\n",
      "iteration: 87/150   loss = 0.7990397810935974   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 88/150   loss = 0.7415037751197815   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 89/150   loss = 0.7070105075836182   train_acc = 0.5223880597014925   val_acc = 0.42857142857142855\n",
      "iteration: 90/150   loss = 0.5854824781417847   train_acc = 0.5   val_acc = 0.42857142857142855\n",
      "iteration: 91/150   loss = 0.6990984678268433   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 92/150   loss = 0.6776485443115234   train_acc = 0.5522388059701493   val_acc = 0.5\n",
      "iteration: 93/150   loss = 0.7161473035812378   train_acc = 0.48507462686567165   val_acc = 0.42857142857142855\n",
      "iteration: 94/150   loss = 0.706152081489563   train_acc = 0.5298507462686567   val_acc = 0.35714285714285715\n",
      "iteration: 95/150   loss = 0.6809667348861694   train_acc = 0.5223880597014925   val_acc = 0.35714285714285715\n",
      "iteration: 96/150   loss = 0.6836360692977905   train_acc = 0.4925373134328358   val_acc = 0.2857142857142857\n",
      "iteration: 97/150   loss = 0.7192480564117432   train_acc = 0.5223880597014925   val_acc = 0.42857142857142855\n",
      "iteration: 98/150   loss = 0.6803184747695923   train_acc = 0.5074626865671642   val_acc = 0.5714285714285714\n",
      "iteration: 99/150   loss = 0.6751556396484375   train_acc = 0.47761194029850745   val_acc = 0.5714285714285714\n",
      "iteration: 100/150   loss = 0.7777960896492004   train_acc = 0.5149253731343284   val_acc = 0.2857142857142857\n",
      "iteration: 101/150   loss = 0.6920197010040283   train_acc = 0.48507462686567165   val_acc = 0.5\n",
      "iteration: 102/150   loss = 0.7440268993377686   train_acc = 0.5149253731343284   val_acc = 0.5\n",
      "iteration: 103/150   loss = 0.727993369102478   train_acc = 0.5   val_acc = 0.35714285714285715\n",
      "iteration: 104/150   loss = 0.677065372467041   train_acc = 0.48507462686567165   val_acc = 0.42857142857142855\n",
      "iteration: 105/150   loss = 0.8457555770874023   train_acc = 0.5298507462686567   val_acc = 0.5\n",
      "iteration: 106/150   loss = 0.6604094505310059   train_acc = 0.5149253731343284   val_acc = 0.5714285714285714\n",
      "iteration: 107/150   loss = 0.6761970520019531   train_acc = 0.47761194029850745   val_acc = 0.35714285714285715\n",
      "iteration: 108/150   loss = 0.768964946269989   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 109/150   loss = 0.6973607540130615   train_acc = 0.5373134328358209   val_acc = 0.5\n",
      "iteration: 110/150   loss = 0.7097660899162292   train_acc = 0.5597014925373134   val_acc = 0.42857142857142855\n",
      "iteration: 111/150   loss = 0.7270700335502625   train_acc = 0.5   val_acc = 0.35714285714285715\n",
      "iteration: 112/150   loss = 0.7261561155319214   train_acc = 0.5447761194029851   val_acc = 0.35714285714285715\n",
      "iteration: 113/150   loss = 0.7054758667945862   train_acc = 0.5447761194029851   val_acc = 0.35714285714285715\n",
      "iteration: 114/150   loss = 0.6453236937522888   train_acc = 0.5447761194029851   val_acc = 0.42857142857142855\n",
      "iteration: 115/150   loss = 0.6852912902832031   train_acc = 0.48507462686567165   val_acc = 0.35714285714285715\n",
      "iteration: 116/150   loss = 0.6894757747650146   train_acc = 0.5373134328358209   val_acc = 0.5\n",
      "iteration: 117/150   loss = 0.6336377859115601   train_acc = 0.5223880597014925   val_acc = 0.5714285714285714\n",
      "iteration: 118/150   loss = 0.7276865243911743   train_acc = 0.5074626865671642   val_acc = 0.42857142857142855\n",
      "iteration: 119/150   loss = 0.6872683167457581   train_acc = 0.5074626865671642   val_acc = 0.5\n",
      "iteration: 120/150   loss = 0.7555662393569946   train_acc = 0.5671641791044776   val_acc = 0.5714285714285714\n",
      "iteration: 121/150   loss = 0.729618489742279   train_acc = 0.5223880597014925   val_acc = 0.42857142857142855\n",
      "iteration: 122/150   loss = 0.6796509027481079   train_acc = 0.4701492537313433   val_acc = 0.5\n",
      "iteration: 123/150   loss = 0.720653772354126   train_acc = 0.4925373134328358   val_acc = 0.42857142857142855\n",
      "iteration: 124/150   loss = 0.6647049188613892   train_acc = 0.5298507462686567   val_acc = 0.42857142857142855\n",
      "iteration: 125/150   loss = 0.7279123663902283   train_acc = 0.5149253731343284   val_acc = 0.35714285714285715\n",
      "iteration: 126/150   loss = 0.5337886810302734   train_acc = 0.5597014925373134   val_acc = 0.42857142857142855\n",
      "iteration: 127/150   loss = 0.7404329776763916   train_acc = 0.5149253731343284   val_acc = 0.42857142857142855\n",
      "iteration: 128/150   loss = 0.7173373699188232   train_acc = 0.5223880597014925   val_acc = 0.5\n",
      "iteration: 129/150   loss = 0.5900998115539551   train_acc = 0.582089552238806   val_acc = 0.42857142857142855\n",
      "iteration: 130/150   loss = 0.687859058380127   train_acc = 0.582089552238806   val_acc = 0.42857142857142855\n",
      "iteration: 131/150   loss = 0.6882133483886719   train_acc = 0.5746268656716418   val_acc = 0.2857142857142857\n",
      "iteration: 132/150   loss = 0.9237804412841797   train_acc = 0.5   val_acc = 0.5\n",
      "iteration: 133/150   loss = 0.6981596946716309   train_acc = 0.5149253731343284   val_acc = 0.42857142857142855\n",
      "iteration: 134/150   loss = 0.6788508892059326   train_acc = 0.5298507462686567   val_acc = 0.35714285714285715\n",
      "iteration: 135/150   loss = 0.625599205493927   train_acc = 0.5447761194029851   val_acc = 0.5\n",
      "iteration: 136/150   loss = 0.737492561340332   train_acc = 0.5895522388059702   val_acc = 0.5714285714285714\n",
      "iteration: 137/150   loss = 0.7310340404510498   train_acc = 0.5298507462686567   val_acc = 0.42857142857142855\n",
      "iteration: 138/150   loss = 0.6683962345123291   train_acc = 0.5373134328358209   val_acc = 0.5\n",
      "iteration: 139/150   loss = 0.7230347394943237   train_acc = 0.5298507462686567   val_acc = 0.5\n",
      "iteration: 140/150   loss = 0.6895348429679871   train_acc = 0.5149253731343284   val_acc = 0.35714285714285715\n",
      "iteration: 141/150   loss = 0.7455140352249146   train_acc = 0.5074626865671642   val_acc = 0.42857142857142855\n",
      "iteration: 142/150   loss = 0.6690747141838074   train_acc = 0.5373134328358209   val_acc = 0.35714285714285715\n",
      "iteration: 143/150   loss = 0.6814107894897461   train_acc = 0.5597014925373134   val_acc = 0.5714285714285714\n",
      "iteration: 144/150   loss = 0.6585130095481873   train_acc = 0.5522388059701493   val_acc = 0.2857142857142857\n",
      "iteration: 145/150   loss = 0.7319139242172241   train_acc = 0.5447761194029851   val_acc = 0.42857142857142855\n",
      "iteration: 146/150   loss = 0.7053344249725342   train_acc = 0.5298507462686567   val_acc = 0.21428571428571427\n",
      "iteration: 147/150   loss = 0.7224158644676208   train_acc = 0.582089552238806   val_acc = 0.5714285714285714\n",
      "iteration: 148/150   loss = 0.7342351675033569   train_acc = 0.5298507462686567   val_acc = 0.42857142857142855\n",
      "iteration: 149/150   loss = 0.6947835683822632   train_acc = 0.5522388059701493   val_acc = 0.5\n",
      "iteration: 150/150   loss = 0.6861409544944763   train_acc = 0.5298507462686567   val_acc = 0.5\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "            lr=1.515914167041141e-06,\n",
    "            weight_decay=0.0002300292096821906,\n",
    "            betas=(0.9, 0.995))\n",
    "\n",
    "data = train(model, optimizer, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5017c08c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotpic(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c3a207",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
